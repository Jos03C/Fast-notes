\documentclass[10pt,a4paper]{book}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{scribe}
\usepackage{listings}
\usepackage{eufrak}

\newenvironment{hints}{\textbf{Hints.}}{}

\Scribe{Zhuohua Shen}
\Lecturer{}
\LectureNumber{}
\LectureDate{Oct 2024}
\LectureTitle{Statistical Inference}

\lstset{style=mystyle}

\begin{document}
	\MakeScribeTop
	\tableofcontents

%#############################################################
%#############################################################
%#############################################################
%#############################################################

\chapter{Preliminary}\label{chap:premi}
\section{Random vectors}\label{sec:random_vector}
\begin{defbox}
	\begin{definition}\label{def:random_vector_moments}
		Let $\x=(x_1,\ldots, x_p)^T\in\bbR^p$ be a random vector, 
		\begin{itemize}
			\item $\bbE \x = \bmu=(\bbE x_1,\ldots,\bbE x_p)^T=(\mu_j)$.
			\item $\Var(\x)=\Sigma=\bbE[(\x-\bbE\x)(\x-\bbE\x)^T]=\bbE \x\x^T - \bbE \x \bbE \x^T = (\sigma_{ij})$. $\Sigma \succeq \0$.
			\item Correlation matrix $R=D^{-1/2}\Sigma D^{-1/2}$, where $D=\diag(\sigma_{11},\ldots,\sigma_{pp})$. We have $R_{ij}=\rho_{ij}=\sigma_{ij}/(\sqrt{\sigma_{ii}}\sqrt{\sigma_{jj}})$.
			\item If $\y\in\bbR^{q}$ random vector, then $\Cov(\x,\y)=\bbE[(\x-\bbE\x)(\y-\bbE\y)^T]=\bbE\x\y^T-\bbE\x\bbE\y^T\in\bbR^{p\times q}$.  
		\end{itemize} 
	\end{definition}
\end{defbox}

\begin{thmbox}
	\begin{proposition}\label{prop:random_vector_moments}
		Let $\x\in\bbR^p$ be a random vector, $\a,\b\in\bbR^p$ be vectors, $A\in\bbR^{r_1\times p},B\in\bbR^{r_2\times p}$ be matrices,  
		\begin{itemize}
			\item $\bbE \a^T\x=\a^T \bbE\x$, $\Var(\a^T \x)=\a^T\Sigma\a$, and $\Cov(\a^T\x,\b^T\x)=\a^T \Sigma\b$.
			\item $\bbE A\x=A\bbE \x$, $\Var(A\x)=A\Sigma A^T$, and $\Cov(A\x,B\x)=A\Sigma B^T$   
		\end{itemize}  
	\end{proposition}
\end{thmbox}

\begin{defbox}
	\begin{definition}\label{def:sample_mean_etc}
		Dataset contains $p$ variables and $n$ observations are represented by 
		$X = (\x_{1},\ldots,\x_n)^T$, where the $i$th row $\x_i^T=(x_{i1},\ldots,x_{ip})$ is the $i$th observation vector, $i=1,\ldots,n$. 
		\begin{itemize}
			\item (Sample mean vector) $\bar{\x}=n^{-1}\sum_{i=1}^{n}\x_i=(\bar{x}_1,\ldots,\bar{x}_p)^T$, where $\bar{x}_j=n^{-1}\sum_{i=1}^{n}x_{ij}$.
			\item (Sum of squares and cross product (SSCP) matrix) $A=\sum_{k=1}^{n}(\x_k-\bar{\x})(\x_k-\bar{\x})^T$.
			\item (Sample covariance matrix) $S=(n-1)^{-1}A$.
			\item (Sample correlation matrix) $R=D^{-1/2}S D^{-1/2}$, where $D^{-1/2}=\diag(1/\sqrt{s_{11}},\ldots,1/\sqrt{s_{pp}})$.
		\end{itemize}
	\end{definition}
\end{defbox}
\begin{itemize}
	\item $\bar{\x}=n^{-1}X^T\1_n$, $A=(X-\1_n\bar{\x}^T)^T(X-\1_n\bar{\x}^T)\succeq \0$.
	\item $\bbE\bar{\x}=\bmu$, $\Var(\bar{\x})=n^{-1}\Sigma$, $\bbE A=(n-1)\Sigma$, and $\bbE S=\Sigma$.
\end{itemize}

\subsection{Basic multivariate distributions}\label{sec:Basic-mult-dist}
\begin{defbox}
	\begin{definition}[$p$-variate normal]\label{def:multi_N} $\x\sim\rmN_p(\mu,\Sigma)$ ($\Sigma\succ \0$) has pdf
			\begin{sequation*}
				f(\boldsymbol{x})=\frac{1}{(2\pi)^{p/2}|{\Sigma}|^{1/2}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T}{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}.
			\end{sequation*}
	\end{definition}
\end{defbox}
\begin{itemize}
	\item (\red{Addition}) $\x\sim\rmN_p(\mu_1,\Sigma_1),\y\sim\rmN_p(\mu_y,\Sigma_y)$, $\x\indep \y$, then $\x+\y \sim \rmN_p(\bmu_1+\bmu_2,\Sigma_1+\Sigma_2)$. 
	\item (\red{Linearity}) Let $B\in\bbR^{q\times p}, \b\in\bbR^{q}$ nonrandom, and $B\Sigma B^T\succ \0$, then $B\x+\b \sim \rmN_q (B\bmu+\b,B\Sigma B^T)$.  
	\item (\red{Sample mean}) If $\x_{1:n}\simiid \rmN_p(\bmu,\Sigma)$, then $\bar{\x}\sim\rmN_p(\bmu,n^{-1}\Sigma)$, and $n(\bar{\x}-\bmu)^{T}\Sigma^{-1}(\bar{\x}-\bmu)\sim\chi_p^2$. The squared generalized distance (Mahalanobis distance) 
	$d_i^2=(\boldsymbol{x}_i-\bar{\boldsymbol{x}})^{T}{S}^{-1}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}) \dto \chi_p^2$.   
	\item \red{MLE} of $(\mu,\Sigma)$ is $(\bar{\x},A/n)$.   
	\item (\red{Representation}) Let $\Sigma=HDH^T$ be the spectral
	decomposition, then $\x=HD^{1/2}\z+\bmu$, where $\z\sim\rmN_p(\0_p,I_p)$.
	\item (\red{Marginal and conditional distribution}) Partition
	\begin{sequation*}
		\x=\left[{\begin{array}{c}x_{1}\\x_{2}\end{array}}\right],\quad\boldsymbol{\mu}=\left[{\begin{array}{c}\mu_{1}\\\mu_{2}\end{array}}\right],\quad\boldsymbol{\Sigma}=\left[{\begin{array}{cc}\Sigma_{11}&\Sigma_{12}\\\boldsymbol{\Sigma}_{21}&\boldsymbol{\Sigma}_{22}\end{array}}\right], \quad \x_1\in\bbR^{q}, \x_2\in\bbR^{p-q}, \Sigma_{12}\in\bbR^{q\times(p-q)}.
	\end{sequation*}    
	Then $\x_1\sim\rmN_q(\bmu_1,\Sigma_{11})$, $\x_1\indep\x_2$ iff $\Sigma_{12}=\0$, and $[\x_1\mid\x_2=\x_2^0]\sim\rmN_q(\boldsymbol{\mu}_1+{\Sigma}_{12}{\Sigma}_{22}^{-1}(\boldsymbol{x}_2^0-\boldsymbol{\mu}_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})$.
\end{itemize}

\begin{defbox}
	\begin{definition}[Wishart distribution]\label{def:multi_Wishart} 
	\end{definition}
\end{defbox}

\chapter{Statistical inference fundamentals}\label{chap:stat-inf}

References: most of the contents are from the undergraduate course STA3020 (by Prof. Jianfeng Mao in 2022-2023 T1, and Prof. Jiasheng Shi in 2023-2024 T2) and postgraduate course STAT5010 (by Kin Wai Keith Chan in 2024-2025 T1), with main textbook Casella and Berger \cite{casella2002statistical} 


\section{Statistical Models}
See Chapter 3 of \cite{casella2002statistical}. Suppose $X_i\simiid \bbP_*$, where $\bbP_*$ refers to the unknown \red{data generating process} (DGPg), we find $\widehat{\bbP}\approx \bbP_*$. A \red{statistical model} is a set of distributions $\scrF=\{\bbP_{\theta}:\theta\in\Theta\}$, where $\Theta$ is the \red{parameter space}. A \red{parametric model} is the model with $\dim(\Theta)<\infty$, while a \red{nonparametric model} satisfies $\dim(\Theta)=\infty$. 

\begin{defbox}
	\begin{definition}[\textbf{Exponential family}]\label{def:exp-family}
		A k-dimensional \red{exponential family} (EF) $\scrF=\{f_\theta:\theta\in\Theta\}$ is a model consisting of pdfs of the form
		\begin{equation}\label{eq:exp-family-pdf}
			f_\theta(x)=c(\theta)h(x)\exp\left\{\sum_{j=1}^k\eta_j(\theta)T_j(x)\right\}
		\end{equation}
	\end{definition}
	where $c(\theta),h(x)\geq 0$, $\Theta=\{\theta:c(\theta)\geq 0, \eta_j(\theta) \text{ being well defined for } 1\leq j\leq k\}$. Let $\eta_j=\eta_j(\theta)$, the \red{canonical form} is 
	\begin{equation}\label{eq:exp-family-can}
		f_\eta(x)=b(\eta)h(x)\exp\left\{\sum_{j=1}^k\eta_jT_j(x)\right\},
	\end{equation}
	\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
		\item $k$-dim \red{natural exponential family} (NEF): $\scrF'=\{f_{\eta}:\eta\in \Xi\}$; 
		\item \red{natural parameter} $\eta=(\eta_1,\ldots,\eta_k)^T$;
		\item \red{natural parameter space}: $\Xi=\{\eta\in\mathbb{R}^k:0<b(\eta)<\infty\}$; 
		\item the NEF $\scrF'$ is of \red{full rank} if $\Xi$ contains an open set in $\bbR^k$;
		\item the EF is a \red{curved exponential family} if $p=\dim(\Theta)<k$.
	\end{itemize}
\end{defbox}
\textbf{Properties of EF}:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
	\item Let $X\sim f_{\eta}$, where $\eta\in\Xi$ such that (i) $f_{\eta}$ is of the form \eqref{eq:exp-family-can} with $B(\eta)=-\log b(\eta)$, and (ii) $\Xi$ contains an open set in $\bbR^k$. Then, for $j,j'=1,\ldots,k$, $\bbE\{T_j(X)\}={\partial B(\eta)}/{\partial\eta_j}$ and $\Cov\{T_j(X),T_{j'}(X)\}={\partial^2B(\eta)}/\sbk{\partial\eta_j\partial\eta_{j'}}$.  
	\item \red{Stein's identity}:
\end{itemize}

\begin{defbox}
	\begin{definition}[\textbf{Location-scale family}]\label{def:ls-family}
		Let $f$ be a density. 
		\begin{itemize}
			\item A \red{location-scale family} is given by $\scrF=\{f_{\mu,\sigma}:\mu \in\bbR, \sigma\in\bbR^{++}\}$, where $f_{\mu,\sigma}(x)= f\left({(x-\mu)}/\sigma\right)/\sigma$.
			\item \red{location parameter}: $\mu$; \red{scale parameter}: $\sigma$; \red{standard density}: $f$;
			\item A \red{location family} is $\scrF=\{f_{\mu,1}:\mu\in\bbR\}$.
			\item A \red{scale family} is $\scrF=\{f_{0,\sigma}:\sigma\in\bbR^{++}\}$ 
		\end{itemize}
	\end{definition}
\end{defbox}
\textbf{Representation}: $X=\mu + \sigma Z$, $Z\sim f_{0,1}(\cdot)$. 
\begin{itemize}
	\item See some examples in Example 3.9, Keith's note 3, and Table 1 in Shi's note L1. 
	\item Transform between location parameter and scale parameter by taking log.
\end{itemize}


\begin{defbox}
	\begin{definition}[\textbf{Identifiable family}]\label{def:id-family}
		If $\forall \theta_1,\theta_2\in\Theta$ that    
		\begin{equation*}
			\theta_1\neq\theta_2\quad\Rightarrow\quad f_{\theta_1}(\cdot)\neq f_{\theta_2}(\cdot),
		\end{equation*}
		then $\scrF$ is said to be an \red{identifiable family}, or equivalently $\theta\in\Theta$ is \red{identifiable}. 
	\end{definition}
\end{defbox}

A typical feature of non-identifiable EF is that $p=\dim(\Theta)>k$. Typically,
\begin{itemize}
	\item $p<k$, curved (must).
	\item $p=k$, of full rank.
	\item $p>k$, non-identifiable.  
\end{itemize} 

\section{Principles of Data Reduction}\label{sec:prin-data-reduce}
\red{Statistics:} $T=T(X_{1:n})$, a function of $X_{1:n}$ and free of any unknown parameter.  

\subsection{Sufficiency Principle}\label{sec:prin-data-reduce-suff}
\textbf{Sufficiency principle}: If $T=T(X_{1:n})$ is a ``sufficient statistics'' for $\theta$, then any inference on $\theta$ will depend on $X_{1:n}$ only through $T$.

\begin{defbox}
	\begin{definition}[\textbf{Sufficient, minimal sufficient, ancillary, and complete statistics}]\label{def:stat-SS-MSS-ANS-CS}
		Suppose $X_{1:n}\simiid \bbP_{\theta}$, where $\theta\in\Theta$. Let $T=T(X_{1:n})$ be a statistic. Then $T$ is \red{sufficient} (SS) for $\theta$
		\begin{itemize}
			\item[$\Leftrightarrow$] (def) $[X_{1:n}\mid T=t]$ is free of $\theta$ for each $t$.
			\item[$\Leftrightarrow$] (technical lemma) $T(x_{1:n})=T(x_{1:n}')$ implies that $f_{\theta}(x_{1:n})/f_{\theta}(x_{1:n}')$ is free of $\theta$.
			\item[$\Leftrightarrow$] (Neyman-Fisher factorization theorem) $\forall\theta\in\Theta$, $x_{1:n}\in\scrX^n$, $f_\theta(x_{1:n})=A(t,\theta)B(x_{1:n})$.
		\item[$\Leftrightarrow$] Define $\Lambda(\theta',\theta''\mid x_{1:n}):=f_{\theta'}(x_{1:n})/f_{\theta''}(x_{1:n})$. $\forall \theta',\theta''\in\Theta$, $\exists$ function $C_{\theta',\theta''}$ such that $\Lambda(\theta',\theta''\mid x_{1:n})=C_{\theta',\theta''}(t)$, for all $x_{1:n}\in\scrX^n$ where $t=T(x_{1:n})$.    
		\end{itemize}
	$T$ is \red{minimal sufficient} (MSS) for $\theta$
	\begin{itemize}
		\item[$\Leftrightarrow$] (def) (1) $T$ is a SS for $\theta$; (2) $T=g(S)$ for any other SS $S$.
		\item[$\Leftrightarrow$] (1) $T$ is a SS for $\theta$; (2) $S(x_{1:n})=S(x_{1:n}')$ implies $T(x_{1:n})=T(x_{1:n}')$ for any SS $S$.   
		\item[$\Leftrightarrow$] (Lehmann-Scheffé theorem) $\forall x_{1:n},x_{1:n}'\in\scrX^n$, $f_{\theta}(x_{1:n})/f_{\theta}(x_{1:n}')$ is free of $\theta$ $\Leftrightarrow$ $T(x_{1:n})=T(x_{1:n}')$.    
	\end{itemize}
	$A=A(X_{1:n})$ is \red{ancillary} (ANS) if the distribution of $A$ does not depend on $\theta$. 

	\noindent $T$ is \red{complete} (CS) if $\forall\theta\in\Theta$, $\Ex_{\theta}g(T)=0$ implies $\forall\theta\in\Theta$, $\bbP_{\theta}\{g(T)=0\}=1$. 
	\end{definition}
\end{defbox}
\noindent\textbf{Properties}
\begin{itemize}
	\item (Transformation) If $T=r(T')$, then (i) $T$ is SS $\Rightarrow$ $T'$ is SS; (ii) $T'$ is CS $\Rightarrow$ $T$ is CS; (iii) $r$ is one-to-one, then if one is SS/MSS/CS, then the another is.    
	\item (\red{Basu's Lemma}) $X_i\simiid\bbP_\theta$, $A$ is ANS and $T$ s CSS, then $A \indep T$.
	\item (\red{Bahadur's theorem}) $X_i\simiid\bbP_\theta$, if an MSS exists, then any CSS is also an MSS.
	\begin{itemize}
		\item Then if a CSS exists, then any MSS is also a CSS $\Rightarrow$ CSS=MSS.
		\item \red{All or nothing}: start with MSS $T$, check whether $T$ is CS. (i) Yes, it is both CSS and MSS, then the set of MSS=CSS; (ii) No, there is no CSS at all.  
	\end{itemize}
	\item (Exp-family) If $X_i\simiid f_{\eta}$ in \eqref{eq:exp-family-can}, then $T=(\sum_{i=1}^nT_1(X_i),\ldots,\sum_{i=1}^nT_k(X_i))$ is a SS, called \red{natural sufficient statistic}. If $\Xi$ contains an open set in $\bbR^k$ (i.e., $\scrF'$ is of full rank), then $T$ is MSS and CSS. 
\end{itemize}

\noindent\textbf{Proof techniques}
\begin{itemize}
	\item Prove $T$ is not sufficient for $\theta$: show if $\exists x_{1_n}, x_{1:n}'\in\calX^n$ and $\theta',\theta''\in\Theta$, such that $T(x_{1:n})=T(x_{1:n}')$ and $\Lambda(\theta',\theta''\mid x_{1:n})\neq \Lambda(\theta',\theta''\mid x_{1:n}')$.  
	\item Prove $A$ is an ANS: consider location-scale representation.
	\item Prove $T$ is a CS: use definition or take $\rmd\Ex_{\theta}g(T)/\rmd\theta=0$. 
	\item Disprove $T$ is CS: 
	\begin{itemize}
		\item Construct an ANS $S(T)$ based on $T$, then $\Ex S(T)$ is free of $\theta$, then $g(T)=S(T)-\Ex S(T)$ is free of $\theta$ but $g(T)\neq 0$ w.p.1. 
		\item (Cancel the 1st moment) Find two unbiased estiamtors for $\theta$ as a function of $T$. E.g., $X_1,X_2\simiid \mathrm{N}(\theta,\theta^2)$, $T=(X_1,X_2)$, $g(T)=X_1-X_2\sim\mathrm{N}(0,2\theta^2)$. 
	\end{itemize}
	
\end{itemize}


\begin{remark}\label{rmk:SS-MSS-ANS-CS}
	\begin{itemize}
		\item ANS $A$ is useless on its own, but useful together with other information. 
		\item $\bbP(A(\X)\mid \theta)$ is free of $\theta$, but for non-SS $T$, $\bbP(A(\X)\mid T(\X))$ is not necessarily free of $\theta$. 
	\end{itemize}
\end{remark}

\subsection{Likelihood principle}\label{sec:prin-data-reduce-lik}




\chapter{Multivariate Inference Fundamentals}\label{chap:multi}



\bibliographystyle{abbrv}
\bibliography{mybib}
%%% end of doc
\end{document}