\documentclass[10pt,a4paper]{book}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{scribe}
\usepackage{listings}
\usepackage{eufrak}

\newenvironment{hints}{\textbf{Hints.}}{}

\Scribe{Zhuohua Shen}
\Lecturer{}
\LectureNumber{}
\LectureDate{Oct 2024}
\LectureTitle{Statistical Inference}

\lstset{style=mystyle}

\begin{document}
	\MakeScribeTop
	\tableofcontents

%#############################################################
%#############################################################
%#############################################################
%#############################################################

\chapter{Preliminary}\label{chap:premi}


\chapter{Statistical inference fundamentals}\label{chap:stat-inf}

References: most of the contents are from the undergraduate course STA3020 (by Prof. Jianfeng Mao in 2022-2023 T1, and Prof. Jiasheng Shi in 2023-2024 T2) and postgraduate course STAT5010 (by Kin Wai Keith Chan in 2024-2025 T1), with main textbook Casella and Berger \cite{casella2002statistical} 


\section{Statistical Models}
See Chapter 3 of \cite{casella2002statistical}. Suppose $X_i\simiid \bbP_*$, where $\bbP_*$ refers to the unknown \red{data generating process} (DGPg), we find $\widehat{\bbP}\approx \bbP_*$. A \red{statistical model} is a set of distributions $\scrF=\{\bbP_{\theta}:\theta\in\Theta\}$, where $\Theta$ is the \red{parameter space}. A \red{parametric model} is the model with $\dim(\Theta)<\infty$, while a \red{nonparametric model} satisfies $\dim(\Theta)=\infty$. 

\begin{defbox}
	\begin{definition}[\textbf{Exponential family}]\label{def:exp-family}
		A k-dimensional \red{exponential family} (EF) $\scrF=\{f_\theta:\theta\in\Theta\}$ is a model consisting of pdfs of the form
		\begin{equation}\label{eq:exp-family-pdf}
			f_\theta(x)=c(\theta)h(x)\exp\left\{\sum_{j=1}^k\eta_j(\theta)T_j(x)\right\}
		\end{equation}
	\end{definition}
	where $c(\theta),h(x)\geq 0$, $\Theta=\{\theta:c(\theta)\geq 0, \eta_j(\theta) \text{ being well defined for } 1\leq j\leq k\}$. Let $\eta_j=\eta_j(\theta)$, the \red{canonical form} is 
	\begin{equation}\label{eq:exp-family-can}
		f_\eta(x)=b(\eta)h(x)\exp\left\{\sum_{j=1}^k\eta_jT_j(x)\right\},
	\end{equation}
	\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
		\item $k$-dim \red{natural exponential family} (NEF): $\scrF'=\{f_{\eta}:\eta\in \Xi\}$; 
		\item \red{natural parameter} $\eta=(\eta_1,\ldots,\eta_k)^\TT$;
		\item \red{natural parameter space}: $\Xi=\{\eta\in\mathbb{R}^k:0<b(\eta)<\infty\}$; 
		\item the NEF $\scrF'$ is of \red{full rank} if $\Xi$ contains an open set in $\bbR^k$;
		\item the EF is a \red{curved exponential family} if $p=\dim(\Theta)<k$.
	\end{itemize}
\end{defbox}
\textbf{Properties of EF}:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
	\item Let $X\sim f_{\eta}$, where $\eta\in\Xi$ such that (i) $f_{\eta}$ is of the form \eqref{eq:exp-family-can} with $B(\eta)=-\log b(\eta)$, and (ii) $\Xi$ contains an open set in $\bbR^k$. Then, for $j,j'=1,\ldots,k$, $\bbE\{T_j(X)\}={\partial B(\eta)}/{\partial\eta_j}$ and $\Cov\{T_j(X),T_{j'}(X)\}={\partial^2B(\eta)}/\sbk{\partial\eta_j\partial\eta_{j'}}$.  
	\item \red{Stein's identity}:
\end{itemize}

\begin{defbox}
	\begin{definition}[\textbf{Location-scale family}]\label{def:ls-family}
		Let $f$ be a density. 
		\begin{itemize}
			\item A \red{location-scale family} is given by $\scrF=\{f_{\mu,\sigma}:\mu \in\bbR, \sigma\in\bbR^{++}\}$, where $f_{\mu,\sigma}(x)= f\left({(x-\mu)}/\sigma\right)/\sigma$.
			\item \red{location parameter}: $\mu$; \red{scale parameter}: $\sigma$; \red{standard density}: $f$;
			\item A \red{location family} is $\scrF=\{f_{\mu,1}:\mu\in\bbR\}$.
			\item A \red{scale family} is $\scrF=\{f_{0,\sigma}:\sigma\in\bbR^{++}\}$ 
		\end{itemize}
	\end{definition}
\end{defbox}
\textbf{Representation}: $X=\mu + \sigma Z$, $Z\sim f_{0,1}(\cdot)$. 
\begin{itemize}
	\item See some examples in Example 3.9, Keith's note 3, and Table 1 in Shi's note L1. 
	\item Transform between location parameter and scale parameter by taking log.
\end{itemize}


\begin{defbox}
	\begin{definition}[\textbf{Identifiable family}]\label{def:id-family}
		If $\forall \theta_1,\theta_2\in\Theta$ that    
		\begin{equation*}
			\theta_1\neq\theta_2\quad\Rightarrow\quad f_{\theta_1}(\cdot)\neq f_{\theta_2}(\cdot),
		\end{equation*}
		then $\scrF$ is said to be an \red{identifiable family}, or equivalently $\theta\in\Theta$ is \red{identifiable}. 
	\end{definition}
\end{defbox}

A typical feature of non-identifiable EF is that $p=\dim(\Theta)>k$. Typically,
\begin{itemize}
	\item $p<k$, curved (must).
	\item $p=k$, of full rank.
	\item $p>k$, non-identifiable.  
\end{itemize} 

\section{Principles of Data Reduction}\label{sec:prin-data-reduce}
\red{Statistics:} $T=T(X_{1:n})$, a function of $X_{1:n}$ and free of any unknown parameter.  

\subsection{Sufficiency Principle}\label{sec:prin-data-reduce-suff}
\textbf{Sufficiency principle}: If $T=T(X_{1:n})$ is a ``sufficient statistics'' for $\theta$, then any inference on $\theta$ will depend on $X_{1:n}$ only through $T$.

\begin{defbox}
	\begin{definition}[\textbf{Sufficient, minimal sufficient, ancillary, and complete statistics}]\label{def:stat-SS-MSS-ANS-CS}
		Suppose $X_{1:n}\simiid \bbP_{\theta}$, where $\theta\in\Theta$. Let $T=T(X_{1:n})$ be a statistic. Then $T$ is \red{sufficient} (SS) for $\theta$
		\begin{itemize}
			\item[$\Leftrightarrow$] (def) $[X_{1:n}\mid T=t]$ is free of $\theta$ for each $t$.
			\item[$\Leftrightarrow$] (technical lemma) $T(x_{1:n})=T(x_{1:n}')$ implies that $f_{\theta}(x_{1:n})/f_{\theta}(x_{1:n}')$ is free of $\theta$.
			\item[$\Leftrightarrow$] (Neyman-Fisher factorization theorem) $\forall\theta\in\Theta$, $x_{1:n}\in\scrX^n$, $f_\theta(x_{1:n})=A(t,\theta)B(x_{1:n})$.
		\item[$\Leftrightarrow$] Define $\Lambda(\theta',\theta''\mid x_{1:n}):=f_{\theta'}(x_{1:n})/f_{\theta''}(x_{1:n})$. $\forall \theta',\theta''\in\Theta$, $\exists$ function $C_{\theta',\theta''}$ such that $\Lambda(\theta',\theta''\mid x_{1:n})=C_{\theta',\theta''}(t)$, for all $x_{1:n}\in\scrX^n$ where $t=T(x_{1:n})$.    
		\end{itemize}
	$T$ is \red{minimal sufficient} (MSS) for $\theta$
	\begin{itemize}
		\item[$\Leftrightarrow$] (def) (1) $T$ is a SS for $\theta$; (2) $T=g(S)$ for any other SS $S$.
		\item[$\Leftrightarrow$] (1) $T$ is a SS for $\theta$; (2) $S(x_{1:n})=S(x_{1:n}')$ implies $T(x_{1:n})=T(x_{1:n}')$ for any SS $S$.   
		\item[$\Leftrightarrow$] (Lehmann-Scheffé theorem) $\forall x_{1:n},x_{1:n}'\in\scrX^n$, $f_{\theta}(x_{1:n})/f_{\theta}(x_{1:n}')$ is free of $\theta$ $\Leftrightarrow$ $T(x_{1:n})=T(x_{1:n}')$.    
	\end{itemize}
	$A=A(X_{1:n})$ is \red{ancillary} (ANS) if the distribution of $A$ does not depend on $\theta$. 

	\noindent $T$ is \red{complete} (CS) if $\forall\theta\in\Theta$, $\bbE_{\theta}g(T)=0$ implies $\forall\theta\in\Theta$, $\bbP_{\theta}\{g(T)=0\}=1$. 
	\end{definition}
\end{defbox}
\noindent\textbf{Properties}
\begin{itemize}
	\item (Transformation) If $T=r(T')$, then (i) $T$ is SS $\Rightarrow$ $T'$ is SS; (ii) $T'$ is CS $\Rightarrow$ $T$ is CS; (iii) $r$ is one-to-one, then if one is SS/MSS/CS, then the another is.    
	\item (\red{Basu's Lemma}) $X_i\simiid\bbP_\theta$, $A$ is ANS and $T$ s CSS, then $A \indep T$.
	\item (\red{Bahadur's theorem}) $X_i\simiid\bbP_\theta$, if an MSS exists, then any CSS is also an MSS.
	\begin{itemize}
		\item Then if a CSS exists, then any MSS is also a CSS $\Rightarrow$ CSS=MSS.
		\item \red{All or nothing}: start with MSS $T$, check whether $T$ is CS. (i) Yes, it is both CSS and MSS, then the set of MSS=CSS; (ii) No, there is no CSS at all.  
	\end{itemize}
	\item (Exp-family) If $X_i\simiid f_{\eta}$ in \eqref{eq:exp-family-can}, then $T=(\sum_{i=1}^nT_1(X_i),\ldots,\sum_{i=1}^nT_k(X_i))$ is a SS, called \red{natural sufficient statistic}. If $\Xi$ contains an open set in $\bbR^k$ (i.e., $\scrF'$ is of full rank), then $T$ is MSS and CSS. 
\end{itemize}

\noindent\textbf{Proof techniques}
\begin{itemize}
	\item Prove $T$ is not sufficient for $\theta$: show if $\exists x_{1_n}, x_{1:n}'\in\calX^n$ and $\theta',\theta''\in\Theta$, such that $T(x_{1:n})=T(x_{1:n}')$ and $\Lambda(\theta',\theta''\mid x_{1:n})\neq \Lambda(\theta',\theta''\mid x_{1:n}')$.  
	\item Prove $A$ is an ANS: consider location-scale representation.
	\item Prove $T$ is a CS: use definition or take $\rmd\bbE_{\theta}g(T)/\rmd\theta=0$. 
	\item Disprove $T$ is CS: 
	\begin{itemize}
		\item Construct an ANS $S(T)$ based on $T$, then $\bbE S(T)$ is free of $\theta$, then $g(T)=S(T)-\bbE S(T)$ is free of $\theta$ but $g(T)\neq 0$ w.p.1. 
		\item (Cancel the 1st moment) Find two unbiased estiamtors for $\theta$ as a function of $T$. E.g., $X_1,X_2\simiid \mathrm{N}(\theta,\theta^2)$, $T=(X_1,X_2)$, $g(T)=X_1-X_2\sim\mathrm{N}(0,2\theta^2)$. 
	\end{itemize}
	
\end{itemize}


\begin{remark}\label{rmk:SS-MSS-ANS-CS}
	\begin{itemize}
		\item ANS $A$ is useless on its own, but useful together with other information. 
		\item $\bbP(A(\X)\mid \theta)$ is free of $\theta$, but for non-SS $T$, $\bbP(A(\X)\mid T(\X))$ is not necessarily free of $\theta$. 
	\end{itemize}
\end{remark}

\subsection{Likelihood principle}\label{sec:prin-data-reduce-lik}




\chapter{Multivariate Inference Fundamentals}\label{chap:multi}
Reference: 
\begin{itemize}
	\item Robb J. Muirhead - Aspects of multivariate statistical theory \cite{muirhead1982aspects}.
	\item CUHK STAT4002 - Applied Multivariate Analysis (2023 Spring), by Zhixiang Lin.
	\item CUHK STAT5030 - Linear Models (2025 Spring), by Yuanyuan Lin.
	\item Peng DING - Linear Model and Extensions.
	\item Ronald Christensen - Plane answers to
	complex questions: the theory of linear models \cite{christensen2002plane}.
\end{itemize}

\section{Random vectors and distributions}\label{sec:random_vector}
\begin{defbox}
	\begin{definition}\label{def:random_vector_moments}
		Let $\x=(x_1,\ldots, x_p)^\TT\in\bbR^p$ be a random vector, 
		\begin{itemize}
			\item \red{Mean} $\bbE \x = \bam=(\bbE x_1,\ldots,\bbE x_p)^\TT=(\mu_j)$.
			\item \red{Covariance matrix} $\Var(\x)=\Cov(\x)=\Sigma=\bbE[(\x-\bbE\x)(\x-\bbE\x)^\TT]=\bbE \x\x^\TT - \bbE \x \bbE \x^\TT = (\sigma_{ij})$, $\Sigma\succeq \0$.
			\item \red{Correlation matrix} $R=D^{-1/2}\Sigma D^{-1/2}$, where $D=\diag(\sigma_{11},\ldots,\sigma_{pp})$. We have $R_{ij}=\rho_{ij}=\sigma_{ij}/(\sqrt{\sigma_{ii}}\sqrt{\sigma_{jj}})$.
			\item If $\y\in\bbR^{q}$ random vector, then $\Cov(\x,\y)=\bbE[(\x-\bbE\x)(\y-\bbE\y)^\TT]=\bbE\x\y^\TT-\bbE\x\bbE\y^\TT\in\bbR^{p\times q}$.  
		\end{itemize} 
		If $\Z=(z_{ij})\in\bbR^{p\times q}$ is a random matrix,
		\begin{itemize}
			\item $\bbE\Z=(\bbE z_{ij})$. 
		\end{itemize} 
	\end{definition}
\end{defbox}

\begin{thmbox}
	\begin{proposition}\label{prop:random_vector_moments}
		Let $\x\in\bbR^p$ be a random vector, $\a,\b\in\bbR^p$ be vectors, $A\in\bbR^{r_1\times p},B\in\bbR^{r_2\times p}$ be matrices,  
		\begin{itemize}
			\item $\bbE \a^\TT\x=\a^\TT \bbE\x$, $\Var(\a^\TT \x)=\a^\TT\Sigma\a$, and $\Cov(\a^\TT\x,\b^\TT\x)=\a^\TT \Sigma\b$.
			\item $\bbE A\x=A\bbE \x$, $\Var(A\x)=A\Sigma A^\TT$, and $\Cov(A\x,B\x)=A\Sigma B^\TT$.
			\item If $\y=A\x+\b$, where $A\in\bbR^{q\times p}$, $\b\in\bbR^{q}$, then $\bam_\y=A \bam_\x+\b$ and $\Sigma_\y=A\Sigma_\x A^\TT$.    
			\item $\bbE(\x^\TT A\x)=\tr(A\Sigma)+\bam^\TT A\bam$.  
		\end{itemize}  
		Let $\Z\in\bbR^{p\times q}$ be a random matrix, $B\in\bbR^{m\times p}$, $C\in\bbR^{q\times n}$, and $D\in\bbR^{m\times n}$ constants, then 
		\begin{itemize}
			\item $\bbE (B\Z C+D) = B \bbE (\Z) C+D$. 
		\end{itemize}   
	\end{proposition}
\end{thmbox}
\begin{itemize}
	\item The $\Sigma\in\bbR^{p\times p}$ is a covariance matrix (i.e., $\Sigma=\Cov(\x)$ for some random vector $\x\in\bbR^p$) iff $\Sigma\succeq \0$.
	\begin{itemize}
		\item \small $(\Leftarrow)$: suppose $\rmr(\Sigma)=r\leq p$, write full rank decomposition $\Sigma=C C^\TT$, $C\in\bbR^{p\times r}$. Let $\y\sim[\0_r,I_r]$, then $\Cov(C\y)=\Sigma$.    
	\end{itemize}
	\item If $\Sigma$ is not PD, then $\exists \a\neq\0_p$ s.t. $\Var(\a^\TT \x)=0$ so w.p.1., $\a^\TT \x=k$, i.e., $\x$ lies in a hyperplane.     
\end{itemize}

\begin{thmbox}
	\begin{theorem}\label{thm:rv_uni_linearFun}
		If $\x\in\bbR^p$ random, then its distribution is uniquely determined by the distributions of $\a^\TT \x$, $\forall \a\in\bbR^p$.  
	\end{theorem}
\end{thmbox}
The proof uses the fact that a distribution in $\bbR^p$ is uniquely determined by its ch.f., see Theorem 1.2.2. \cite{muirhead1982aspects}.



\begin{defbox}
	\begin{definition}\label{def:sample_mean_etc}
		Dataset contains $p$ variables and $n$ observations are represented by 
		$\X = (\x_{1},\ldots,\x_n)^\TT$, where the $i$th row $\x_i^\TT=(x_{i1},\ldots,x_{ip})$ is the $i$th observation vector, $i=1,\ldots,n$. 
		\begin{itemize}
			\item (\red{Sample mean vector}) $\bar{\x}=n^{-1}\sum_{i=1}^{n}\x_i=(\bar{x}_1,\ldots,\bar{x}_p)^\TT$, where $\bar{x}_j=n^{-1}\sum_{i=1}^{n}x_{ij}$.
			\item (\red{Sum of squares and cross product (SSCP) matrix}) $\A=\sum_{i=1}^{n}(\x_i-\bar{\x})(\x_i-\bar{\x})^\TT$.
			\item (\red{Sample covariance matrix}) $\S=(n-1)^{-1}\A$.
			\item (\red{Sample correlation matrix}) $\R=D^{-1/2}\S D^{-1/2}$, where $D^{-1/2}=\diag(1/\sqrt{s_{11}},\ldots,1/\sqrt{s_{pp}})$.
		\end{itemize}
	\end{definition}
\end{defbox}
\begin{itemize}
	\item $\bar{\x}=n^{-1}\X^\TT\1_n$, and
	\begin{sequation*}
		\begin{aligned}
			\A&= \sum_{i=1}^{n}(\x_i-\bam)(\x_i-\bam)^\TT-n(\bar{\x}-\bam)(\bar{\x}-\bam)^\TT \\ & =(\X-\1_n\bar{\x}^\TT)^\TT(\X-\1_n\bar{\x}^\TT)\succeq \0.
		\end{aligned}
	\end{sequation*}
	\item $\bbE\bar{\x}=\bam$, $\Var(\bar{\x})=n^{-1}\Sigma$, $\bbE \A=(n-1)\Sigma$, and $\bbE \S=\Sigma$.
\end{itemize}

\subsection{Multivariate normal distribution}\label{sec:mult_normal}
\begin{defbox}
	\begin{definition}[Original definition of multivariate normal]\label{def:oridef_multi_normal}
		The random vector $\x\in\bbR^p$ is said to have an $p$-variate normal distribution ($\x\sim\rmN_p$) if $\forall \a\in\bbR^p$, the distribution of $\a^\TT\x$ is univariate normal.  
	\end{definition}
\end{defbox}
\begin{thmbox}
	\begin{theorem}[Fundamental properties]\label{thm:multi_normal}
		Let $\x\sim\rmN_p$, we have
		\begin{enumerate}
			\item Both $\bam=\bbE\x$ and $\Sigma=\Cov(\x)$ exist and the distribution of $\x$ is determined by $\bam$ and $\Sigma$. Write $\x\sim\rmN_p(\bam,\Sigma)$. 
			\item (\red{Representation}) Let $\Sigma\succeq\0_{p\times p}$, $\rmr(\Sigma)=r\leq p$, and $u_{1:r}\simiid\rmN(0,1)$, i.e., $\u\sim\rmN_r(\0_r,I_r)$, then if $C$ is the full rank decomposition of $\Sigma$ and $\bam\in\bbR^p$, then $\x=C\u+\bam\sim\rmN_p(\bam,\Sigma)$.  
			\begin{itemize}
				\item Let $\Sigma=HDH^\TT$ be the spectral
				decomposition, then $\x=HD^{1/2}\z+\bam$, where $\z\sim\rmN_p(\0_p,I_p)$.
			\end{itemize}
			\item If $\x\sim\rmN_p(\bam,\Sigma)$, then its \red{ch.f.} $\phi_\x(\t)=\exp(i\bam^\TT\t-\t^\TT\Sigma\t/2)$.
			\item (\red{Density}) $\x\sim\rmN_p(\bam,\Sigma)$ with $\Sigma\succ \0$, then $\x$ has pdf
			\begin{sequation}\label{eq:mult_normal_density}
				f(\boldsymbol{x})=\frac{1}{(2\pi)^{p/2}|{\Sigma}|^{1/2}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T}{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}.
			\end{sequation}
		\end{enumerate}
	\end{theorem}
\end{thmbox}
Note that we guarantee the existence of $\rmN_p(\bam,\Sigma)$ by means of the representation in point 2. By its density, we have \underline{MVN kernel}: If
\begin{sequation*}
	f(\x)\propto \exp\cbk{-\frac{1}{2}(\x^\TT A\x-2\x^\TT B)}=\exp\cbk{-\frac{1}{2}(\x-A^{-1}B)^\TT A (\x-A^{-1}B)-B^\TT A^{-1} B},
\end{sequation*}
then $\x\sim\rmN_p(A^{-1}B,A^{-1})$.

\begin{thmbox}
	\begin{theorem}[Properties of multivariate normal]\label{thm:multi_normal_prop}
		If $\x\sim\rmN_p(\bam,\Sigma)$, then we have 
		\begin{enumerate}
			\item (\red{Linearity}) Let $B\in\bbR^{q\times p}, \b\in\bbR^{q}$ nonrandom, and $B\Sigma B^\TT\succ \0$, then $B\x+\b \sim \rmN_q (B\bam+\b,B\Sigma B^\TT)$.  
			\item (\red{Linear combinations}) If $\x_k\sim\rmN_p(\bam_k,\Sigma_k)\indep$ for $k=1,\ldots,N$, then for any fixed constants $\alpha_1,\ldots,\alpha_N$, $\sum_{k=1}^N\alpha_k\x_k\sim\rmN_p(\sum_{k=1}^{N}\alpha_k\bam_k,\sum_{k=1}^{N}\alpha_k^2\Sigma_k)$.
			\begin{itemize}
				\item The sample mean $\bar{\x}\sim\rmN_p(\bam,\Sigma/N)$. 
			\end{itemize}
			\item (\red{Subset}) The marginal distribution of any subset of $k(<p)$ components of $\x$ is $k$-variate normal.   
			\item (\red{Marginal distribution}) Partition
			\begin{sequation*}
				\x=\left[{\begin{array}{c}\x_{1}\\\x_{2}\end{array}}\right],\quad\boldsymbol{\mu}=\left[{\begin{array}{c}\bam_{1}\\\bam_{2}\end{array}}\right],\quad\boldsymbol{\Sigma}=\left[{\begin{array}{cc}\Sigma_{11}&\Sigma_{12}\\ {\Sigma}_{21}& {\Sigma}_{22}\end{array}}\right], \quad \x_1\in\bbR^{q}, \x_2\in\bbR^{p-q}, \Sigma_{12}\in\bbR^{q\times(p-q)}.
			\end{sequation*}    
			Then $\x_1\sim\rmN_q(\bam_1,\Sigma_{11})$, $\x_1\indep\x_2$ iff $\Sigma_{12}=\0$. 
			\item (\red{Conditional distribution}) Let $\Sigma_{22}^{-}$ be a generalized inverse of $\Sigma_{22}$ (i.e., $\Sigma_{22}\Sigma_{22}^-\Sigma_{22}=\Sigma_{22}$), then 
			\begin{itemize}
				\item[(a)] $\x_1-\Sigma_{12}\Sigma_{22}^-\x_2\sim \rmN_q(\bam_1-\Sigma_{12}\Sigma_{22}^-\bam_2,\Sigma_{11}-\Sigma_{12}\Sigma_{22}^-\Sigma_{21})$, and $\indep \x_2$.
				\item[(b)] $[\x_1\mid\x_2]\sim\rmN_q(\bam_1+{\Sigma}_{12}{\Sigma}_{22}^-(\x_2-\bam_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-}\Sigma_{21})$. 
			\end{itemize}  
			\item (\red{Cramér}) If $p\times 1$ random vectors $\x\indep \y$ and $\x+\y\sim\rmN_p$, then both $\x,\y\sim\rmN_p$. 
			\item (\red{MLE}) of $(\mu,\Sigma)$ is $(\bar{\x},A/n)$.    
			\item (\red{Inverse of $\Sigma$ and conditional independence}) Denote $\Sigma^{-1}=(\nu^{jk})_{1\leq j,k\leq p}$. Then $\forall j\neq k$, $v^{jk}=0\Leftrightarrow  x_j\indep x_k \mid \x\setminus\{x_j,x_k\}$.    
		\end{enumerate} 
	\end{theorem}
\end{thmbox}
For point 3, each component of a random vector is (marginally) normal does not imply that the vector has a multivariate normal distribution. Counterexample: let $U_1,U_2,U_3\simiid\rmN(0,1)$, $Z\indep U_{1:3}$. Define 
\begin{sequation*}
	X_1 = \frac{U_1+ZU_3}{\sqrt{1+Z^2}},\quad X_2 = \frac{U_2+ZU_3}{\sqrt{1+Z^2}}.
\end{sequation*}   
Then $[X_1|Z]\sim\rmN(0,1)$, free of $Z$, so $X_1\sim\rmN(0,1)$, and $X_2\sim\rmN(0,1)$. But $(X_1,X_2)$ not normal.
The converse is true if the components of $\x$ are all independent and normal, or if $\x$ consists of independent subvectors, each of which is normally distributed.

For the proof of point 5, we use the lemma: if $\Sigma\succeq \0$, 
then $\ker(\Sigma_{22})\subset \ker(\Sigma_{12})$, and $\range(\Sigma_{21})\subset\range(\Sigma_{22})$. 
So $\exists B\in\bbR^{q\times(p-q)}$ satisfying $\Sigma_{12}=B\Sigma_{22}$.  

\subsubsection{Quadratic form of MVN}\label{sec:quad_MVN}
\begin{thmbox}
	\begin{proposition}[MGF]\label{prop:MVN_MGF_quad}
		If $\x\sim\rmN(\bam,\Sigma)$, $A$ is symmetric and $\sigma$ IS non-singular, then 
		\begin{equation*}
			M_{\x^\TT\A\x}(t)=|I-2tA\Sigma|^{-\frac{1}{2}}\exp\cbk{-\frac{1}{2}\bam^\TT[I-(I-2t\A\Sigma)^{-1}]{\Sigma}^{-1}\bam}.
		\end{equation*}
	\end{proposition}
\end{thmbox}


\begin{thmbox}
	\begin{proposition}[Variance]\label{prop:var_MVN_quad}
		If $\x\sim\rmN(\bam,\Sigma)$, then 
		\begin{sequation*}
			\begin{aligned}
			\Var(\x^\TT A \x)&=2\tr(A\Sigma A\Sigma)+4\bam^\TT A\Sigma A \bam, \\
			\Cov(\x^\TT A_1\x,\x^\TT A_2\x)&=2\tr(A_1\Sigma A_2 \Sigma)+4\bam^\TT A_1 \Sigma A_2 \bam.
			\end{aligned}
		\end{sequation*} 
	\end{proposition}
\end{thmbox}
See Problem B 2.14 in Peng DING. 

\begin{thmbox}
	\begin{theorem}[Independence of quadratic form]\label{thm:MVN_quad_indep}
		Assume $\x\sim N_p(\mu,\Sigma)$. 
		\begin{enumerate}
			\item 		For two symmetric matrix $A,B\in\bbR^{p\times p}$, $\x^\TT A \x\indep \x^\TT B\x$ iff  
			\begin{equation*}
				\Sigma A\Sigma B\Sigma=\0,\quad\Sigma A\Sigma B\bam=\Sigma B\Sigma A\bam=\0,\quad\bam^\TT A\Sigma B\bam=0.
			\end{equation*}	
			If $\Sigma\succ \0$, then iff $A\Sigma B=\0$.  
			\item If $\Sigma\succ \0$, $A\in\bbR^{p\times p}$ symmetric, and $B\in\bbR^{r\times p}$, then $\x^\TT A\x \indep B\x$ iff $B\Sigma A=\0$.    
		\end{enumerate}
	\end{theorem}
\end{thmbox}
See Theorem B.11 in Peng DING, and Problem 1.22--1.23 in \cite{muirhead1982aspects}.

\begin{thmbox}
	\begin{theorem}[Quadratic form of $\Sigma$]\label{thm:quad_MVN}
		If $\x,\x_{1:N}\simiid\rmN_p(\bam,\Sigma)$, where $\Sigma$ is nonsingular, then 
		\begin{itemize}
			\item $(\x-\bam)^\TT \Sigma^{-1}(\x-\bam)\sim\chi_p^2$,
			\item $\x^\TT \Sigma^{-1}\x\sim\chi_p^2(\bam^\TT\Sigma^{-1}\bam)$,
			\item partition
			\begin{sequation*}
				\x = \begin{bmatrix}
					\x_1 \\ \x_2
				\end{bmatrix},\ \bam=\begin{bmatrix}
					\bam_1 \\ \bam_2
				\end{bmatrix},\ \Sigma=\begin{bmatrix}
					\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}
				\end{bmatrix}, \quad \x_1,\bam_1\in\bbR^{k}, \ \Sigma_{11}\in\bbR^{k\times k}, \text{ then }
			\end{sequation*} 
			\begin{sequation*}
				Q=(\x-\bam)^\TT \Sigma^{-1}(\x-\bam)-(\x_1-\bam_1)^\TT \Sigma_{11}^{-1}(\x_1-\bam_1) \sim \chi_{p-k}^2.
			\end{sequation*}
			\item $N(\bar{\x}_N-\bam)^{\TT}\Sigma^{-1}(\bar{\x}_N-\bam)\sim\chi_p^2$,
			\item the Mahalanobis distance
			$d_i^2=(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_N)^{\TT}{\S}^{-1}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_N) \dto \chi_p^2$.
		\end{itemize} 
		If $\rmr(\Sigma)=k\leq p$, then 
		\begin{enumerate}
			\item 
			\begin{sequation*}
				\begin{aligned}
					(\x-\bam)^\TT \Sigma^{-}(\x-\bam)&\sim\chi_{k}^2,\\
					(\x-\bam)^\TT \Sigma^{+}(\x-\bam)&\sim\chi_{k}^2.
				\end{aligned}
			\end{sequation*}
			\item If $\Sigma$ is idempotent with $r(\Sigma)=k$, then $(\x-\bam)^\TT  (\x-\bam)\sim\chi_k^2$.  
			\begin{itemize}
				\item If $\bam\in\Col(\Sigma)$, then $\x^\TT \x\sim\chi_k^2(\bam^\TT \bam)$.  
			\end{itemize}
		\end{enumerate}
	\end{theorem}
\end{thmbox}
\begin{thmbox}
	\begin{theorem}[Quadratic form of any matrices]\label{thm:quad_MVN_general}If $\x\sim\rmN_p(\bam,\Sigma)$,
		\begin{enumerate}
			\item if $\Sigma$ is nonsingular, $B\in\bbR^{p\times p}$ is symmetric, then $\x^\TT B\x\sim\chi_{k}^2(\delta)$ iff $B\Sigma$ is idempotent (equiv., $B\Sigma B=B$), in which case $k=\rmr(B)$ and $\delta=\bam^\TT B\bam$;
			
			\item for $A\in\bbR^{p\times p}$, $\x^\TT A \x\sim\chi_{\rmr(A\Sigma)}^2(\bam^\TT A\bam)$ if 
			\begin{sequation*}
				(1) \Sigma A\Sigma A\Sigma=\Sigma A\Sigma,\quad (2)\bam^\TT A \Sigma A \bam=\bam^\TT A \bam, \quad (3) \Sigma A\Sigma A\bam=\Sigma A\bam;
			\end{sequation*} 

			\item let $A_i\in\bbR^{p\times p}$ be symmetric with rank $k_i$ for $i=1,\ldots,m$.   
			Denote $A=\sum_{i=1}^m A_i$, which is symmetric with rank $k$.
			Then $\x^\TT A_i \x\sim\chi_{k_i}^2 (\bam^\TT A_i\bam)$, 
			$\x^\TT A_i\x$ are pairwise independent and $\x^\TT A \x\sim\chi_{k}^2 (\bam^\TT A \bam)$, iff 
			\begin{enumerate}
				\item[(I)] any two of the following are true: (a) $A_i\Sigma$ idempotent, $\forall i$; (b) $A_i\Sigma A_j=0$, $\forall i<j$; and (c) $A\Sigma$ idempotent; \textbf{OR}
				\item[(II)] (c) is true and (d) $k=\sum_{i=1}^{m}k_i$; \textbf{OR}
				\item[(III)] (c) is true and (e) $A_1\Sigma,\ldots,A_{m-1}\Sigma$ are idempotent and $A_m \Sigma\succeq \0$.    
			\end{enumerate}    
			
			\item (Cochran's theorem) $\x\sim \rmN_p(\0_p,I_p)$ and $A_i$ is symmetric of rank $k_i$, for $i=1,\ldots,m$ with $\sum_{i=1}^{m}A_i=I_p$,
			then $\x^\TT A_i \x \sim \chi_{k_i}^2$ independently iff $\sum_{i=1}^{m}k_i=p$.       
		\end{enumerate}      
	\end{theorem}
\end{thmbox}

\subsection{The noncentral $\chi^2$ and F distribution}\label{sec:noncen_chi2_F}



\section{Asymptotic properties}\label{sec:asym_multi}
\subsection{Asymptotic distributions of sample means and covariance matrices}\label{sec:asym_multi_sampleMeanCov}
Refer to section 1.2.2, \cite{muirhead1982aspects}.
\begin{thmbox}
	\begin{theorem}[CLT for sample means]\label{thm:CLT_multi_sampleMean_iid}
		Let $\x_{1:n}\simiid[\bam,\Sigma]$, then 
		\begin{sequation*}
			\sqrt{n}(\bar{\x}_n-\bam)=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(\x_i-\bam)\dto \rmN_p(\0_p,\Sigma). 
		\end{sequation*} 
	\end{theorem}
\end{thmbox}

\begin{thmbox}
	\begin{theorem}[CLT for sample covariance matrices]\label{thm:CLT_multi_sampleCov_iid}
		Let $\x_{1:n}\simiid[\bam,\Sigma]$ with finite fourth moments, SSCP matrix $\A=\sum_{i=1}^{n}(\x_i-\bar{\x})(\x_i-\bar{\x})^\TT$, and $\S=(n-1)^{-1}\A$. Let $V=\Cov[\vec((\x_1-\bam)(\x_1-\bam)^\TT)]$, then 
		\begin{sequation*}
			\begin{aligned}
				\frac{1}{\sqrt{n}}(\vec(\A)-n\cdot\vec(\Sigma))&\dto \rmN_{p^2}(\0,V), \\
				\sqrt{n-1}(\vec(\S)-\vec(\Sigma)) & \dto \rmN_{p^2}(\0,V).
			\end{aligned}
		\end{sequation*}
	\end{theorem}
\end{thmbox}
Note that $V\in\bbR^{p^2\times p^2}$ is singular as the LHS vectors above have repeated elements. 


\chapter{Linear Models}\label{chap:linear_models}
\section{Linear regression for the full-rank model}\label{sec:LR_full_rank}
Consider the linear model 
\begin{equation}\label{eq:LR_mat}
	\Y_{n}=\X_{n\times p}\bab_{p}+\bave_n, \quad \rmr(\X)=p.
\end{equation}



\section{Ridge regression}\label{sec:ridge_reg}



\section{LASSO}\label{sec:lasso}




\chapter{Generalized Linear Models}\label{chap:GLM}



\chapter{Bayesian Inference}\label{chap:bayes_inference}



\chapter{Structural Equation Model (SEM)}\label{chap:SEM}
Reference: 
\begin{itemize}
	\item CUHK STAT5020 - Topics in multivariate analysis (2025 Spring), by Xin Yuan SONG.
	\item Sik-Yum Lee and Xin-Yuan Song - Basic and advanced
	Bayesian structural equation modeling: With applications in the
	medical and behavioral sciences \cite{lee2012basic}.
\end{itemize}

\section{SEM models}\label{sec:SEM_models}

\noindent \textbf{\underline{Goal}}: to examine the relationships among the variables of interest.

\noindent \textbf{\underline{Approach}}: group observed variables to form \red{\textit{latent variables}} (a combination of several observed variables).
\begin{itemize}
	\item Reduce the number of variables compared to direct regression.
	\item As highly correlated observed variables are grouped into latent variables, the problem induced by multicollinearity is alleviated.
	\item It gives better assessments on the interrelationships of latent constructs.
\end{itemize}

\begin{defbox}
	\begin{definition}[\textbf{Linear SEMs}]\label{def:linearSEM}
		Assume that the \brown{observed variables} $\brown{\y}_p\simiid \rmN_p$ with mean $\bam_p$. Let $\red{\bao}_q$ be \red{latent variables}. $\red{\bao}_q = (\pink{\bet}_{q_1}^\TT,\teal{\bax}_{q_2}^\TT)^\TT$, where $\pink{\bet}_{q_1}$ is the \pink{key outcome
		latent variables}, and $\teal{\bax}_{q_2}$ is the \teal{explanatory latent variables}. Define 
		\begin{equation}\label{eq:linearSEM}
			\begin{aligned}
			\text{(\textbf{Measurement equation} (MeaEq))}\qquad\qquad & \brown{\y}_p = \bam_p + \baL_{p\times q} \red{\bao}_q + \bae_p, \\
			\text{(\textbf{Structural equation} (StEq))}\qquad\qquad &
			\pink{\bet}_{q_1} = \baG_{q_1\times q_2} \teal{\bax}_{q_2} + \bad_{q_1},
			\end{aligned}
		\end{equation}
		where $\baL$ is the unknown \textit{factor loading matrix}, $\baG$ is the unknown matrix of regression
		coeffcients, and $\bae$ and $\bad$ are measurement (residual) errors.   
	\end{definition}
\end{defbox}
The basic SEM is formulated by two components:
\begin{enumerate}
	\item (MeaEq) \red{\textit{Confirmatory factor analysis (CFA)}} model groups the highly correlated observed variables into latent variables. 
	We know the information/structure of $\baL$ (e.g., know $y_1,y_2$ are only affected by $\eta$ and $y_3,y_4$ are affected by $\xi$). 
	\begin{itemize}
		\item \red{\textit{Exploratory factor analysis (EFA)}}: we have $\y$ and just know they have lower-dimensional structure, but don't know the structure of $\baL$ and $q$.   
	\end{itemize}
	It regresses $\y$ on a smaller number of latent variables. 
	\item (StEq) A regression type model, in which $\bet$ is regressed on $\bax$. 
\end{enumerate}




\begin{assbox}
\begin{assumption}[\textbf{Standard linear SEMs}]\label{ass:linearSEM}
For $i=1,\ldots,n$, 
\begin{itemize}
	\item[(A1)] $\bae_i\simiid\rmN[\0_p,\baY_\bae]$, where $\baY_\bae\in\bbR^{p\times p}$
	is diagonal.
	\item[(A2)] $\teal{\bax}_i\simiid\rmN[\0_{q_2},\baF]$,
	where $\baF$ is a general (we can evaluate the correlations of the latent variables). 
	\item[(A3)] $\bad_i\simiid\rmN[\0_{q_1},\baY_{\bad}]$, where $\baY_{\bad}$
	is diagonal.
	\item[(A4)] $\bad_i\indep\bax_i$, and $\bae_i\indep\bao_i,\bad_i.$
\end{itemize} 
\end{assumption}
\end{assbox}
These assumptions imply that 
\begin{sequation*}
	\pink{\bet}_i\simiid\rmN_{q_1}(\0_{q_1},\baG\baF\baG^\TT+\baY_{\bad}),\quad \red{\bao}_i\simiid\rmN_{q}\sbk{\0_{q},\baS_{\red{\bao}}=\begin{bmatrix}
		\baG\baF\baG^\TT+\baY_{\bad} & \baG\baF \\
		\baF\baG^\TT & \baF
	\end{bmatrix}}, \quad \brown{\y}_i\simiid\rmN_{p}(\bam,\baS(\baq)=\baL\baS_{\red{\bao}}\baL^\TT+\baY_{\bae}). 
\end{sequation*}

\noindent\underline{\textbf{Identifiability issue}}: 
The measurement equation is identfied if $\forall \baq_1,\baq_2$, $\mathrm{MeaEq}(\baq_1)=\mathrm{MeaEq}(\baq_2)$ implies $\baq_1=\baq_2$. The structural equation is identfied if $\forall \baq_1,\baq_2$, $\mathrm{StEq}(\baq_1)=\mathrm{StEq}(\baq_2)$ implies $\baq_1=\baq_2$. The SEM is identified if both of its MeaEq and StEq are identfied.
For example, consider \eqref{eq:linearSEM} MeaEq:
\begin{sequation*}
	\y=\bam + (\baL \M) (\M^{-1}\bao) + \bae = \bam + \baL^* \bao^* + \bae, \quad \bao^*\sim\rmN(\0_{q},\M^{-1}\Cov(\bao)\M^{-\TT}).
\end{sequation*}
We have to impose restrictions on $\baL$ and/or $\Cov(\bao)$ such that the only nonsingular $\M=I_q$.    
\begin{itemize}
	\item A simple and common method is using a $\baL$ with the \red{\textit{non-overlapping structure}}, e.g.,
	\begin{sequation*}
		\baL^{\TT}=\begin{bmatrix}1&\lambda_{21}&\lambda_{31}&\lambda_{41}&0&0&0&0&0&0\\0&0&0&0&1&\lambda_{62}&\lambda_{72}&0&0&0\\0&0&0&0&0&0&0&1&\lambda_{93}&\lambda_{10,3}\end{bmatrix}
	\end{sequation*}  
	where $1$'s are fixed to introduce a scale to latent variables.
	\item We can also allow $\lambda_{11}$, $\lambda_{52}$, and/or $\lambda_{83}$ to be unknowns, and fix $\diag(\Cov(\bao))=I_q$, such that $\Cov(\bao)$ is a correlation matrix. But it is not convenient for identifying an SEM with StEq, and induces complication in the Bayesian analysis.  
\end{itemize}


\begin{exbox}
	\begin{example}\label{ex:kidney}\rm
		Study the kidney disease of type 2 diabetic patients. We observe: plasma creatine (PCr), urinary albumin creatinine ratio (ACR), systolic blood pressure (SBP), diastolic blood pressure (DBP), body mass index (BMI), waist hip ratio (WHR), glycated hemoglobin (HbAlc), fasting plasma glucose (FPG). Group 
		\begin{itemize}
			\item \{PCr, ACR\}: `kidney disease (KD)'
			\item \{SBP, DBP\}: `blood pressure (BP)'
			\item \{BMI, WHR\}: `obesity (OB)'
			\item \{HbA1c, FPG\}: `glycemic control (GC)'
		\end{itemize}
		$\y=(\rm PCr, ACR, SBP, DBP, BMI, WHR)^\TT$, $\bao=(\rm KD,BP,OB)^\TT$, $\bet=\rm KD$, $\bax=(\rm BP,OB)^\TT$, $p=6$, $q=3,q_1=1,q_2=2$. Then 
		\begin{sequation*}
			\text{(MeaEq)}\qquad\qquad \begin{bmatrix}\mathrm{PCr}\\\mathrm{ACR}\\\mathrm{SBP}\\\mathrm{DBP}\\\mathrm{BMI}\\\mathrm{WHR}\end{bmatrix}=\begin{bmatrix}\mu_{1}\\\mu_{2}\\\mu_{3}\\\mu_{4}\\\mu_{5}\\\mu_{6}\end{bmatrix}+\begin{bmatrix}\lambda_{11}&0&0\\\lambda_{21}&0&0\\0&\lambda_{32}&0\\0&\lambda_{42}&0\\0&0&\lambda_{53}\\0&0&\lambda_{63}\end{bmatrix}\begin{bmatrix}\mathrm{KD}\\\mathrm{BP}\\\mathrm{OB}\end{bmatrix}+\begin{bmatrix}\epsilon_{1}\\\epsilon_{2}\\\epsilon_{3}\\\epsilon_{4}\\\epsilon_{5}\\\epsilon_{6}\end{bmatrix}.
		\end{sequation*}
		We know that KD is only linked with PCr and ACR.
		\begin{sequation*}
			\text{(StEq)}\qquad\qquad \mathrm{KD}=\gamma_{1}\mathrm{BP}+\gamma_{2}\mathrm{OB}+\delta.
		\end{sequation*}
		Suppose we wish to study the effects of $\bax$ on $\bet=(\mathrm{KD},\eta_A)^\TT$, $q_1=2$,   
		\begin{sequation*}
			\begin{pmatrix}\mathrm{KD}\\\eta_{A}\end{pmatrix}=\begin{pmatrix}0&0\\\pi&0\end{pmatrix}\begin{pmatrix}\mathrm{KD}\\\eta_{A}\end{pmatrix}+\begin{pmatrix}\gamma_{1}&\gamma_{2}\\\gamma_{3}&\gamma_{4}\end{pmatrix}\begin{pmatrix}\mathrm{BP}\\\mathrm{OB}\end{pmatrix}+\begin{pmatrix}\delta\\\delta_{A}\end{pmatrix}.
		\end{sequation*}
	\end{example}
\end{exbox} 

\begin{defbox}
	\begin{definition}[More SEM models]\label{def:moreSEM}  
	Extensions of StEq:
	\begin{itemize}
		\item \red{Basic} StEq: ${\bet}_{q_1} = \baG_{q_1\times q_2} {\bax}_{q_2} + \bad_{q_1}$.
		
		\item \red{An extension}: 
		\begin{sequation*}
			{\bet}_{q_1}=\baP_{q_1\times q_1}{\bet}_{q_1}+\baG_{q_1\times q_2} {\bax}_{q_2}+\bad_{q_1},
		\end{sequation*}
		where $\baP$ is a matrix of unknown coeffcients such that $I_{q_1}-\baP$ is nonsingular and the diagonal elements of $\baP$ are zero. 

		\item Add \red{fixed known covariates} $\d$: 
		\begin{sequation*}
			\bet_{q_1}=\B_{q_1\times r_2}\d_{r_2} + \baP_{q_1\times q_1}\bet_{q_1}+\baG_{q_1\times q_2}\bax_{q_2}+\bad_{q_1}.
		\end{sequation*}
		\item Add \red{nonlinear structure}: 
		\begin{sequation}\label{eq:nonlinearStEq}
			\bet_{q_1}=\B_{q_1\times r_2}\d_{r_2}+ \baP_{q_1\times q_1}\bet_{q_1}+\baG_{q_1\times t}\F_{t}(\bax_{q_2})+\bad_{q_1},
		\end{sequation}
		where $\F(\bax)=(f_1(\bax),\ldots,f_t(\bax))^\TT$ with nonzero, known, and linearly independent differentiable functions $f_1,\ldots,f_t$, $t\geq q_2$. 
		To identity StEq, structures like $\F_1(\bax)=(\xi_1,\xi_2,\xi_1^2,\xi_1^2)^\TT$ and $\F_2(\bax)=(\xi_1,\xi_2,\xi_1\xi_2,0)^\TT$ are not allowed.
		
		\item \red{Combine $\d$ and $\F$}: 
		\begin{sequation}\label{eq:nonlinearStEq_comb}
			\bet_{q_1}=\baP_{q_1\times q_1}\bet_{q_1}+\baL_{\bao,q_1\times t}\G_t(\d,\bax)+\bad_{q_1},
		\end{sequation}
		where $\G(\d\bax)=(g_1(\d,\bax),\ldots,g_t(\d,\bax))^\TT$ is a vector-valued function with nonzero, known, and linearly independent differentiable functions.  
		\eqref{eq:nonlinearStEq} can be obtained by letting $\baL_\bao=(\B,\baG)$ and $\G(\d,\bax)=(\d^\TT,\F(\bax)^\TT)^\TT$.  
	\end{itemize}

	An Extension of MeaEq:
	\begin{itemize}
		\item Add fixed known covariates $\c$ (If $\bam_p$ is included, then $\c=[1,\c_2]^\TT$.)
		\begin{sequation}\label{eq:covaMeaEq}
			\y_{p}=\A_{p\times r_1}\c_{r_1}+\baL_{p\times q}\bao_{q}+\bae_{p}.
		\end{sequation}
	\end{itemize}
	\end{definition}
\end{defbox}
Let $\baL_k^\TT$ be the kth row of $\baL$, and $\baL_k^\TT=(\baL_{k\bet}^\TT,\baL_{k\bax}^{\TT})$ be a partition correspondings to the partition of $\bao=(\bet^\TT,\bax^\TT)^\TT$. 
For StEq \eqref{eq:nonlinearStEq} with $r_2=0$ and MeaEq \eqref{eq:linearSEM},
\begin{sequation*}
	\begin{aligned}
	\bbE(\bax)&=\0_{q_2},\quad \bbE(\bet)=[(I_{q_1}-\baP)^{-1}\baG]\bbE(\F(\bax)),\\
	\bbE(y_k)&=\mu_k+\baL_{k\bet}^\TT[(I_{q_1}-\baP)^{-1}\baG]\bbE(\F(\bax)).
	\end{aligned}
\end{sequation*}   
For StEq \eqref{eq:nonlinearStEq_comb} and MeaEq \eqref{eq:covaMeaEq}, let $\A_k^\TT$ be the kth row of $\A$,
\begin{sequation*}
	\bbE(y_k)=\A_k^\TT\c+\baL_{k\bet}^\TT\bbE(\bet)=\A_k^\TT\c+\baL_{k\bet}^\TT[(I_{q_1}-\baP)^{-1}\baL_{\bao}]\bbE(\G(\d,\bax)).
\end{sequation*}  

\noindent\underline{\textbf{Issues of developing a comprehensive SEM}}:
\begin{enumerate}
	\item Make sure the sample size of $\y$ is large enough to achieve accurate statistical results.
	\item If the size of the proposed SEM and the number of parameters are large, we may encounter diffculties in achieving convergence of the related computing algorithm for obtaining statistical results.
	\item So far, the most general SEM is MeaEq \eqref{eq:covaMeaEq} and StEq \eqref{eq:nonlinearStEq_comb}:
	\begin{sequation*}
		\y=\A\c+\baL\bao+\bae,\qquad \bet=\baP\bet+\baL_{\bao}\G(\d,\bax)+\bad.
	\end{sequation*} 
	It has limitations. As $\G(\d,\bax)$ does not involve any $\eta_k$ in $\bet$, nonlinear terms related to $\eta_k$ cannot be used to predict the other $\eta_{k'}$'s, i.e., $\eta_k$ cannot be accommodated in $\G(\d,\bax)$ and nonlinear effects of $\eta_k$ on $\eta_{k'}$ cannot be assessed.         
	
\end{enumerate}

\section{Bayesian methods for estimating SEM}\label{sec:Bayes_est_SEM}
\underline{\textbf{Motivation}}:
A traditional method is the \textit{covariance structure approach}, which focuses on fitting the covariance structure under the proposed model to the sample covariance matrix $\S$.
\begin{itemize}
	\item In many complex situations, deriving the explicit covariance structure $\Sigma=\Cov(\y)$ or obtaining an appropriate $\S$ is difficult (e.g., when $\exists$ missing data).
\end{itemize}
\underline{\textbf{Advarntages of Bayesian estimates (BE)}}: $\log\bbP(\baq\mid\Y)=\log\bbP(\Y\mid\baq)+\log\bbP(\baq)+\mathrm{const}$
\begin{enumerate}
	\item Methods are based on the first moment properties of $\y$ which are simpler than the second moment properties of $\S$. 
	Hence, it has potential to be applied to more complex situations.

	\item It estimates $\bao$ directly, which cannot be obtained with classical methods.
	
	\item As $n\to \infty$, $\log\bbP(\Y\mid\baq)$ could dominate $\log\bbP(\baq)$, hence the BE have the same optimal properties
	as the MLE. 

	\item It allows the use of genuine prior information for producing better results. 
	With small or moderate sample sizes, $\bbP(\baq)$ plays a more substantial role in BE than $\bbP(\Y\mid\baq)$, and is useful for achieving better results. 
	
	\item It provides more easily assessable statistics for goodness-of-fit and model comparison, and also other useful statistics such as the posterior mean and percentiles. (Don't need to derive the asymptotic distributions.)
	\item It can give more reliable results for small samples.
\end{enumerate}

\subsection{Priors for SEM}\label{sec:prior_SEM}
\noindent \underline{\textbf{Informative prior}}:
\begin{assbox}
	\begin{assumption}[\textbf{Conjugate prior for SEMs}]\label{ass:conj_prior_SEM}
		In developing the Bayesian methods for analyzing SEMs, we usually assign fixed known values to the hyperparameters in the conjugate prior distributions.
		Consider
		\begin{sequation*}
			\begin{aligned}
			\y_i&=\bam+\baL\bao_{i}+\bae_{i}, \\
			\bet_{i}&=\B\d_{i}+ \baP\bet_{i}+\baG\F(\bax_{i})+\bad_{i}=\baL_{\bao}\G(\bao_i)+\bad_i,
			\end{aligned}
	\end{sequation*}
	where $\baL_{\bao}=(\B,\baP,\baG)\in\bbR^{q_1\times (r_2+q_1+t)}$, and $\G(\bao_i)=(\d_i^\TT,\bet_i^\TT,\F(\bax_i)^\TT)^\TT\in\bbR^{r_2+q_1+t}$.     
	Assumption \ref{ass:linearSEM} is satisfied.
	${\bax}_i\simiid\rmN[\0_{q_2},\baF]$, $\bae_i\simiid\rmN[\0_p,\baY_\bae=\diag(\psi_{\bae k})]$,and $\bad_i\simiid\rmN[\0_{q_1},\baY_{\bad}=\diag(\psi_{\bad k})]$.
	\begin{itemize}
		\item Prior (conjugate) for $\baq_\y=(\bam,\baL,\baY_{\bae})$: let $\baL_k^\TT$ be the kth row of $\baL$,
		\begin{equation*}
			\begin{aligned}
			\psi_{\bae k}&\sim\IG(\alpha_{0 \bae k}, \beta_{0 \bae k}), \quad [\baL_k\mid\psi_{\bae k}]\sim \rmN_q(\baL_{0k},\psi_{\bae k}\H_{0\y k}), \ k=1,\ldots,p,\\
			\bam&\sim\rmN_p(\bam_0,\baS_0).
			\end{aligned}
		\end{equation*}
		\item Prior (conjugate) for $\baq_\bao=(\baL_\bao,\baY_{\bad},\baF)$: let $\baL_{\bao k}^\TT$ be the kth row of $\baL_{\bao}$, $\baY_{\bad}$,
		\begin{equation*}
			\begin{aligned}
				\baF&\sim \IW_{q_2}(\R_0^{-1},\rho_0), \text{ or } \baF^{-1}\sim\mathrm{W}_{q_2}(\R_0,\rho_0)  \\
				\psi_{\bad k}&\sim \IG(\alpha_{0 \bad k},\beta_{0 \bad k}),\quad [\baL_{\bao k}\mid \psi_{\bad k}]\sim \rmN_{r_2+q_1+t}(\baL_{0\bao k},\psi_{\bad k}\H_{0\bao k}), \ k=1,\ldots,q_1.
			\end{aligned}
		\end{equation*} \
		\item Assume the prior $\baq_\y\indep\baq_\bao$. 
	\end{itemize}
	\end{assumption}
\end{assbox}



{{Hyperparameter selection}}: If we have good prior information about a parameter, select the prior
distribution with a small variance. E.g., 
\begin{itemize}
	\item if $\baL_k\approx\baL_{0k}$, then $\H_{0yk}=0.5 I_q$. If not, select the prior with a larger variance;
	\item since $\epsilon_{ik}\sim \rmN(0,\psi_{\bae k})$, if the variation is small, $\psi_{\bae k}$ is small, then choose small $\bbE(\psi_{\bae k})=\beta_{0\bae k}/(\alpha_{0\bae k}-1)$ and $\Var(\psi_{\bae k})=\beta_{0\bae k}^2/\{(\alpha_{0\bae k}-1)^2(\alpha_{0\bae k}-2)\}$;
	\item if $\baF\approx \baF_{0}$, since $\bbE(\baF)=\R_0^{-1}/(\rho_0-q_2-1)$, choose $\R_0^{-1}=(\rho_0-q_2-1)\baF_0$.   
\end{itemize}

\noindent\textbf{\underline{Noninformative prior (Jeffrey)}}: If information is not available and the sample size is small, 
\begin{sequation*}
	\begin{aligned}
		&\bbP(\baL,\baY_{\bae})\propto \bbP(\psi_{\bae1},\cdots,\psi_{\bae p})\propto\prod_{k=1}^{p}\psi_{\bae k}^{-1},\quad 
		\bbP(\baL_{\bao},\baY_{\bad})\propto \bbP(\psi_{\bad1},\cdots,\psi_{\bad q_{1}})\propto\prod_{k=1}^{q_{1}}\psi_{\bad k}^{-1},\\
		&\bbP(\baF)\propto|\baF|^{-(q_2+1)/2}.\end{aligned}
\end{sequation*}
If the sample size is large, can use a portion of the data to estimate $\baL_{0k}$, $\baL_{0\bao k}$ and $\baF_0$ with noninformative priors. If the sample size is moderate, can use the same data twice.  


\subsection{Bayesian estimation using MCMC}
\textbf{\underline{Model}}: Linear SEM with fixed covariates without intercept:
\begin{equation*}
	\begin{aligned}
		\y_i&=\baL\bao_i+\bae_i, \\
		\bet_i&=\B\d_i+\baP\bet_i+\baG\bax_i+\bad_i=\baL_{\bao}\v_i+\bad_i,
	\end{aligned}
\end{equation*}
where $\baL_{\bao}=(\B,\baP,\baG)\in\bbR^{q_1\times (r_2+q_1+q_2)}$, and $\v_i=(\d_i^\TT,\bet_i^\TT,\bax_i^\TT)^\TT\in\bbR^{r_2+q_1+q_2}$. 
That is, assume $\bam=\0_p$ and $\F(\bax_i)-\bax_i$. 

Denote data $\Y=(\y_1,\ldots,\y_n)=(\Y_1,\ldots,\Y_p)^\TT\in\bbR^{p\times n}$, $\V=(\v_1,\ldots,\v_n)=(\V_1,\ldots,\V_{r_2+q_1+q_2})^\TT$, $\baX_{k}=(\eta_{1k},\cdots,\eta_{nk})^\TT$ for $k=1,\ldots,q_1$, matrix of latent variables $\baO=(\bao_1,\ldots,\bao_n)\in\bbR^{q\times n}$, $\baO_1=(\bet_1,\ldots,\bet_n)$, $\baO_2=(\bax_1,\ldots,\bax_n)$, and 
\begin{sequation*}
	\baq=(\baL,\B,\baP,\baG,\baF,\baY_\bae,\baY_\bad)=(\underbrace{\baL,\baY_\bae}_{\baq_\y},\underbrace{\baL_\bao,\baF,\baY_\bad}_{\baq_{\bao}}).
\end{sequation*}   
\begin{thmbox}
	\begin{proposition}\label{prop:post_linearSEM}
		The above model has the following posterior distributions:
		\begin{enumerate} 
			\item Conditional distribution $\red{\bbP(\baO\mid\Y,\baq)}=\prod_{i=1}^n\bbP(\bao_i\mid\y_i,\baq)\propto\prod_{i=1}^{n}\bbP(\bao_i\mid\baq)\bbP(\y_i\mid\bao_i,\baq)$, where 
			\begin{equation*}
				\begin{aligned}
					[\bao_i\mid\baq]&\sim\rmN_q(\bam_{\bao_i},\baS_\bao), \quad 
					[\y_i\mid\bao_i,\baq]\sim\rmN_{p}(\baL\bao_i,\baY_\bae),\\
					[\bao_i\mid\y_i,\baq]&\sim\rmN_q({\baS^{*}}^{-1}(\baS_{\bao}^{-1}\bam_{\bao_i}+\baL^\TT\baY_{\bae}^{-1}\y_i),{\baS^{*}}^{-1})
				\end{aligned}
			\end{equation*}
			where 
			\begin{equation*}
				\begin{aligned}
					\baP_0&=I_{q_1}-\baP, \ \bam_{\bao_i}=\begin{pmatrix}\baP_0^{-1}\B \d_i\\\0_{q_2}\end{pmatrix}, \ \baS_\bao=\begin{bmatrix}\baP_0^{-1}(\baG\baF\baG^\TT+\baY_\bad)\baP_0^{-\TT}&&\baP_0^{-1}\baG\baF\\\baF\baG^\TT\baP_0^{-\TT}&&\baF\end{bmatrix},\\
					\baS^{*}&=\baS_{\bao}^{-1}+\baL^\TT \baY_{\bae}^{-1}\baL.
				\end{aligned}
			\end{equation*}
			\item Assume all elements of $\baL_k$ and $\baL_\bao$ are unknown, conditional distribution $\red{\bbP(\baq\mid\Y,\baO)}=\bbP(\baq_\y\mid\Y,\baO)\bbP(\baq_\bao\mid\Y,\baO)$, where we can show $[\baL_k,\psi_{\bae k}\mid \Y,\baO] \indep$ and $[\baL_{\bao k},\psi_{\bad k}\mid \baO]\indep$, and  
			\begin{sequation*}
				\begin{aligned}
					\bbP(\baq_\y\mid\Y,\baO)&\propto \prod_{k=1}^{p} \bbP(\baL_k,\psi_{\bae k}\mid \Y,\baO),\\
					\bbP(\baq_\bao\mid\Y,\baO)&\propto \mbk{\prod_{k=1}^{q_1}\bbP(\baL_{\bao k},\psi_{\bad k}\mid \baO)}\bbP(\baF\mid\baO_2)
				\end{aligned}
			\end{sequation*} 
			where 
			\begin{sequation*}
				\begin{aligned}
					\bbP(\baL_k,\psi_{\bae k}^{-1}\mid\Y,\baO)&\propto \rmN_q(\a_k,\psi_{\bae k}\A_k)\cdot\Ga(n/2+\alpha_{0\bae k},\beta_{\bae k}), \\
					\bbP(\baL_{\bao k},\psi_{\bad k}^{-1}\mid \baO)&\propto\rmN_{r_2+q_1+q_2}(\a_{\bao k},\psi_{\bad k}\A_{\bao k})\cdot\Ga(n/2+\alpha_{0\bad k}, \beta_{\bad k}),\\
					[\baF\mid\baO_2]&\sim \IW_{q_2}[(\baO_2\baO_2^\TT+\R_0^{-1}),n+\rho_0].
				\end{aligned}
			\end{sequation*}
			where 
			\begin{equation*}
				\begin{aligned}
					\A_k&=(\H_{0yk}^{-1}+\baO\baO^\TT)^{-1}, \ \a_k=\A_k(\H_{0yk}^{-1}\baL_{0k}+\baO\Y_k), \\
					\beta_{\bae k}&=\beta_{0\bae k}+\frac{1}{2}(\Y_k^{\TT}\Y_{k}-\a_{k}^{\TT}\A_{k}^{-1}\a_{k}+\baL_{0k}^{\TT}\H_{0yk}^{-1}\baL_{0k}), \\
					\A_{\bao k}&=(\H_{0\bao k}^{-1}+\V_k\V_k^\TT)^{-1},\ \a_{\bao k}=\A_{\bao k}(\H_{0\bao k}^{-1}\baL_{0\bao k}+\V_k\baX_k), \\
					\beta_{\bad k}&=\beta_{0\bad k}+\frac{1}{2}(\baX_{k}^{\TT}\baX_{k}-\a_{\bao k}^{\TT}\A_{\bao k}^{-1}\a_{\bao k}+\baL_{0\bao k}^{\TT}\H_{0\bao k}^{-1}\baL_{0\bao k}).
				\end{aligned}
			\end{equation*}
		\end{enumerate}
	\end{proposition}
\end{thmbox}

\begin{remark}\label{rmk:post_SEM}
	For general nonlinear SEMs with MeaEq \eqref{eq:covaMeaEq} and StEq \eqref{eq:nonlinearStEq_comb}, we can defind $\u=[\c^\TT, \bao^{\TT}]^\TT\in\bbR^{r_1+q}$ and use similar procedure to derive the full conditional distributions. 
	But by the nonlinear structure $\G(\bao)$, $\bbP(\baO\mid\Y,\baq)$ may not have closed form like normal, while $\bbP(\baq\mid\Y,\baO)$ is not affected and keeps normal-Gamma.
	To handle fixed parameters, see Appendix 3.3 in \cite{lee2012basic} and Sec 4.3.1 and Appendix 4.3 in \cite{lee2007structural}. Also see STAT5020 HW1 Q3.
\end{remark}


\section{Bayesian model comparison}\label{sec:SEM_model_comp}



\section{Hierarchical and Multisample
Data}
\begin{defbox}
	\begin{definition}[Two-level nonlinear SEM with mixed type variables]
	Consider a collection of $p$-variate random vectors $\u_{gi}$, $i=1,\ldots,N_g$, nested within groups $g=1,\ldots,G$.   
	\begin{align*}
		\text{(Within-groups) }\u_{gi}&=\v_{g}+\baL_{1g}\bao_{1gi}+\bae_{1gi},\quad g=1,\cdots,G,\quad i=1,\cdots,N_{g},\\
		\text{(Between-groups) }\v_g&=\bam+\baL_2\bao_{2g}+\bae_{2g},\quad g=1,\ldots,G
	\end{align*}
	where $\baL_{1g}\in\bbR^{p\times q_1}$, $\bae_{1gi}\in\bbR^{q_1}$, $\bae_{1gi}\in\bbR^{p}\sim\rmN(\0,\baY_{1g})$ independent of $\bao_{1gi}$, where $\baY_{1g}$ is diagonal. Then 
	\begin{equation*}
		\mathbf{u}_{\mathrm{gi}}=\mathbf{\mu}+\mathbf{\Lambda}_{2}\mathbf{\omega}_{2\mathrm{g}}+\mathbf{\epsilon}_{2\mathrm{g}}+\mathbf{\Lambda}_{1\mathrm{g}}\mathbf{\omega}_{1\mathrm{gi}}+\mathbf{\epsilon}_{1\mathrm{gi}}.
	\end{equation*}  
	The SEM is 
	\begin{equation*}
		\begin{aligned}&\eta_{1gi}=\Pi_{1g}\eta_{1gi}+\Gamma_{1g}\mathbf{F}_1(\xi_{1gi})+\delta_{1gi},\\&\eta_{2g}=\Pi_2\eta_{2g}+\Gamma_2\mathbf{F}_2(\xi_{2g})+\delta_{2g},\end{aligned}
	\end{equation*}
	\end{definition}
\end{defbox}



\bibliographystyle{abbrv}
\bibliography{mybib}
%%% end of doc
\end{document}