\documentclass[10pt,a4paper]{book}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{scribe}
\usepackage{listings}
\usepackage{eufrak}

\newenvironment{hints}{\textbf{Hints.}}{}

\Scribe{Zhuohua Shen}
\Lecturer{}
\LectureNumber{}
\LectureDate{Oct 2024}
\LectureTitle{Statistical Inference}

\lstset{style=mystyle}

\begin{document}
	\MakeScribeTop
	\tableofcontents

%#############################################################
%#############################################################
%#############################################################
%#############################################################

\chapter{Preliminary}\label{chap:premi}
\section{Distributions}
\subsection{$\chi^2$ distribution}
Let $X\sim\chi_k^2$, then if $m>-k/2$, the moment $\bbE(X^m)$ exists and is equal to:
\begin{sequation}\label{eq:chi2_moment}
	\bbE(X^m)=2^m\frac{\Gamma\left(\frac{k}{2}+m\right)}{\Gamma\left(\frac{k}{2}\right)}.
\end{sequation}  
See \href{https://statproofbook.github.io/P/chi2-mom.html}{[link]} for the proof.

\chapter{Statistical inference fundamentals}\label{chap:stat-inf}

References: most of the contents are from the undergraduate course STA3020 (by Prof. Jianfeng Mao in 2022-2023 T1, and Prof. Jiasheng Shi in 2023-2024 T2) and postgraduate course STAT5010 (by Kin Wai Keith Chan in 2024-2025 T1), with main textbook Casella and Berger \cite{casella2002statistical} 


\section{Statistical Models}
See Chapter 3 of \cite{casella2002statistical}. Suppose $X_i\simiid \bbP_*$, where $\bbP_*$ refers to the unknown \red{data generating process} (DGPg), we find $\widehat{\bbP}\approx \bbP_*$. A \red{statistical model} is a set of distributions $\scrF=\{\bbP_{\theta}:\theta\in\Theta\}$, where $\Theta$ is the \red{parameter space}. A \red{parametric model} is the model with $\dim(\Theta)<\infty$, while a \red{nonparametric model} satisfies $\dim(\Theta)=\infty$. 

\begin{defbox}{Exponential family}
	\begin{definition}\label{def:exp-family}
		A k-dimensional \red{exponential family} (EF) $\scrF=\{f_\theta:\theta\in\Theta\}$ is a model consisting of pdfs of the form
		\begin{equation}\label{eq:exp-family-pdf}
			f_\theta(x)=c(\theta)h(x)\exp\left\{\sum_{j=1}^k\eta_j(\theta)T_j(x)\right\}
		\end{equation}
	\end{definition}
	where $c(\theta),h(x)\geq 0$, $\Theta=\{\theta:c(\theta)\geq 0, \eta_j(\theta) \text{ being well defined for } 1\leq j\leq k\}$. Let $\eta_j=\eta_j(\theta)$, the \red{canonical form} is 
	\begin{equation}\label{eq:exp-family-can}
		f_\eta(x)=b(\eta)h(x)\exp\left\{\sum_{j=1}^k\eta_jT_j(x)\right\},
	\end{equation}
	\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
		\item $k$-dim \red{natural exponential family} (NEF): $\scrF'=\{f_{\eta}:\eta\in \Xi\}$; 
		\item \red{natural parameter} $\eta=(\eta_1,\ldots,\eta_k)^\TT$;
		\item \red{natural parameter space}: $\Xi=\{\eta\in\mathbb{R}^k:0<b(\eta)<\infty\}$; 
		\item the NEF $\scrF'$ is of \red{full rank} if $\Xi$ contains an open set in $\bbR^k$;
		\item the EF is a \red{curved exponential family} if $p=\dim(\Theta)<k$.
	\end{itemize}
\end{defbox}
\textbf{Properties of EF}:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
	\item Let $X\sim f_{\eta}$, where $\eta\in\Xi$ such that (i) $f_{\eta}$ is of the form \eqref{eq:exp-family-can} with $B(\eta)=-\log b(\eta)$, and (ii) $\Xi$ contains an open set in $\bbR^k$. Then, for $j,j'=1,\ldots,k$, $\bbE\{T_j(X)\}={\partial B(\eta)}/{\partial\eta_j}$ and $\Cov\{T_j(X),T_{j'}(X)\}={\partial^2B(\eta)}/\sbk{\partial\eta_j\partial\eta_{j'}}$.  
	\item \red{Stein's identity}:
\end{itemize}

\begin{defbox}{Location-scale family}
	\begin{definition}\label{def:ls-family}
		Let $f$ be a density. 
		\begin{itemize}
			\item A \red{location-scale family} is given by $\scrF=\{f_{\mu,\sigma}:\mu \in\bbR, \sigma\in\bbR^{++}\}$, where $f_{\mu,\sigma}(x)= f\left({(x-\mu)}/\sigma\right)/\sigma$.
			\item \red{location parameter}: $\mu$; \red{scale parameter}: $\sigma$; \red{standard density}: $f$;
			\item A \red{location family} is $\scrF=\{f_{\mu,1}:\mu\in\bbR\}$.
			\item A \red{scale family} is $\scrF=\{f_{0,\sigma}:\sigma\in\bbR^{++}\}$ 
		\end{itemize}
	\end{definition}
\end{defbox}
\textbf{Representation}: $X=\mu + \sigma Z$, $Z\sim f_{0,1}(\cdot)$. 
\begin{itemize}
	\item See some examples in Example 3.9, Keith's note 3, and Table 1 in Shi's note L1. 
	\item Transform between location parameter and scale parameter by taking log.
\end{itemize}


\begin{defbox}{Identifiable family}
	\begin{definition}\label{def:id-family}
		If $\forall \theta_1,\theta_2\in\Theta$ that    
		\begin{equation*}
			\theta_1\neq\theta_2\quad\Rightarrow\quad f_{\theta_1}(\cdot)\neq f_{\theta_2}(\cdot),
		\end{equation*}
		then $\scrF$ is said to be an \red{identifiable family}, or equivalently $\theta\in\Theta$ is \red{identifiable}. 
	\end{definition}
\end{defbox}

A typical feature of non-identifiable EF is that $p=\dim(\Theta)>k$. Typically,
\begin{itemize}
	\item $p<k$, curved (must).
	\item $p=k$, of full rank.
	\item $p>k$, non-identifiable.  
\end{itemize} 

\section{Principles of Data Reduction}\label{sec:prin-data-reduce}
\red{Statistics:} $T=T(X_{1:n})$, a function of $X_{1:n}$ and free of any unknown parameter.  

\subsection{Sufficiency Principle}\label{sec:prin-data-reduce-suff}
\textbf{Sufficiency principle}: If $T=T(X_{1:n})$ is a ``sufficient statistics'' for $\theta$, then any inference on $\theta$ will depend on $X_{1:n}$ only through $T$.

\begin{defbox}{Sufficient, minimal sufficient, ancillary, and complete statistics}
	\begin{definition}\label{def:stat-SS-MSS-ANS-CS}
		Suppose $X_{1:n}\simiid \bbP_{\theta}$, where $\theta\in\Theta$. Let $T=T(X_{1:n})$ be a statistic. Then $T$ is \red{sufficient} (SS) for $\theta$
		\begin{itemize}
			\item[$\Leftrightarrow$] (def) $[X_{1:n}\mid T=t]$ is free of $\theta$ for each $t$.
			\item[$\Leftrightarrow$] (technical lemma) $T(x_{1:n})=T(x_{1:n}')$ implies that $f_{\theta}(x_{1:n})/f_{\theta}(x_{1:n}')$ is free of $\theta$.
			\item[$\Leftrightarrow$] (Neyman-Fisher factorization theorem) $\forall\theta\in\Theta$, $x_{1:n}\in\scrX^n$, $f_\theta(x_{1:n})=A(t,\theta)B(x_{1:n})$.
		\item[$\Leftrightarrow$] Define $\Lambda(\theta',\theta''\mid x_{1:n}):=f_{\theta'}(x_{1:n})/f_{\theta''}(x_{1:n})$. $\forall \theta',\theta''\in\Theta$, $\exists$ function $C_{\theta',\theta''}$ such that $\Lambda(\theta',\theta''\mid x_{1:n})=C_{\theta',\theta''}(t)$, for all $x_{1:n}\in\scrX^n$ where $t=T(x_{1:n})$.    
		\end{itemize}
	$T$ is \red{minimal sufficient} (MSS) for $\theta$
	\begin{itemize}
		\item[$\Leftrightarrow$] (def) (1) $T$ is a SS for $\theta$; (2) $T=g(S)$ for any other SS $S$.
		\item[$\Leftrightarrow$] (1) $T$ is a SS for $\theta$; (2) $S(x_{1:n})=S(x_{1:n}')$ implies $T(x_{1:n})=T(x_{1:n}')$ for any SS $S$.   
		\item[$\Leftrightarrow$] (Lehmann-Scheffé theorem) $\forall x_{1:n},x_{1:n}'\in\scrX^n$, $f_{\theta}(x_{1:n})/f_{\theta}(x_{1:n}')$ is free of $\theta$ $\Leftrightarrow$ $T(x_{1:n})=T(x_{1:n}')$.    
	\end{itemize}
	$A=A(X_{1:n})$ is \red{ancillary} (ANS) if the distribution of $A$ does not depend on $\theta$. 

	\noindent $T$ is \red{complete} (CS) if $\forall\theta\in\Theta$, $\bbE_{\theta}g(T)=0$ implies $\forall\theta\in\Theta$, $\bbP_{\theta}\{g(T)=0\}=1$. 
	\end{definition}
\end{defbox}
\noindent\textbf{Properties}
\begin{itemize}
	\item (Transformation) If $T=r(T')$, then (i) $T$ is SS $\Rightarrow$ $T'$ is SS; (ii) $T'$ is CS $\Rightarrow$ $T$ is CS; (iii) $r$ is one-to-one, then if one is SS/MSS/CS, then the another is.    
	\item (\red{Basu's Lemma}) $X_i\simiid\bbP_\theta$, $A$ is ANS and $T$ s CSS, then $A \indep T$.
	\item (\red{Bahadur's theorem}) $X_i\simiid\bbP_\theta$, if an MSS exists, then any CSS is also an MSS.
	\begin{itemize}
		\item Then if a CSS exists, then any MSS is also a CSS $\Rightarrow$ CSS=MSS.
		\item \red{All or nothing}: start with MSS $T$, check whether $T$ is CS. (i) Yes, it is both CSS and MSS, then the set of MSS=CSS; (ii) No, there is no CSS at all.  
	\end{itemize}
	\item (Exp-family) If $X_i\simiid f_{\eta}$ in \eqref{eq:exp-family-can}, then $T=(\sum_{i=1}^nT_1(X_i),\ldots,\sum_{i=1}^nT_k(X_i))$ is a SS, called \red{natural sufficient statistic}. If $\Xi$ contains an open set in $\bbR^k$ (i.e., $\scrF'$ is of full rank), then $T$ is MSS and CSS. 
\end{itemize}

\noindent\textbf{Proof techniques}
\begin{itemize}
	\item Prove $T$ is not sufficient for $\theta$: show if $\exists x_{1_n}, x_{1:n}'\in\calX^n$ and $\theta',\theta''\in\Theta$, such that $T(x_{1:n})=T(x_{1:n}')$ and $\Lambda(\theta',\theta''\mid x_{1:n})\neq \Lambda(\theta',\theta''\mid x_{1:n}')$.  
	\item Prove $A$ is an ANS: consider location-scale representation.
	\item Prove $T$ is a CS: use definition or take $\rmd\bbE_{\theta}g(T)/\rmd\theta=0$. 
	\item Disprove $T$ is CS: 
	\begin{itemize}
		\item Construct an ANS $S(T)$ based on $T$, then $\bbE S(T)$ is free of $\theta$, then $g(T)=S(T)-\bbE S(T)$ is free of $\theta$ but $g(T)\neq 0$ w.p.1. 
		\item (Cancel the 1st moment) Find two unbiased estiamtors for $\theta$ as a function of $T$. E.g., $X_1,X_2\simiid \mathrm{N}(\theta,\theta^2)$, $T=(X_1,X_2)$, $g(T)=X_1-X_2\sim\mathrm{N}(0,2\theta^2)$. 
	\end{itemize}
	
\end{itemize}


\begin{remark}\label{rmk:SS-MSS-ANS-CS}
	\begin{itemize}
		\item ANS $A$ is useless on its own, but useful together with other information. 
		\item $\bbP(A(\X)\mid \theta)$ is free of $\theta$, but for non-SS $T$, $\bbP(A(\X)\mid T(\X))$ is not necessarily free of $\theta$. 
	\end{itemize}
\end{remark}

\subsection{Likelihood principle}\label{sec:prin-data-reduce-lik}

\section{M-estimation}\label{sec:M_est}

\begin{defbox}{Estimating equation}
	\begin{definition}\label{def:est_eq}
		Let the unknown $\b\in\bbR^p$. Define the \red{\textit{estimating equation}}
		\begin{sequation}\label{eq:est_eq}
			\bar{\m}(W,\b)=\frac{1}{n}\sum_{i=1}^n \m(w_i,\b)=\0_p,
		\end{sequation}
		where $\m(\cdot,\cdot)\in\bbR^p$, and $W=\{w_i\}_{i=1}^n$ are the observed data. Denote $\hat{\bab}$ and $\bab$ the solution of $\bar{\m}(W,\b)=\0_p$ and $\bbE\{\bar{\m}(W,\b)\}=\0_p$, respectively:
		\begin{salign*}
			\bar{\m}(W,\hat{\bab})&=\0_p,\\
			\bbE\{\bar{\m}(W,\bab)\}&=\0_p.
		\end{salign*}  
	\end{definition}
\end{defbox}
Under mild regularity conditions, $\hat{\bab}\pto\bab$ and $\sqrt{n}(\hat{\bab}-\bab)\dto\rmN_p(\0_p,\Sigma)$.  

\subsection{Asymptotic properties}\label{sec:M_est_asym}
\begin{thmbox}{Asymptotic properties of M-estimator under with iid data}
	\begin{theorem}\label{thm:M_est_asym_iid}
		Assume that $W=\{w_i\}_{i=1}^{n}$ are iid with the same distribution as $w$. The true parameter $\bab\in\mathbb{R}^p$ is the unique solution of
		$$\bbE\{\m(w,\b)\}=\0_p,$$
		and the estimator $\hat{\bab}\in\mathbb{R}^p$ is the solution of
		$$
		\bar{\m}(W,\b)=\0_p.$$
		Under some regularity conditions,
		$$\sqrt{n}\left(\hat{\bab}-\bab\right)\dto \rmN_p(\0_p,B^{-1}MB^{-\mathrm{T}}),$$
		where
		$$
		B=-\frac{\partial \bbE\{\m(w,\bab)\}}{\partial \b^\TT},\quad M=\bbE\{\m(w,\bab)\m(w,\bab)^\TT\}.$$
	\end{theorem}
\end{thmbox}

\begin{thmbox}{Asymptotic properties of M-estimator under with independent data}
	\begin{theorem}\label{thm:M_est_asym_indpt}
		Assume that $W=\{w_i\}_{i=1}^{n}$ are independent observations. The true parameter $\bab\in\mathbb{R}^p$ is the unique solution of
		$$\bbE\{\bar{\m}(W,\b)\}=\0_p,$$
		and the estimator $\hat{\bab}\in\mathbb{R}^p$ is the solution of
		$$
		\bar{\m}(W,\b)=\0_p.$$
		Under some regularity conditions,
		$$\sqrt{n}\left(\hat{\bab}-\bab\right)\dto \rmN_p(\0_p,B^{-1}MB^{-\mathrm{T}}),$$
		where
		$$
		B=-\lim_{n\to \infty}\frac{1}{n}\sum_{i=1}^{n}\frac{\partial \bbE\{\m(w_i,\bab)\}}{\partial \b^\TT},\quad M=\lim_{n\to \infty}\frac{1}{n}\sum_{i=1}^{n}\Cov\{\m(w_i,\bab)\}.$$
	\end{theorem}
\end{thmbox}

\begin{egbox}{MLE}
	\begin{example}\label{eg:M_est_MLE}
		Suppose $y_1,\ldots,y_n\simiid f(y\mid\baq)$, $\baq\in\bbR^p$. 
		The MLE satisfies the following estimating equation 
		\begin{sequation*}
			\bbE\cbk{
				\frac{\partial \log f(y\mid\baq)}{\partial \baq}
			}=\0_p,
		\end{sequation*}
		which is Bartlett's first identity. Under regularity conditions, $\sqrt{n}(\hat{\baq}-\baq)\dto\rmN_p(\0_p,B^{-1}MB^{-1})$, where
		\begin{salign*}
			B&=I(\baq)=-\frac{\partial}{\partial\baq^\TT}\bbE\sbk{
				\frac{\partial \log f(y\mid\baq)}{\partial \baq}
			}=\bbE\cbk{
				-\frac{\partial^2 \log f(y\mid\baq)}{\partial \baq \partial \baq^\TT}
			}, \\
			M&=J(\baq)=\bbE\cbk{
				\frac{\partial \log f(y\mid\baq)}{\partial \baq}
				\frac{\partial \log f(y\mid\baq)}{\partial \baq^\TT}
			}.
		\end{salign*} 
		If the model is correct, Bartlett’s second identity ensures that $I(\baq)=J(\baq)$, and therefore $\sqrt{n}(\hat{\baq}-\baq)\dto\rmN_p(\0_p,I(\baq)^{-1})$  
	\end{example}
\end{egbox}

\chapter{Multivariate Inference Fundamentals}\label{chap:multi}
Reference: 
\begin{itemize}
	\item Robb J. Muirhead - Aspects of multivariate statistical theory \cite{muirhead1982aspects}.
	\item CUHK STAT4002 - Applied Multivariate Analysis (2023 Spring), by Zhixiang Lin.
	\item CUHK STAT5030 - Linear Models (2025 Spring), by Yuanyuan Lin.
	\item Peng Ding - Linear Model and Extensions.
	\item Ronald Christensen - Plane answers to
	complex questions: the theory of linear models \cite{christensen2002plane}.
\end{itemize}

\section{Random vectors and distributions}\label{sec:random_vector}
\begin{defbox}{}
	\begin{definition}\label{def:random_vector_moments}
		Let $\x=(x_1,\ldots, x_p)^\TT\in\bbR^p$ be a random vector, 
		\begin{itemize}
			\item \red{Mean} $\bbE \x = \bam=(\bbE x_1,\ldots,\bbE x_p)^\TT=(\mu_j)$.
			\item \red{Covariance matrix} $\Var(\x)=\Cov(\x)=\Sigma=\bbE[(\x-\bbE\x)(\x-\bbE\x)^\TT]=\bbE \x\x^\TT - \bbE \x \bbE \x^\TT = (\sigma_{ij})$, $\Sigma\succeq \0$.
			\item \red{Correlation matrix} $R=D^{-1/2}\Sigma D^{-1/2}$, where $D=\diag(\sigma_{11},\ldots,\sigma_{pp})$. We have $R_{ij}=\rho_{ij}=\sigma_{ij}/(\sqrt{\sigma_{ii}}\sqrt{\sigma_{jj}})$.
			\item If $\y\in\bbR^{q}$ random vector, then $\Cov(\x,\y)=\bbE[(\x-\bbE\x)(\y-\bbE\y)^\TT]=\bbE\x\y^\TT-\bbE\x\bbE\y^\TT\in\bbR^{p\times q}$.  
			\item \red{MSE} $\MSE(\hat{\bab};\bab)=\bbE \littlenorm{\hat{\bab}-\bab}^2=\littlenorm{\bbE\hat{\bab}-\bab}^2+\tr[\Var(\hat{\bab})]$. 
		\end{itemize} 
		If $\Z=(z_{ij})\in\bbR^{p\times q}$ is a random matrix,
		\begin{itemize}
			\item $\bbE\Z=(\bbE z_{ij})$. 
		\end{itemize} 
	\end{definition}
\end{defbox}

\begin{thmbox}{}
	\begin{proposition}\label{prop:random_vector_moments}
		Let $\x\in\bbR^p$ be a random vector, $\a,\b\in\bbR^p$ be vectors, $A\in\bbR^{r_1\times p},B\in\bbR^{r_2\times p}$ be matrices,  
		\begin{itemize}
			\item $\bbE \a^\TT\x=\a^\TT \bbE\x$, $\Var(\a^\TT \x)=\a^\TT\Sigma\a$, and $\Cov(\a^\TT\x,\b^\TT\x)=\a^\TT \Sigma\b$.
			\item $\bbE A\x=A\bbE \x$, $\Var(A\x)=A\Sigma A^\TT$, and $\Cov(A\x,B\x)=A\Sigma B^\TT$.
			\item If $\y=A\x+\b$, where $A\in\bbR^{q\times p}$, $\b\in\bbR^{q}$, then $\bam_\y=A \bam_\x+\b$ and $\Sigma_\y=A\Sigma_\x A^\TT$.    
			\item $\bbE(\x^\TT A\x)=\tr(A\Sigma)+\bam^\TT A\bam$.  
		\end{itemize}  
		Let $\Z\in\bbR^{p\times q}$ be a random matrix, $B\in\bbR^{m\times p}$, $C\in\bbR^{q\times n}$, and $D\in\bbR^{m\times n}$ constants, then 
		\begin{itemize}
			\item $\bbE (B\Z C+D) = B \bbE (\Z) C+D$. 
		\end{itemize}   
	\end{proposition}
\end{thmbox}
\begin{itemize}
	\item The $\Sigma\in\bbR^{p\times p}$ is a covariance matrix (i.e., $\Sigma=\Cov(\x)$ for some random vector $\x\in\bbR^p$) iff $\Sigma\succeq \0$.
	\begin{itemize}
		\item \small $(\Leftarrow)$: suppose $\rmr(\Sigma)=r\leq p$, write full rank decomposition $\Sigma=C C^\TT$, $C\in\bbR^{p\times r}$. Let $\y\sim[\0_r,I_r]$, then $\Cov(C\y)=\Sigma$.    
	\end{itemize}
	\item If $\Sigma$ is not PD, then $\exists \a\neq\0_p$ s.t. $\Var(\a^\TT \x)=0$ so w.p.1., $\a^\TT \x=k$, i.e., $\x$ lies in a hyperplane.     
\end{itemize}

\begin{thmbox}{}
	\begin{theorem}\label{thm:rv_uni_linearFun}
		If $\x\in\bbR^p$ random, then its distribution is uniquely determined by the distributions of $\a^\TT \x$, $\forall \a\in\bbR^p$.  
	\end{theorem}
\end{thmbox}
The proof uses the fact that a distribution in $\bbR^p$ is uniquely determined by its ch.f., see Theorem 1.2.2. \cite{muirhead1982aspects}.



\begin{defbox}{}
	\begin{definition}\label{def:sample_mean_etc}
		Dataset contains $p$ variables and $n$ observations are represented by 
		$\X = (\x_{1},\ldots,\x_n)^\TT$, where the $i$th row $\x_i^\TT=(x_{i1},\ldots,x_{ip})$ is the $i$th observation vector, $i=1,\ldots,n$. 
		\begin{itemize}
			\item (\red{Sample mean vector}) $\bar{\x}=n^{-1}\sum_{i=1}^{n}\x_i=(\bar{x}_1,\ldots,\bar{x}_p)^\TT$, where $\bar{x}_j=n^{-1}\sum_{i=1}^{n}x_{ij}$.
			\item (\red{Sum of squares and cross product (SSCP) matrix}) $\A=\sum_{i=1}^{n}(\x_i-\bar{\x})(\x_i-\bar{\x})^\TT$.
			\item (\red{Sample covariance matrix}) $\S=(n-1)^{-1}\A$.
			\item (\red{Sample correlation matrix}) $\R=D^{-1/2}\S D^{-1/2}$, where $D^{-1/2}=\diag(1/\sqrt{s_{11}},\ldots,1/\sqrt{s_{pp}})$.
		\end{itemize}
	\end{definition}
\end{defbox}
\begin{itemize}
	\item $\bar{\x}=n^{-1}\X^\TT\1_n$, and
	\begin{sequation*}
		\begin{aligned}
			\A&= \sum_{i=1}^{n}(\x_i-\bam)(\x_i-\bam)^\TT-n(\bar{\x}-\bam)(\bar{\x}-\bam)^\TT \\ & =(\X-\1_n\bar{\x}^\TT)^\TT(\X-\1_n\bar{\x}^\TT)\succeq \0.
		\end{aligned}
	\end{sequation*}
	\item $\bbE\bar{\x}=\bam$, $\Var(\bar{\x})=n^{-1}\Sigma$, $\bbE \A=(n-1)\Sigma$, and $\bbE \S=\Sigma$.
\end{itemize}

\subsection{Multivariate normal distribution}\label{sec:mult_normal}
\begin{defbox}{Original definition of multivariate normal}
	\begin{definition}\label{def:oridef_multi_normal}
		The random vector $\x\in\bbR^p$ is said to have an $p$-variate normal distribution ($\x\sim\rmN_p$) if $\forall \a\in\bbR^p$, the distribution of $\a^\TT\x$ is univariate normal.  
	\end{definition}
\end{defbox}
\begin{thmbox}{Fundamental properties}
	\begin{theorem}\label{thm:multi_normal}
		Let $\x\sim\rmN_p$, we have
		\begin{enumerate}
			\item Both $\bam=\bbE\x$ and $\Sigma=\Cov(\x)$ exist and the distribution of $\x$ is determined by $\bam$ and $\Sigma$. Write $\x\sim\rmN_p(\bam,\Sigma)$. 
			\item (\red{Representation}) Let $\Sigma\succeq\0_{p\times p}$, $\rmr(\Sigma)=r\leq p$, and $u_{1:r}\simiid\rmN(0,1)$, i.e., $\u\sim\rmN_r(\0_r,I_r)$, then if $C$ is the full rank decomposition of $\Sigma$ and $\bam\in\bbR^p$, then $\x=C\u+\bam\sim\rmN_p(\bam,\Sigma)$.  
			\begin{itemize}
				\item Let $\Sigma=HDH^\TT$ be the spectral
				decomposition, then $\x=HD^{1/2}\z+\bam$, where $\z\sim\rmN_p(\0_p,I_p)$.
			\end{itemize}
			\item If $\x\sim\rmN_p(\bam,\Sigma)$, then its \red{ch.f.} $\phi_\x(\t)=\exp(i\bam^\TT\t-\t^\TT\Sigma\t/2)$.
			\item (\red{Density}) $\x\sim\rmN_p(\bam,\Sigma)$ with $\Sigma\succ \0$, then $\x$ has pdf
			\begin{sequation}\label{eq:mult_normal_density}
				f(\boldsymbol{x})=\frac{1}{(2\pi)^{p/2}|{\Sigma}|^{1/2}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T}{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}.
			\end{sequation}
		\end{enumerate}
	\end{theorem}
\end{thmbox}
Note that we guarantee the existence of $\rmN_p(\bam,\Sigma)$ by means of the representation in point 2. By its density, we have \underline{MVN kernel}: If
\begin{sequation*}
	f(\x)\propto \exp\cbk{-\frac{1}{2}(\x^\TT A\x-2\x^\TT B)}=\exp\cbk{-\frac{1}{2}(\x-A^{-1}B)^\TT A (\x-A^{-1}B)-B^\TT A^{-1} B},
\end{sequation*}
then $\x\sim\rmN_p(A^{-1}B,A^{-1})$.

\begin{thmbox}{Properties of multivariate normal}
	\begin{theorem}\label{thm:multi_normal_prop}
		If $\x\sim\rmN_p(\bam,\Sigma)$, then we have 
		\begin{enumerate}
			\item (\red{Linearity}) Let $B\in\bbR^{q\times p}, \b\in\bbR^{q}$ nonrandom, and $B\Sigma B^\TT\succ \0$, then $B\x+\b \sim \rmN_q (B\bam+\b,B\Sigma B^\TT)$.  
			\item (\red{Linear combinations}) If $\x_k\sim\rmN_p(\bam_k,\Sigma_k)\indep$ for $k=1,\ldots,N$, then for any fixed constants $\alpha_1,\ldots,\alpha_N$, $\sum_{k=1}^N\alpha_k\x_k\sim\rmN_p(\sum_{k=1}^{N}\alpha_k\bam_k,\sum_{k=1}^{N}\alpha_k^2\Sigma_k)$.
			\begin{itemize}
				\item The sample mean $\bar{\x}\sim\rmN_p(\bam,\Sigma/N)$. 
			\end{itemize}
			\item (\red{Subset}) The marginal distribution of any subset of $k(<p)$ components of $\x$ is $k$-variate normal.   
			\item (\red{Marginal distribution}) Partition
			\begin{sequation*}
				\x=\left[{\begin{array}{c}\x_{1}\\\x_{2}\end{array}}\right],\quad\boldsymbol{\mu}=\left[{\begin{array}{c}\bam_{1}\\\bam_{2}\end{array}}\right],\quad\boldsymbol{\Sigma}=\left[{\begin{array}{cc}\Sigma_{11}&\Sigma_{12}\\ {\Sigma}_{21}& {\Sigma}_{22}\end{array}}\right], \quad \x_1\in\bbR^{q}, \x_2\in\bbR^{p-q}, \Sigma_{12}\in\bbR^{q\times(p-q)}.
			\end{sequation*}    
			Then $\x_1\sim\rmN_q(\bam_1,\Sigma_{11})$, $\x_1\indep\x_2$ iff $\Sigma_{12}=\0$. 
			\item (\red{Conditional distribution}) Let $\Sigma_{22}^{-}$ be a generalized inverse of $\Sigma_{22}$ (i.e., $\Sigma_{22}\Sigma_{22}^-\Sigma_{22}=\Sigma_{22}$), then 
			\begin{itemize}
				\item[(a)] $\x_1-\Sigma_{12}\Sigma_{22}^-\x_2\sim \rmN_q(\bam_1-\Sigma_{12}\Sigma_{22}^-\bam_2,\Sigma_{11}-\Sigma_{12}\Sigma_{22}^-\Sigma_{21})$, and $\indep \x_2$.
				\item[(b)] $[\x_1\mid\x_2]\sim\rmN_q(\bam_1+{\Sigma}_{12}{\Sigma}_{22}^-(\x_2-\bam_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-}\Sigma_{21})$. 
			\end{itemize}  
			\item (\red{Cramér}) If $p\times 1$ random vectors $\x\indep \y$ and $\x+\y\sim\rmN_p$, then both $\x,\y\sim\rmN_p$. 
			\item (\red{MLE}) of $(\mu,\Sigma)$ is $(\bar{\x},A/n)$.    
			\item (\red{Inverse of $\Sigma$ and conditional independence}) Denote $\Sigma^{-1}=(\nu^{jk})_{1\leq j,k\leq p}$. Then $\forall j\neq k$, $v^{jk}=0\Leftrightarrow  x_j\indep x_k \mid \x\setminus\{x_j,x_k\}$.    
		\end{enumerate} 
	\end{theorem}
\end{thmbox}
For point 3, each component of a random vector is (marginally) normal does not imply that the vector has a multivariate normal distribution. Counterexample: let $U_1,U_2,U_3\simiid\rmN(0,1)$, $Z\indep U_{1:3}$. Define 
\begin{sequation*}
	X_1 = \frac{U_1+ZU_3}{\sqrt{1+Z^2}},\quad X_2 = \frac{U_2+ZU_3}{\sqrt{1+Z^2}}.
\end{sequation*}   
Then $[X_1|Z]\sim\rmN(0,1)$, free of $Z$, so $X_1\sim\rmN(0,1)$, and $X_2\sim\rmN(0,1)$. But $(X_1,X_2)$ not normal.
The converse is true if the components of $\x$ are all independent and normal, or if $\x$ consists of independent subvectors, each of which is normally distributed.

For the proof of point 5, we use the lemma: if $\Sigma\succeq \0$, 
then $\ker(\Sigma_{22})\subset \ker(\Sigma_{12})$, and $\range(\Sigma_{21})\subset\range(\Sigma_{22})$. 
So $\exists B\in\bbR^{q\times(p-q)}$ satisfying $\Sigma_{12}=B\Sigma_{22}$.  

\subsubsection{Quadratic form of MVN}\label{sec:quad_MVN}
\begin{thmbox}{MGF}
	\begin{proposition}\label{prop:MVN_MGF_quad}
		If $\x\sim\rmN(\bam,\Sigma)$, $A$ is symmetric and $\sigma$ IS non-singular, then 
		\begin{equation*}
			M_{\x^\TT\A\x}(t)=|I-2tA\Sigma|^{-\frac{1}{2}}\exp\cbk{-\frac{1}{2}\bam^\TT[I-(I-2t\A\Sigma)^{-1}]{\Sigma}^{-1}\bam}.
		\end{equation*}
	\end{proposition}
\end{thmbox}


\begin{thmbox}{Variance}
	\begin{proposition}\label{prop:var_MVN_quad}
		If $\x\sim\rmN(\bam,\Sigma)$, then 
		\begin{sequation*}
			\begin{aligned}
			\Var(\x^\TT A \x)&=2\tr(A\Sigma A\Sigma)+4\bam^\TT A\Sigma A \bam, \\
			\Cov(\x^\TT A_1\x,\x^\TT A_2\x)&=2\tr(A_1\Sigma A_2 \Sigma)+4\bam^\TT A_1 \Sigma A_2 \bam.
			\end{aligned}
		\end{sequation*} 
	\end{proposition}
\end{thmbox}
See Problem B 2.14 in Peng DING. 

\begin{thmbox}{Independence of quadratic form}
	\begin{theorem}\label{thm:MVN_quad_indep}
		Assume $\x\sim N_p(\mu,\Sigma)$. 
		\begin{enumerate}
			\item 		For two symmetric matrix $A,B\in\bbR^{p\times p}$, $\x^\TT A \x\indep \x^\TT B\x$ iff  
			\begin{equation*}
				\Sigma A\Sigma B\Sigma=\0,\quad\Sigma A\Sigma B\bam=\Sigma B\Sigma A\bam=\0,\quad\bam^\TT A\Sigma B\bam=0.
			\end{equation*}	
			If $\Sigma\succ \0$, then iff $A\Sigma B=\0$.  
			\item If $\Sigma\succ \0$, $A\in\bbR^{p\times p}$ symmetric, and $B\in\bbR^{r\times p}$, then $\x^\TT A\x \indep B\x$ iff $B\Sigma A=\0$.    
		\end{enumerate}
	\end{theorem}
\end{thmbox}
See Theorem B.11 in Peng DING, and Problem 1.22--1.23 in \cite{muirhead1982aspects}.

\begin{thmbox}{Quadratic form of $\Sigma$}
	\begin{theorem}\label{thm:quad_MVN}
		If $\x,\x_{1:N}\simiid\rmN_p(\bam,\Sigma)$, where $\Sigma$ is nonsingular, then 
		\begin{itemize}
			\item $(\x-\bam)^\TT \Sigma^{-1}(\x-\bam)\sim\chi_p^2$,
			\item $\x^\TT \Sigma^{-1}\x\sim\chi_p^2(\bam^\TT\Sigma^{-1}\bam)$,
			\item partition
			\begin{sequation*}
				\x = \begin{bmatrix}
					\x_1 \\ \x_2
				\end{bmatrix},\ \bam=\begin{bmatrix}
					\bam_1 \\ \bam_2
				\end{bmatrix},\ \Sigma=\begin{bmatrix}
					\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}
				\end{bmatrix}, \quad \x_1,\bam_1\in\bbR^{k}, \ \Sigma_{11}\in\bbR^{k\times k}, \text{ then }
			\end{sequation*} 
			\begin{sequation*}
				Q=(\x-\bam)^\TT \Sigma^{-1}(\x-\bam)-(\x_1-\bam_1)^\TT \Sigma_{11}^{-1}(\x_1-\bam_1) \sim \chi_{p-k}^2.
			\end{sequation*}
			\item $N(\bar{\x}_N-\bam)^{\TT}\Sigma^{-1}(\bar{\x}_N-\bam)\sim\chi_p^2$,
			\item the Mahalanobis distance
			$d_i^2=(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_N)^{\TT}{\S}^{-1}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_N) \dto \chi_p^2$.
		\end{itemize} 
		If $\rmr(\Sigma)=k\leq p$, then 
		\begin{enumerate}
			\item 
			\begin{sequation*}
				\begin{aligned}
					(\x-\bam)^\TT \Sigma^{-}(\x-\bam)&\sim\chi_{k}^2,\\
					(\x-\bam)^\TT \Sigma^{+}(\x-\bam)&\sim\chi_{k}^2.
				\end{aligned}
			\end{sequation*}
			\item If $\Sigma$ is idempotent with $r(\Sigma)=k$, then $(\x-\bam)^\TT  (\x-\bam)\sim\chi_k^2$.  
			\begin{itemize}
				\item If $\bam\in\Col(\Sigma)$, then $\x^\TT \x\sim\chi_k^2(\bam^\TT \bam)$.  
			\end{itemize}
		\end{enumerate}
	\end{theorem}
\end{thmbox}
\begin{thmbox}{Quadratic form of any matrices}
	\begin{theorem}\label{thm:quad_MVN_general}If $\x\sim\rmN_p(\bam,\Sigma)$,
		\begin{enumerate}
			\item if $\Sigma$ is nonsingular, $B\in\bbR^{p\times p}$ is symmetric, then $\x^\TT B\x\sim\chi_{k}^2(\delta)$ iff $B\Sigma$ is idempotent (equiv., $B\Sigma B=B$), in which case $k=\rmr(B)$ and $\delta=\bam^\TT B\bam$;
			
			\item for $A\in\bbR^{p\times p}$, $\x^\TT A \x\sim\chi_{\rmr(A\Sigma)}^2(\bam^\TT A\bam)$ if 
			\begin{sequation*}
				(1) \Sigma A\Sigma A\Sigma=\Sigma A\Sigma,\quad (2)\bam^\TT A \Sigma A \bam=\bam^\TT A \bam, \quad (3) \Sigma A\Sigma A\bam=\Sigma A\bam;
			\end{sequation*} 

			\item let $A_i\in\bbR^{p\times p}$ be symmetric with rank $k_i$ for $i=1,\ldots,m$.   
			Denote $A=\sum_{i=1}^m A_i$, which is symmetric with rank $k$.
			Then $\x^\TT A_i \x\sim\chi_{k_i}^2 (\bam^\TT A_i\bam)$, 
			$\x^\TT A_i\x$ are pairwise independent and $\x^\TT A \x\sim\chi_{k}^2 (\bam^\TT A \bam)$, iff 
			\begin{enumerate}
				\item[(I)] any two of the following are true: (a) $A_i\Sigma$ idempotent, $\forall i$; (b) $A_i\Sigma A_j=0$, $\forall i<j$; and (c) $A\Sigma$ idempotent; \textbf{OR}
				\item[(II)] (c) is true and (d) $k=\sum_{i=1}^{m}k_i$; \textbf{OR}
				\item[(III)] (c) is true and (e) $A_1\Sigma,\ldots,A_{m-1}\Sigma$ are idempotent and $A_m \Sigma\succeq \0$.    
			\end{enumerate}    
			
			\item (Cochran's theorem) $\x\sim \rmN_p(\0_p,I_p)$ and $A_i$ is symmetric of rank $k_i$, for $i=1,\ldots,m$ with $\sum_{i=1}^{m}A_i=I_p$,
			then $\x^\TT A_i \x \sim \chi_{k_i}^2$ independently iff $\sum_{i=1}^{m}k_i=p$.       
		\end{enumerate}      
	\end{theorem}
\end{thmbox}

\subsection{The noncentral $\chi^2$ and F distribution}\label{sec:noncen_chi2_F}



\section{Asymptotic properties}\label{sec:asym_multi}
\subsection{Asymptotic distributions of sample means and covariance matrices}\label{sec:asym_multi_sampleMeanCov}
Refer to section 1.2.2, \cite{muirhead1982aspects}.
\begin{thmbox}{CLT for sample means}
	\begin{theorem}\label{thm:CLT_multi_sampleMean_iid}
		Let $\x_{1:n}\simiid[\bam,\Sigma]$, then 
		\begin{sequation*}
			\sqrt{n}(\bar{\x}_n-\bam)=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(\x_i-\bam)\dto \rmN_p(\0_p,\Sigma). 
		\end{sequation*} 
	\end{theorem}
\end{thmbox}

\begin{thmbox}{CLT for sample covariance matrices}
	\begin{theorem}\label{thm:CLT_multi_sampleCov_iid}
		Let $\x_{1:n}\simiid[\bam,\Sigma]$ with finite fourth moments, SSCP matrix $\A=\sum_{i=1}^{n}(\x_i-\bar{\x})(\x_i-\bar{\x})^\TT$, and $\S=(n-1)^{-1}\A$. Let $V=\Cov[\vec((\x_1-\bam)(\x_1-\bam)^\TT)]$, then 
		\begin{sequation*}
			\begin{aligned}
				\frac{1}{\sqrt{n}}(\vec(\A)-n\cdot\vec(\Sigma))&\dto \rmN_{p^2}(\0,V), \\
				\sqrt{n-1}(\vec(\S)-\vec(\Sigma)) & \dto \rmN_{p^2}(\0,V).
			\end{aligned}
		\end{sequation*}
	\end{theorem}
\end{thmbox}
Note that $V\in\bbR^{p^2\times p^2}$ is singular as the LHS vectors above have repeated elements. 


\chapter{Linear Models}\label{chap:linear_models}
Reference:
\begin{enumerate}
	\item CUHK STAT5030 - Linear Models (2025 Spring), by Yuanyuan Lin.
	\item CUHKSZ STA3001 - Linear Models (2022 Fall), by Zhou Zhou.
	\item Peng Ding - Linear Model and Extensions.
\end{enumerate}
\section{Linear regression for the full-rank model}\label{sec:LR_full_rank}
\begin{defbox}{Full-rank linear model (fixed design $\X$)}
	\begin{definition}\label{def:LR_full_rank}
		\begin{equation}\label{eq:LR_mat}
			\Y_{n}=\X_{n\times p}\bab_{p}+\bave_n, \quad \rmr(\X)=p,
		\end{equation}
		where $(\Y,X)$ is a pair of response, $\X$ is fixed and determistic, $\bab$ is a vector of covariates, and $\bave$ is unobservable error term.   
		\begin{sequation*}
			\Y=\begin{pmatrix}
				y_1 \\ y_2 \\ \vdots \\ y_n
			\end{pmatrix}, \ 
			\X=\begin{pmatrix}
				\x_1^\TT \\ \x_2^\TT \\ \vdots \\ \x_n^\TT
			\end{pmatrix}=\begin{pmatrix}
				\X_1 & \ldots & \X_p
			\end{pmatrix}.
		\end{sequation*}
	\end{definition}
\end{defbox}

\subsection{Ordinary least squares (OLS) estimation}\label{sec:LR_full_rank_OLS}

The \red{\textit{least squares estimate (LSE)}} minimizes 
\begin{salign*}
	\hat{\bab}=\argmin{\bab\in\bbR^p}\ L(\bab)\equiv \sum_{i=1}^{n}(y_i-\x_i^\TT\bab)^2 = (\Y-\X\bab)^\TT (\Y-\X\bab),
\end{salign*}
which satisfies 
\begin{salign*}
	\text{(FONC)} \qquad {\partial L(\bab)}/{\partial\bab}&=-2\X^\TT\Y+2X^\TT X\bab=\0_p, \\ 
	\textit{(\red{Normal equation})} \qquad \X^\TT \Y&=\X^\TT \X\hat{\bab}.
\end{salign*}
As $\X^\TT\X$ has full rank,
\begin{salign}\label{eq:LR_OLSE}
	\hat{\bab} = (\X^\TT\X)^{-1}\X^\TT \Y = \sbk{\sum_{i=1}^{n}\x_i\x_i^\TT}^{-1}\sbk{\sum_{i=1}^{n}\x_i y_i}.
\end{salign} 
We decompose
\begin{salign*}
	\Y=\hat{\Y}+\hat{\bave}, \quad
	\text{(fitted vector)} \ \hat{\Y}=\X\hat{\bab}, \quad
	\text{(residual)} \ \hat{\bave}=\Y-\hat{\Y}=\Y-\X\hat{\bab}
\end{salign*}



\begin{thmbox}{The geometry of LSE}
	\begin{proposition}\label{prop:geo_LSE}
		The OLS problem is to find the vector in $\Col(\X)$ that is the closest to $\Y$.
		So $\hat{\Y}$ is the projection of $\Y\in\Col(\X)$ onto $\Col(X)$, and the residual $\hat{\bave}\perp\Col(\X)$ algebraically. 
		\begin{enumerate}
			\item $\X_j^\TT\hat{\bave}=0 \Longleftrightarrow \X^\TT\hat{\bave}=\0_p \Longleftrightarrow \X^\TT (\Y-\hat{\Y})=\0_p $.
			\item $\hat{\Y}^\TT\hat{\bave}=0$, the Pythagorean Theorem implies that $\norm{\Y}^2=\littlenorm{\hat{\Y}}^2+\norm{\hat{\bave}}^2$. 
			\item If $\X$ contains $\1_n$, then $\1_n^\TT \hat{\bave}=0 \Rightarrow n^{-1}\sum_{i=1}^{n}\hat{\varepsilon_i}=0$.
			\item $\forall \b\in\bbR^{p}$, $(\X\hat{\bab}-\X\b)^\TT \hat{\bave}=0$, we have the following decomposition
			\begin{sequation*}
				\|\Y-\X\b\|^2=\|\Y-\X\hat{\bab}\|^2+\|\X(\hat{\bab}-\b)\|^2,
			\end{sequation*} 
			which implies that $\|\Y-\X\b\|^2\geq\|\Y-\X\hat{\bab}\|^2$ with equality holding iff $\b=\hat{\bab}$. 
		\end{enumerate}
		
	\end{proposition}
\end{thmbox}
Let $\H=\X(\X^\TT \X)^{-1}\X^\TT=(h_{ij})_{1\leq i,j\leq n}$ be the so-called \red{\textit{hat matrix}}.

\begin{salign*}
	\hat{\bave}= (I_n-\H)\Y, \qquad
	\hat{\Y}=\H\Y, \qquad \hat{y}_i=h_{ii}y_i+\sum_{j\neq i}h_{ij}y_j.
\end{salign*}
$h_{ii}$ is called the \textit{leverage score}. 
 
\begin{thmbox}{Properties of hat matrix $\H$}
\begin{proposition}\label{prop:OLS_H}\
	\begin{enumerate}
		\item $\H$ is symmetric idempotent (projection matrix), so $\H\X=\X$, $(I_n-\H)\X=\0_n$, and $\rmr(\H)=p$.   
		\item $I_n-\H$ is symmetric idempotent, so $\tr(I_n-\H)=n-p$. 
		\item $\hat{\bave}^\TT\hat{\bave}=\Y^\TT(I_n-\H)\Y=\Y^\TT\Y-\hat{\bab}^\TT \X^\TT \Y=\tr(\Y\Y^\TT(I_n-\H))$. 
	\end{enumerate}
	If $\X=(\1_n,\X_1)^\TT\in\bbR^{n\times (p+1)}$, then 
	\begin{enumerate}
		\item $\H\1_n=\1_n\Rightarrow \sum_{j=1}^{n}h_{ij}=1$, $\forall i=1,\ldots,n$.  
	\end{enumerate}
\end{proposition}
\end{thmbox}

\subsubsection{Gauss-Markov model}
\begin{assbox}{Gauss-Markov model assumption}
	\begin{assumption}\label{ass:GM_ass}
		For the linear model in Definition \ref{def:LR_full_rank}, we assume 
		\begin{sequation}\label{eq:GM_ass_diag}
			\bbE(\bave)=\0_n, \qquad \Cov(\bave)=\sigma^2 I_n. 
		\end{sequation}
		The unknown parameters are $(\bab,\sigma^2)$. 
	\end{assumption}
\end{assbox}
The mean-zero condition is an identifiability condition for the intercept component of $\bab$. 
\begin{thmbox}{Statistical properties of LSE}
	\begin{proposition}\label{prop:LR_LSE}
		Under Assumption \ref{ass:GM_ass}, for the LSE $\hat{\bab}$ in \eqref{eq:LR_OLSE}, we have 
		\begin{enumerate}
			\item (Unbiased estimate) $\bbE(\hat{\bab})=\bab$. 
			\item $\Cov(\hat{\bab})=\sigma^2(\X^\TT\X)^{-1}$.
			\item $\hat{\Y}$ and $\hat{\bave}$ are uncorrealted:   
			\begin{sequation*}
				\bbE \begin{pmatrix}
					\hat{\Y} \\ \hat{\bave}
				\end{pmatrix} = \begin{pmatrix}
					\X\bab \\ \0_n
				\end{pmatrix}, \qquad \Cov\begin{pmatrix}
					\hat{\Y} \\ \hat{\bave}
				\end{pmatrix} = \sigma^2 \begin{pmatrix}
					\H & \0_{n\times n} \\ \0_{n\times n} & I_n-\H
				\end{pmatrix} 
			\end{sequation*}
			\begin{itemize}
				\item $\Cov(\hat{y}_i,\hat{y}_j)=\sigma^2 h_{ij}$ and $\Cov(\hat{\varepsilon}_i,\hat{\varepsilon}_j)=-\sigma^2 h_{ij}$ for $i\neq j$.   
			\end{itemize}
			\item Let the \red{\textit{residual error sum of squares(RSS)/sum of squares due to error(SSE)}} be
			\begin{sequation}\label{eq:LR_LSE_SSE}
				\SSE=\RSS=\hat{\bave}^\TT\hat{\bave}=\sum_{i=1}^n \hat{\varepsilon}_i^2,
			\end{sequation}
			then
			\begin{sequation}\label{eq:LR_LSE_varest}
				\bbE(\hat{\varepsilon}_i)=\sigma^2(1-h_{ii}), \quad
				\bbE(\hat{\sigma}^2):=\bbE \cbk{\frac{\RSS}{n-\rmr(\X)}}=\sigma^2. 
			\end{sequation}  
		\end{enumerate}
	\end{proposition}
\end{thmbox}

\begin{thmbox}{Gauss-Markov theorem}
	\begin{theorem}\label{thm:GM_OLS}
		Under Assumption \ref{ass:GM_ass}, the LSE $\hat{\bab}$ for $\bab$ is the \red{best linear unbiased estimator (BLUE)} in the sense that 
		\begin{sequation*}
			\Cov(\tilde{\bab})\succeq\Cov(\hat{\bab})
		\end{sequation*}
		for $\forall \tilde{\bab}$ satisfying: (C1) $\tilde{\bab}=A\Y$ for some $A\in\bbR^{p\times n}$ not depending on $\Y$; and (C2) $\bbE\tilde{\bab}=\bab$ for any $\bab$.     
		In the process of the proof, we have that for all such $\tilde{\bab}$, 
		\begin{salign*}
			\Cov(\tilde{\bab}-\hat{\bab},\hat{\bab})=\0_p, \qquad \Cov(\tilde{\bab}-\hat{\bab})=\Cov(\tilde{\bab})-\Cov(\hat{\bab}).
		\end{salign*}
	\end{theorem}
\end{thmbox}
It implies that 
\begin{salign*}
	\c^\TT \Cov(\tilde{\bab})\c \geq \c^\TT \Cov(\hat{\bab})\c \quad  \Leftrightarrow \quad & \Var(\c^\TT \tilde{\bab}) \geq \Var(\c^\TT \hat{\bab}),\\
	& \Var(\tilde{\beta}_j)\geq \Var(\hat{\beta}_j), \ j=1,\ldots,p.
\end{salign*}

\subsubsection{Model with intercept}\label{sec:LR_full_rank_OLS_intercept}
Consider the linear model in Definition \ref{def:LR_full_rank} with intercept
\begin{sequation}\label{eq:LR_mat_intercept}
	\underset{n}{\Y}=\X \underset{k+1}{\bab} +{\bave},
\end{sequation}
where $\X=(\1_n,\X_c)$, $\bab^\TT=(\beta_0,\b^\TT)$, and $\b^\TT=(\beta_1,\ldots,\beta_k)$.
Let $\bar{\X}_c^\TT=(\bar{x}_{.1},\ldots,\bar{x}_{.k})$.
Note that
\begin{sequation*}
	\1_n^\TT \Y=n\bar{y}, \qquad \1_n^\TT \X_c=n\bar{\X}_c^\TT.
\end{sequation*}
\begin{thmbox}{Properties of LSE with intercept}
	\begin{proposition}\label{prop:LR_LSE_intercept}
		For the LSE $\hat{\bab}^\TT=(\hat{\beta}_0,\hat{\b}^\TT)$ in \eqref{eq:LR_OLSE}, we have let $\Z=\X_c-\1_n\bar{\X}_c^\TT$ 
		\begin{salign*}
			\hat{\beta}_0&=\bar{y}-\bar{\X}_c^\TT\hat{\b}=\frac{1}{n}\1_n^\TT \Y - \bar{\X}_c^\TT \hat{\b},\\
			\hat{\b}&=(\Z^\TT\Z)^{-1}\Z^\TT\Y.
		\end{salign*}
	\end{proposition}
\end{thmbox}

\subsection{Weighted least square (WLS) estimation}\label{sec:LR_full_rank_WLS}
\begin{assbox}{WLS assumption}
	\begin{assumption}\label{ass:GM_ass_WLS}
		For the linear model in Definition \ref{def:LR_full_rank}, we assume 
		\begin{sequation}\label{eq:GM_ass_WLS}
			\bbE(\bave)=\0_n, \qquad \Cov(\bave)=\Sigma, 
		\end{sequation}
		where $\Sigma\succ\0$ is known. 
	\end{assumption}
\end{assbox}
The \red{\textit{weighted least squares estimator} (WLSE)} or \red{\textit{generalized least squares estimator} (GLSE)} is defined as the minimizer 
\begin{salign*}
	\hat{\bab}^{\WLS}=\argmin{\bab\in\bbR^p}\ S(\bab)\equiv (\Y-\X\bab)^\TT\Sigma^{-1}(\Y-\X\bab),
\end{salign*}
which satisfies 
\begin{salign*}
	\text{(FONC)} \qquad {\partial S(\bab)}/{\partial\bab}&=-2\X^\TT\Sigma^{-1}\Y+2\X^\TT \Sigma^{-1} \X\bab=\0_p, \\ 
	\textit{({Normal equation})} \qquad \X^\TT\Sigma^{-1} \Y&=\X^\TT\Sigma^{-1}\X\hat{\bab}^{\WLS}.
\end{salign*}
As $\X^\TT\Sigma^{-1}\X$ has full rank since $\X$ and $\Sigma^{-1}$ have full rank,
\begin{salign}\label{eq:LR_WLSE}
	\hat{\bab}^{\WLS} = (\X^\TT\Sigma^{-1}\X)^{-1}\X^\TT\Sigma^{-1}\Y, \qquad \bbE\hat{\bab}^{\WLS}=\bab.
\end{salign} 


\bg{
	\begin{remark}
		\begin{enumerate}
			\item Another aspect to motivate the WLS: Since $\Sigma\succ\0$, $\Sigma^{-1/2}$ exists and thus 
			\begin{sequation}\label{eq:LR_WLSE_OLS}
				\underbrace{\Sigma^{-\frac{1}{2}}\Y}_{\tilde{\Y}}=\underbrace{\Sigma^{-\frac{1}{2}}\X}_{\tilde{\X}}\bab+\underbrace{\Sigma^{-\frac{1}{2}}\bave}_{\tilde{\bave}}., \qquad \hat{\bab}^{\WLS}=(\tilde{\X}^\TT\tilde{\X})^{-1}\tilde{\X}\tilde{\Y}.
			\end{sequation}  
			Now $\bbE(\tilde{\bave})=\0_n$ and $\Cov(\tilde{\bave})=I_n$ satisfy Assumption \ref{ass:GM_ass} of the OLS.  
			
			\item Theorem \ref{thm:GM_OLS} states that the LSE with $\Sigma=I_n$ has the smallest covariance matrix under the Gauss-Markov model.
		\end{enumerate}
	\end{remark}
}

\begin{thmbox}{WLS version of Gauss-Markov theorem}
	\begin{corollary}\label{cor:GM_WLS}
		Under Assumption \ref{ass:GM_ass_WLS}, the WLSE $\hat{\bab}^{\WLS}$ for $\bab$ is the BLUE: 
		\begin{sequation*}
			\Cov(\tilde{\bab})\succeq\Cov(\hat{\bab}^{\WLS})
		\end{sequation*}
		for $\forall \tilde{\bab}$ satisfying: (C1) $\tilde{\bab}=A\Y$ for some $A\in\bbR^{p\times n}$ not depending on $\Y$; and (C2) $\bbE\tilde{\bab}=\bab$ for any $\bab$.     
		For all such $\tilde{\bab}$, 
		\begin{salign*}
			\Cov(\tilde{\bab}-\hat{\bab}^{\WLS},\hat{\bab}^{\WLS})=\0_p, \qquad \Cov(\tilde{\bab}-\hat{\bab}^{\WLS})=\Cov(\tilde{\bab})-\Cov(\hat{\bab}^{\WLS}).
		\end{salign*}
	\end{corollary}
\end{thmbox}
\bg{
	\begin{remark}
		Let $\t\in\bbR^p$, by similar technique, the BLUE of $\t^\TT \bab$ is $\t^\TT \hat{\bab}^{\WLS}$, i.e., for any ${\bal}\in\bbR^{p}$ satisfies $\bbE(\bal^\TT\Y)=\t^\TT\bab$, we have 
		\begin{sequation*}
			\Var(\bal^\TT\Y)\geq \Var(\t^\TT\hat{\bab}^{\WLS})=\Var(\t^\TT (\X^\TT\Sigma^{-1}\X)^{-1}\X^\TT\Sigma^{-1}\Y).
		\end{sequation*} 
	\end{remark}
}
    

\subsection{LS theory of random-effect model}
\begin{defbox}{Full-rank linear model (random effect)}
	\begin{definition}\label{def:LR_full_rank_random_effect}
		\begin{equation}\label{eq:LR_mat_random_effect}
			\underset{n}{\Y}=\underset{n\times k}{\X} \underset{k}{\b} +\underset{n}{\e},
		\end{equation}
		where $\{Y_i,b_i,e_i\}_{i=1}^n$ are iid copies of $(Y,b,e)$,
		and $\bbE\b=\baq$ and $\Cov(\b)=\F$, $baq\in\bbR^k$ and $\F\in\bbR^{k\times k}$ PD. Also assume  
		\begin{sequation*}
			\bbE(\e\mid\b)=\0, \quad \Cov(\e\mid\b)=\V.
		\end{sequation*}    
	\end{definition}
\end{defbox}
We want to find the BLUE of $\p^\TT\b$ where $\p\in\bbR^k$ is given.
\begin{salign*}
	\bbE\Y &= \bbE\{\bbE(\Y\mid\b)\}=\X\baq,\\
	\Var(\Y) &= \bbE\{\Var(\boldsymbol{Y}\mid\boldsymbol{b})\}+\Var\{\bbE(\boldsymbol{Y}|\boldsymbol{b})\}=\V+\boldsymbol{X}\boldsymbol{F}\boldsymbol{X}^\top, \\
	\Cov(\boldsymbol{Y},\boldsymbol{p}^\top\boldsymbol{b})&=\bbE\{\Cov(\boldsymbol{Y},\boldsymbol{p}^\top\boldsymbol{b}|\boldsymbol{b})\}+\Cov[\bbE(\boldsymbol{Y}|\boldsymbol{b}),\boldsymbol{p}^\top\boldsymbol{b}]=\boldsymbol{X}\boldsymbol{F}\boldsymbol{p}.
\end{salign*}  

\subsection{Normal linear model}\label{sec:LR_full_rank_normal}
\begin{assbox}{Normal linear model}
	\begin{assumption}\label{ass:normal_linear}
		For the linear model in Definition \ref{def:LR_full_rank}, we assume $\bave\sim\rmN_n(\0_n,\sigma^2 I_n)$. 
		The unknown parameters are $(\bab,\sigma^2)$.  
	\end{assumption}
\end{assbox}
By Assumption \ref{ass:normal_linear}, we have 
\begin{salign*}
	\Y\sim\rmN_n(\X\bab,\sigma^2 I_n)\qquad \Longleftrightarrow\qquad y_i\simiid \rmN(\x_i^\TT\bab,\sigma^2),\ i=1,\ldots,n.
\end{salign*}
The likelihood function is 
\begin{sequation*}
	L(\bab,\sigma^2)=\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}}\exp\cbk{-\frac{1}{2\sigma^2}(\Y-\X\bab)^\top(\Y-\X\bab)}.
\end{sequation*}
By setting
\begin{salign*}
	\frac{\partial\log L}{\partial\bab}&=\frac{1}{\sigma^2}(\boldsymbol{X}^\top\boldsymbol{Y}-\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{\beta})=\0_p, \qquad \frac{\partial\log L}{\partial\sigma^2}=-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}(\Y-\X\bab)^\top(\Y-\X\bab)=0,
\end{salign*}
the MLE of $\bab$ and $\sigma^2$ are 
\begin{salign}\label{eq:LR_full_rank_MLE}
	\hat{\bab}=\hat{\bab}^{\MLE}&=(\X^\TT\X)^{-1}\X^\TT\Y, \qquad {\hat{\sigma}^{2\MLE}}=\frac{1}{n}(\Y-\X\hat{\bab}^{\MLE})^{\TT}(\Y-\X\hat{\bab}^{\MLE})=\frac{1}{n}\Y^{\TT}(I_n-\H)\Y,
\end{salign}  
that is, LSE is the MLE.
Note that ${\hat{\sigma}^{2\MLE}}$ is biased for finite $n$. 
\begin{thmbox}{Properties under normal assumption}
	\begin{proposition}\label{prop:LR_full_rank_normal_prop} The MLE \eqref{eq:LR_full_rank_MLE} has the following properties:
		\begin{enumerate}
			\item $\hat{\bab}^{\MLE}\sim\rmN_p(\bab,(\X^\TT\X)^{-1}\sigma^2)$. 
			\item The MLE/LSE $\hat{\bab}$ and SSE \eqref{eq:LR_LSE_SSE} are independent.
			\item Recall the variance estimator $\hat{\sigma}=\SSE/(n-\rmr(\X))$ in \eqref{eq:LR_LSE_varest}, we have \begin{salign*}
				\frac{\SSE}{\sigma^2}\sim\chi_{n-\rmr(\X)}^2, \quad \text{ or } \quad \frac{(n-\rmr(\X))\hat{\sigma}^2}{\sigma^2}\sim\chi_{n-\rmr(\X)}^2.
			\end{salign*}
			\begin{itemize}
				\item $\bbE\hat{\sigma}=\sqrt{\frac{2\sigma^2}{n-p}}\frac{\Gamma((n-p+1)/2)}{\Gamma((n-p)/2)}$, and $\bbE(\hat{\sigma}^{-2})=\frac{n-p}{2\sigma^2}\frac{\Gamma((n-p-2)/2)}{\Gamma((n-p)/2)}$.
			\end{itemize}
		\end{enumerate}
	\end{proposition}
\end{thmbox}

\subsubsection{ANOVA}\label{sec:LR_full_rank_normal_ANOVA}

The \red{\textit{total sum of squares (SST)}}, \red{\textit{sum of squares due to regression (SSR)/reduction in sum of squares}}, and SSE are 
\begin{salign*}
	\SST&=\Y^\TT\Y,\\
	\SSE&=\Y^\TT\Y-\hat{\bab}^\TT\X^\TT\Y=\Y^\TT (I_n-\H)\Y,\\
	\SSR=\SST-\SSE&=\hat{\bab}^\TT\X^\TT\Y= \Y^\TT\H\Y,
\end{salign*}
which have the following distributions
\begin{salign*}
	\frac{\SST}{\sigma^2}\sim\chi_{n}^2\sbk{\frac{\bab^\TT \X^\TT\X\bab}{\sigma^2}}, \quad 
	\frac{\SSE}{\sigma^2}\sim\chi_{n-\rmr(\X)}^2, \quad
	\frac{\SSR}{\sigma^2}\sim\chi_{\rmr(\X)}^2\sbk{\frac{\bab^\TT \X^\TT\X\bab}{\sigma^2}}, 
\end{salign*}
we can show $\SSR\indep\SSE$, thus
\begin{salign*}
	F(R)=\frac{\MSR}{\MSE}=\frac{\SSR/\rmr(\X)}{\SSE/(n-\rmr(\X))}=\frac{\SSR/\rmr(\X)}{\hat{\sigma}^2} \sim \textsc{F}_{[\rmr(\X),\n-\rmr(\X)]}\sbk{\frac{\bab^\TT\X^\TT\X\bab}{\sigma^2}}.
\end{salign*}

\begin{defbox}{$\SSM$, $\SST_m$, $\SSR_m$}
	\begin{definition}\label{def:LR_SSM}
		For the model without predictors $y_i=c_0+\varepsilon_i$, the LSE of $c_0$ is $\hat{c}_0=n^{-1}\1_n^\TT\Y=\bar{y}$.  
		Define the SSM to be the SSR without $x$:
		\begin{salign*}
			\SSM&=\hat{c}_0\1_n^\TT\Y=\Y^\TT \sbk{\frac{\1_n\1_n^\TT}{n}}\Y = n\bar{y}^2.
		\end{salign*} 
		The \red{\textit{total sum of squares corrected for the mean}} and the \red{\textit{sum of squares of regression corrected for the mean}} are 
		\begin{salign*}
			\SST_m&=\SST-\SSM=\Y(\I_n-\frac{1}{n}\1_n\1_n^\TT)\Y, \\
			\SSR_m&=\SSR-\SSM=\hat{\bab}^\TT\X^\TT\Y-\Y^\TT \1_n\1_n^\TT \Y = \hat{\b}^\TT \Z^\TT \Y=\hat{\b}^\TT (\Z^\TT\Z)\hat{\b}.
		\end{salign*} 
	\end{definition}
\end{defbox}

\begin{thmbox}{Distributions of $\SSM$, $\SST_m$, and $\SSR_m$}
\begin{proposition}\label{prop:LR_SSM_dist}
	Under model \eqref{eq:LR_mat} and normal assumption \ref{ass:normal_linear},
	\begin{salign*}
		\frac{\SSM}{\sigma^2}&\sim\chi_1^2\sbk{\frac{(\1_n^\TT\X\bab)^2}{n\sigma^2}},\quad 
		\frac{\SST_m}{\sigma^2}\sim\chi_{n-1}^2\sbk{\frac{\bab^\TT\X^\TT\X\bab-n^{-1}(\1_n^\TT\X\bab)^2}{\sigma^2}},
	\end{salign*}
	and if the model has intercept,
	\begin{salign*}
		\frac{\SSR_m}{\sigma^2}&\sim\chi_{\rmr(\X)-1}^2\sbk{\frac{\b^\TT(\Z^\TT\Z)\b}{\sigma^2}}.
	\end{salign*}
	Note that $\SST=(\SSM+\SSR_m)+\SSE$, each terms are mutually independent. 
\end{proposition}	
\end{thmbox}
Construct F-statistics:
\begin{salign*}
	F(M)&=\frac{\MSM}{\MSE}=\frac{\SSM/1}{\SSE/(n-\rmr(\X))}\sim \textsc{F}_{[1,n-\rmr(\X)]}\sbk{\frac{(\1_n^\TT\X\bab)^2}{n\sigma^2}},\\
	F(R_m)&=\frac{\MSR_m}{\MSE}=\frac{\SSR_m/(\rmr(\X)-1)}{\SSE/(n-\rmr(\X))}\sim\textsc{F}_{[\rmr(\X)-1,n-\rmr(\X)]}\sbk{\frac{\b^\TT(\Z^\TT\Z)\b}{\sigma^2}}
\end{salign*}

\subsubsection{General linear hypothesis and constrained LSE}\label{sec:LR_full_rank_normal_general_linear_hypo}
The general hypothesis we consider is
\begin{sequation*}
	H_0:\ \K^\TT \bab=\m, \quad \K^\TT\in\bbR^{s\times (k+1)},\ \bab\in\bbR^{k+1},\ \m\in\bbR^s,
\end{sequation*}
where $\K$ and $\m$ are specified constants, and $\K^\TT$ is assumed to be of full row rank to guarantee that the linear function of $\bab$ must be linearly independent.
\begin{thmbox}{Distribution of test statistics}
	\begin{proposition}\label{prop:LR_full_rank_normal_general_linear_hypo}
		Under Assumption \ref{ass:normal_linear}, 
		\begin{sequation*}
			\K^\TT\hat{\bab}-\m\sim\rmN_s[\K^\TT\bab-\m,\sigma^2\K^\TT(\X^\TT\X)^{-1}\K].
		\end{sequation*}
		Define 
		\begin{sequation*}
			Q=(\K^\TT\hat{\bab}-\m)^\TT [\K^\TT(\X^\TT\X)^{-1}\K]^{-1}(\K^\TT\hat{\bab}-\m).
		\end{sequation*}
		Then $Q\indep\SSE$, and 
		\begin{sequation*}
			\frac{Q}{\sigma^2}\sim\chi_{s}^2 \cbk{\sigma^{-2}(\K^\TT{\bab}-\m)^\TT [\K^\TT(\X^\TT\X)^{-1}\K]^{-1}(\K^\TT{\bab}-\m)}.
		\end{sequation*}
		Thus we have the test statistics 
		\begin{sequation*}
			F(H)=\frac{Q/s}{\SSE/[n-\rmr(\X)]}=\frac{Q}{s\hat{\sigma}^2}\sim\textsc{F}_{[s,n-\rmr(\X)]}\cbk{\sigma^{-2}(\K^\TT{\bab}-\m)^\TT [\K^\TT(\X^\TT\X)^{-1}\K]^{-1}(\K^\TT{\bab}-\m)}.
		\end{sequation*}
	\end{proposition}
\end{thmbox}
When $H_0$ is true, the solution of the constrained LSE $\mathrm{minimize}_{\bab}\norm{\Y-\X\bab}^2$ subject to $\K^\TT\bab=\m$ is 
\begin{salign*}
	\tilde{\bab}&=(\X^\top \X)^{-1}[\X^\top \Y-\K(\K^\top(\X^\top \X)^{-1}\K)^{-1}(\K^\top(\X^\top \X)^{-1}\X^\top \Y-\m)]\\
	&=\hat{\bab}-(\X^\top \X)^{-1}\K(\K^\top(\X^\top \X)^{-1}\K)^{-1}(\K^\top\hat{\bab}-\m).
\end{salign*}   
Under the reduced model ($H_0$), the SSE is 
\begin{salign*}
	\SSE_{H_0}=(\Y-\X\tilde{\bab})^\TT (\Y-\X\tilde{\bab}) 
	=\SSE + Q
\end{salign*}

\subsection{Proofs}\label{sec:LR_full_rank_proof}
\begin{pfbox}{Proof of Proposition \ref{prop:LR_LSE}}
	\begin{proof}
		1.
		\begin{salign*}
			\bbE(\hat{\bab})=\bbE((\X^\TT\X)^{-1}\X^\TT\Y)=(\X^\TT\X)^{-1}\X^\TT \bbE(\Y)=(\X^\TT\X)^{-1}\X^\TT\X\bab=\bab.
		\end{salign*}

		2.
		\begin{salign*}
			\Cov(\hat{\bab})&=\Cov((\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{Y})=(\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top \Cov(\boldsymbol{Y})\boldsymbol{X}(\boldsymbol{X}^\top\boldsymbol{X})^{-1}\\
			&=\sigma^2(\X^\top \X)^{-1}\X^\top I_n \X(\X^\top \X)^{-1}=\sigma^2(\X^\top \X)^{-1}.
		\end{salign*}

		3. 
			\begin{salign*}
				\bbE\begin{pmatrix}\hat{\Y}\\\hat{\bave}\end{pmatrix}&= \begin{pmatrix}\H\\I_n-\H\end{pmatrix}\bbE(\Y)
				=\begin{pmatrix}\H\\I_n-\H\end{pmatrix}\X\bab
				=\begin{pmatrix}\H\X\bab\\(I_n-\H)\X\bab\end{pmatrix}=\begin{pmatrix}\X\bab\\\0_{n}\end{pmatrix}.
			\end{salign*}
			\begin{salign*}\Cov\begin{pmatrix}{\hat{\Y}}\\{\hat{\bave}}\end{pmatrix}
				&=\begin{pmatrix}
					\H\\I_n-\H
				\end{pmatrix}\Cov(\Y)\begin{pmatrix}\H^\TT&(I_n-\H)^\TT\end{pmatrix}
				=\sigma^2\begin{pmatrix}\H\\I_n-\H\end{pmatrix}
				\begin{pmatrix}\H&I_n-\H\end{pmatrix}
				\\
				&\left.=\sigma^2\left(\begin{array}{cc}\H^2&\H(I_n-\H)\\(I_n-\H)\H&(I_n-\H)^2\end{array}\right.\right)=\sigma^2\begin{pmatrix}\H&\0\\\0 &I_n-\H\end{pmatrix},\end{salign*}

		4. By item 3, $\bbE(\hat{\varepsilon}_i)=\sigma^2(1-h_{ii})$, hence $\bbE(\RSS)=\sum_{i=1}^{n}\sigma^2(1-h_{ii})=\sigma^2\{n-\tr(H)\}=\sigma^2(n-p)$. Alternatively, we can directly show  
			\begin{salign*}
				\bbE(\hat{\boldsymbol{\varepsilon}}^\top\hat{\boldsymbol{\varepsilon}})=\bbE(\boldsymbol{Y}^\top(I_n-\boldsymbol{H})\boldsymbol{Y})
				=\tr((\I_n-\boldsymbol{H}){\Sigma})+\boldsymbol{\beta}^\top\boldsymbol{X}^\top(I_n-\boldsymbol{H})\boldsymbol{X}\bab
				=\sigma^2\tr(I_n-\H)=\sigma^2(n-p).
			\end{salign*}
	\end{proof}
\end{pfbox}

\begin{pfbox}{Proof of Theorem \ref{thm:GM_OLS}}
	\begin{proof}
		We must verify that the OLS estimator itself satisfies (C1) and (C2). We have $\hat{\bab} = \hat{A}\Y$ with $\hat{A} = (\X^\TT \X)^{-1}\X^\TT$, and it is unbiased by Proposition \ref{prop:LR_LSE}.

		First, the unbiasedness requirement implies that
		\begin{salign*}
			\bbE(\tilde{\bab}) = \bab \implies \bbE({A}\Y) = A \bbE(\Y) = A\X\bab = \bab \implies  A\X\bab=\bab, \quad \forall \bab.
		\end{salign*}
		So $A\X=I_p$ must hold.
		In particular, the OLS estimator satisfies $\hat{A}\X = (\X^\TT\X)^{-1}\X^\TT\X = I_p$.

		Second, we can decompose the covariance of $\tilde{\bab}$ as
		\begin{salign*}
			\Cov(\tilde{\bab})&=\Cov(\hat{\bab}+\tilde{\bab}-\hat{\bab})\\
			&=\Cov(\hat{\bab})+\Cov(\tilde{\bab}-\hat{\bab})+\Cov(\hat{\bab},\tilde{\bab}-\hat{\bab})+\Cov(\tilde{\bab}-\hat{\bab},\hat{\bab}).
		\end{salign*}
		The last two terms are in fact zero. By symmetry, we only need to show that the third term is zero:
		\begin{salign*}
			\Cov(\hat{\bab},\tilde{\bab}-\hat{\bab}) &= \Cov\{\hat{A}\Y,(A-\hat{A})\Y\}=\sigma^2 \hat{A}(A-\hat{A})^{\TT}\\
			&=\sigma^2(\hat{A}A^\TT-\hat{A} \hat{A}^\TT)\\
			&=\sigma^2\left\{(\X^\TT\X)^{-1}\X^\TT A^\TT-(\X^\TT\X)^{-1}\X^\TT\X(\X^\TT\X)^{-1}\right\}\\
			&=\sigma^2\{(\X^\TT \X)^{-1} I_p - (\X^\TT\X)^{-1}\}=\0_p.
		\end{salign*}
		The above covariance decomposition simplifies to
		\begin{sequation*}
			\Cov(\tilde{\bab})=\Cov(\hat{\bab})+\Cov(\tilde{\bab}-\hat{\bab}),
		\end{sequation*}
		which implies 
		\begin{sequation*}
			\Cov(\tilde{\bab})-\Cov(\hat{\bab})=\Cov(\tilde{\bab}-\hat{\bab})\succeq \0.
		\end{sequation*}
	\end{proof}
\end{pfbox}

\begin{pfbox}{Proof of Corollary \ref{cor:GM_WLS}}
	\begin{proof}
		The WLSE satisfies conditions (C1)--(C2).
		We write WLS in the form of OLS \eqref{eq:LR_WLSE_OLS} such that $\hat{\bab}^{\WLS}=(\tilde{\X}^\TT\tilde{\X})^{-1}\tilde{\X}\tilde{\Y}$. 
		Then by Theorem \ref{thm:GM_OLS}, 
		\begin{sequation*}
			\Cov(\tilde{\bab}')\succeq \Cov(\hat{\bab}^{\WLS})
		\end{sequation*}
		for all $\tilde{\bab}'$ satisfying: (1) $\tilde{\bab}'=\tilde{A}\tilde{\Y}$ for some $\tilde{A}\in\bbR^{p\times n}$ not depending on $\tilde{\Y}$; and (2) $\bbE\tilde{\bab}'=\bab$.
		Suppose $\tilde{\bab}$ satisfies: (C1) $\tilde{\bab}=A\Y$ for some $A\in\bbR^{p\times n}$ not depending on $\Y$; and (C2) $\bbE\tilde{\bab}=\bab$.
		Then $\tilde{\bab}=(A\Sigma^{1/2})\Sigma^{-1/2}\Y=(A\Sigma^{1/2})\tilde{\Y}$, i.e. satisfying (1); and (2) is satisfied by definition. 
		So 
		\begin{sequation*}
			\Cov(\tilde{\bab})\succeq \Cov(\hat{\bab}^{\WLS}).
		\end{sequation*}
	\end{proof}
\end{pfbox}

\begin{pfbox}{Proof of Proposition \ref{prop:LR_full_rank_normal_prop}}
	\begin{proof}
		Item 1 is obvious. Item 2 applies Theorem \ref{thm:MVN_quad_indep}. Item 3 uses Theorem \ref{thm:quad_MVN_general} to show the $\chi^2$ distribution and \eqref{eq:chi2_moment} to show the moments. 
	\end{proof}
\end{pfbox}


\section{Linear regression for the non full-rank models}\label{sec:LR_nonfull_rank}

\section{Quantile Regression}\label{sec:qt_reg}
\subsection{Sample quantile}\label{sec:sample_qt}
For a random variable $y$, we can define its distribution function as $F(c)=\bbP(y\leq c)$ and its $\tau$th quantile as 
\begin{sequation*}
	F^{-1}(\tau)=\inf\{q:F(q)\geq \tau\}.
\end{sequation*}  
\begin{thmbox}{Check function and quantile}
	\begin{proposition}\label{prop:check_function_qt}
		With a monotone distribution function and positive density at the $\tau$th quantile, we have  
		\begin{sequation}\label{eq:qt}
			F^{-1}(\tau)=\arg\min_{q\in\bbR}\bbE\left\{\rho_{\tau}(y-q)\right\},
		\end{sequation}
		where 
		\begin{sequation*}
			\rho_\tau(u)=u\left\{\tau-\1_{(u<0)}\right\}=\begin{cases}
				u\tau,& if\ u\geq0,\\
				-u(1-\tau),& if\ u<0,
			\end{cases}
		\end{sequation*}
		is the check function. In particular, the median of $y$ is  
		\begin{sequation*}
			\operatorname{median}(y)=F^{-1}(0.5)=\operatorname{arg}\operatorname*{min}_{q\in\mathbb{R}}\bbE\left\{|y-q|\right\}.
		\end{sequation*}
	\end{proposition}
\end{thmbox}

The empirical distribution function is $\hat{F}(c)=n^{-1}\sum_{i=1}^{n}\1_{(y_i\leq c)}$, which is a step function,
increasing but not strictly monotone.
\begin{defbox}{Sample quantile}
	\begin{definition}\label{def:sample_qt}
		We define the \red{\textit{sample quantile}} as 
		\begin{sequation}\label{eq:sample_qt}
			\hat{F}^{-1}(\tau)=\arg\min_{q\in\mathbb{R}}n^{-1}\sum_{i=1}^n\rho_\tau(y_i-q).
		\end{sequation}
	\end{definition}
\end{defbox}
$\hat{F}^{-1}(\tau)$ may not be unique even though the population quantile is. We can view $\hat{F}^{-1}(\tau)$ as a set containing all minimizers, and with large samples the values in the set do not differ much.

\begin{thmbox}{CLT for sample quantile}
	\begin{theorem}\label{thm:CLT_qt}
		Assume that $(y_i)_{i=1}^n\simiid y$ with distribution function $F(\cdot)$ that is strictly increasing and density function $f(\cdot)$ that is positive at the $\tau$th quantile. 
		The sample quantile is consistent for the true quantile and is asymptotically Normal:
		\begin{sequation*}
			\sqrt{n}\left\{\hat{F}^{-1}(\tau)-F^{-1}(\tau)\right\} \dto \rmN\left(0,\frac{\tau(1-\tau)}{\left[f\left\{F^{-1}(\tau)\right\}\right]^{2}}\right).
		\end{sequation*}
		In particular, the sample median satisfies 
		\begin{sequation*}
			\sqrt{n}\left\{\hat{F}^{-1}(0.5)-\operatorname{median}(y)\right\} \dto \rmN\left(0,\frac{1}{4\left[f\left\{\operatorname{median}(y)\right\}\right]^{2}}\right).
		\end{sequation*}
	\end{theorem}
\end{thmbox}

\subsection{Linear regression quantile}\label{sec:linear_reg_qt}
\begin{defbox}{Conditional quantile function}
	\begin{definition}\label{def:cond_qt}
		Let $\x\in\bbR^p$. The \red{\textit{conditional quantile function}} is defined as 
		\begin{sequation*}
			F^{-1}(\tau\mid\x)=\argmin{q(\cdot)} \bbE[\rho_\tau\{y-q(\x)\}].
		\end{sequation*} 
		We can use a linear function $\x^\TT\bab(\tau)$ to approximate the conditional quantile function with 
		\begin{sequation*}
			\bab(\tau)=\argmin{\b\in\bbR^p}\bbE\{\rho_\tau(y-\x^\TT \b)\}
		\end{sequation*}
		called the $\tau$th \red{\textit{population regression quantile}}, and 
		\begin{sequation*}
			\hat{\bab}(\tau)=\argmin{\b\in\bbR^p}\frac{1}{n}\sum_{i=1}^{n}\{\rho_\tau(y_i-\x_i^\TT \b)\}
		\end{sequation*}
		called the $\tau$th \red{\textit{sample regression quantile}}.
		As a special case, when $\tau=0.5$, we have the
		\red{\textit{regression median/least absolute deviations (LAD)}}:
		\begin{sequation*}
			\hat{\bab}(0.5)=\argmin{\b\in\bbR^p}\frac{1}{n}\sum_{i=1}^{n}\abs{y_i-\x_i^\TT \b}.
		\end{sequation*} 
	\end{definition}
\end{defbox}
Conditional quantile model: $F^{-1}(\tau\mid\x)=\x^\TT\bab(\tau)$, where $\beta_j(\tau)$ is the partial influence of $x_{ij}$ on the $\tau$th conditional quantile of $y_i$ given $\x_i$.

\begin{thmbox}{CLT for regression quantiles}
	\begin{theorem}\label{thm:CLT_reg_qt}
		Assume $(y_i,\x_i)_{i=1}^n\simiid(y,\x)$, under some regularity conditions, we have 
		\begin{sequation*}
			\sqrt{n}\{\hat{\bab}(\tau)-\bab(\tau)\}\dto \rmN_p(\0_p,B^{-1}MB^{-1}),
		\end{sequation*}
		where 
		\begin{sequation*}
			B=\bbE\mbk{
				f_{y\mid\x}\{\x^\TT \bab(\tau)\}\x\x^\TT
			}, \qquad 
			M = \bbE\mbk{
				\{\tau-\1_{(y-\x^\TT\bab(\tau)\leq 0)}\}^{2}\x\x^\TT
			}.
		\end{sequation*}
	\end{theorem}
\end{thmbox}



\subsection{Proofs}
\begin{pfbox}{Proof of Proposition \ref{prop:check_function_qt}}
	\begin{proof}
		We use Leibniz's integral rule. Write 
		\begin{salign*}
			\bbE\{\rho_{\tau}(y-q)\}=\bbE\{(y-q)(\tau-\1_{(y<q)})\}=\int_{-\infty }^{q}(\tau-1)(c-q)\rmd F(c) + \int_{q}^{\infty }\tau (c-q)\rmd F(c).
		\end{salign*}
		To minimize it over $q$, we can solve the FONC by assuming that the derivative and integral can be exchanged:
		\begin{salign*}
			\frac{\rmd \bbE\{\rho_{\tau}(y-q)\}}{\rmd q} &= (1-\tau)\int_{-\infty }^{q}\rmd F(c) - \tau \int_{q}^{\infty }\rmd F(c)=(1-\tau)F(q)-\tau\{1-F(q)\}=0,\\
			& \Rightarrow \tau=F(q).
		\end{salign*} 
		So the $\tau$th quantile satisfies the FONC.
		If $y$ has density $f(\cdot)$, the second-order condition ensures it is the minimizer:
		\begin{sequation*}
			\frac{\rmd^2\bbE\{\rho_{\tau}(y-q)\}}{\rmd q^2}\bigg|_{q=F^{-1}(\tau)}=f\{F^{-1}(\tau)\}>0,
		\end{sequation*}   
		by Leibniz's integral rule again.
	\end{proof}
\end{pfbox}

\begin{pfbox}{Proof of Theorem \ref{thm:CLT_qt}}
	\begin{proof}
		Recall the definition \eqref{eq:sample_qt} of sample quantile
		\begin{sequation*}
			\hat{F}^{-1}(\tau)=\arg\min_{q\in\mathbb{R}}n^{-1}\sum_{i=1}^n\rho_\tau(y_i-q).
		\end{sequation*}
		Since 
		\begin{sequation*}
			\frac{\rmd \rho_\tau(y_i-q)}{\rmd q}= -\tau + \1_{(y_i<q)} = \begin{cases}
				-\tau &\text{ if } y>q,\\
				1-\tau &\text{ if } y<q,
			\end{cases}
		\end{sequation*} 
		FONC gives the estimating equation \eqref{eq:est_eq} 
		\begin{salign*}
			\bar{m}(y_{1:n},q)=\frac{1}{n}\sum_{i=1}^{n}\frac{\rmd \rho_\tau(y_i-q)}{\rmd q}=\frac{1}{n}\sum_{i=1}^{n}\underbrace{\{-\tau+\1_{(y_i<q)}\}}_{m(y_i,q)}=0.
		\end{salign*}
		As $F(\cdot)$ is strictly increasing, and $\bbE\{m(y,q)\}=-\tau+F(q)$,  the unique solution $q_0$ of $\bbE\{m(y,q)\}=0$ is $q_0=F^{-1}(\tau)$, the unique true $\tau$th quantile.
		Besides, the solution $\hat{q}$ of $\bar{m}(y_{1:n},q)=0$ is the sample quantile $\hat{q}=\hat{F}^{-1}(\tau)$ by definition.
		
		Now we find $B$ and $M$ in Theorem \ref{thm:M_est_asym_iid}:
		\begin{salign*}
			\frac{\rmd\bbE m(y,q)}{\rmd q}=F'(q)=f(q) \ &\Rightarrow \ B=-\frac{\rmd\bbE m(y,q_0)}{\rmd q}=-f(F^{-1}(\tau)), \\
			\bbE m(y,q)^2=\tau^2-2\tau F(q) + F(q) \ & \Rightarrow \ M=\bbE m(y,q_0)^2=\tau(1-\tau).
		\end{salign*}  
		By Theorem \ref{thm:M_est_asym_iid},
		\begin{sequation*}
			\sqrt{n}(\hat{q}-q_0)=\sqrt{n}(\hat{F}^{-1}(\tau)-F^{-1}(\tau))\dto \rmN\left(0,\frac{\tau(1-\tau)}{\left[f\left\{F^{-1}(\tau)\right\}\right]^{2}}\right).
		\end{sequation*}
	\end{proof}
\end{pfbox}

\begin{pfbox}{Proof of Theorem \ref{thm:CLT_reg_qt}}
	\begin{proof}
	\end{proof}
\end{pfbox}

\section{Regularization}\label{sec:regularization}
\subsection{Ridge regression}\label{sec:ridge_reg}



\subsection{LASSO}\label{sec:lasso}




\chapter{Generalized Linear Models}\label{chap:GLM}
\section{Logistic regression}\label{sec:logistic_reg}
\subsection{Logistic regression for binary outcomes}\label{sec:logistic_reg_binary}
\begin{defbox}{Logistic model}
	\begin{definition}\label{def:logistic_model}
		Let the binary outcomes $y_i\in\{0,1\}$ and $p$-dim covariates $\x_i\sim f_\x$ for $i=1,\ldots,n$. 
		Model 
		\begin{sequation*}
			\bbP(y_i=1\mid \x_i) = g(\x_i^\TT\bab),
		\end{sequation*}  
		where $g(\cdot):\bbR\to [0,1]$ is the link function. 
		We have:
		\begin{enumerate}
			\item (\red{\textit{Logit model}}) $g(z)={e^z}/\sbk{1+e^z}$. It is the distribution function of the standard logistic distribution with density $g'(z)=e^z/(1+e^z)^2=g(z)\{1-g(z)\}$. 
			\item ({\textit{Probit model}}) $g(z)=\Phi(z)$. 
			\item (\textit{Cauchit model}) $g(z)=\pi^{-1}\arctan(z)+1/2$. It is the distribution function of the standard Cauchy
			distribution with density $g'(z)=\pi^{-1}(1+z^2)^{-1}$. 
			\item (\textit{Cloglog model}) $g(z)=1-\exp(-e^{z})$. It is the distribution function of the standard log-Weilbull distribution with density $g'(z)=\exp(z-e^z)$. 
		\end{enumerate} 
	\end{definition}
\end{defbox}

We mainly focus on the logit model:
\begin{sequation*}
	\bbP(y_i=1\mid \x_i) = g(\x_i^\TT\bab)=\frac{e^{\x_i^\TT\bab}}{1+e^{\x_i^\TT\bab}},
\end{sequation*} 
with likelihood, log-likelihood, and MLE are
\begin{salign*}
	L(\bab,f_\x\mid y_{1:n},\x_{1:n})&=\prod_{i=1}^{n}\{g(\x_i^\TT\bab)\}^{y_i}\{1-g(\x_i^\TT\bab)\}^{1-y_i} f_\x(\x_i), \\
	\ell(\bab,f_\x\mid y_{1:n},\x_{1:n})&=\sum_{i=1}^{n}\mbk{
		y_i\log g(\x_i^\TT\bab)+(1-y_i)\log\{1-g(\x_i^\TT\bab)\}
	}+\sum_{i=1}^{n}\log f_\x(\x_i),\\
	\hat{\bab}&=\argmax{\bab\in\bbR^p}\ell(\bab,f_\x\mid y_{1:n},\x_{1:n}).
\end{salign*}

\begin{thmbox}{Properties of logit model by M-estimation}
\begin{proposition}\label{prop:logit_model}\
	\begin{enumerate}
		\item (FONC and estimating equation for $\hat{\bab}$)
		\begin{sequation*}
			\bar{\m}((y_{1:n},\x_{1:n}),\bab)=\frac{\partial \ell(\bab,f_\x\mid \cdot)}{\partial \bab} =
			\sum_{i=1}^{n} \underbrace{\{y_i-g(\x_i^\TT\bab)\}\x_i}_{\m((y_i,\x_i),\bab)}.
		\end{sequation*}

		\item (Second-order derivative)
		\begin{sequation*}
			\frac{\partial \m((y,\x),\bab)}{\partial \bab^\TT} = -g(\x^\TT\bab)\{1-g(\x^\TT\bab)\}\x\x^\TT.
		\end{sequation*}

		\item (CLT for $\hat{\bab}$) Suppose $\bab_0$ is the true parameter, then  
		\begin{salign*}
			& \sqrt{n}(\hat{\bab}-\bab_0)\dto\rmN_p(\0_p,A^{-1}BA^{-\TT})=\rmN_p(\0_p,B^{-1}),\\
			 A&=\bbE g(\x^\TT\bab_0)\{1-g(\x^\TT\bab_0)\}\x\x^\TT,  \quad B=-A=I(\bab_0).
		\end{salign*}
	\end{enumerate}
\end{proposition}
\end{thmbox}

\chapter{Bayesian Inference}\label{chap:bayes_inference}



\chapter{Structural Equation Model (SEM)}\label{chap:SEM}
Reference: 
\begin{itemize}
	\item CUHK STAT5020 - Topics in multivariate analysis (2025 Spring), by Xin Yuan SONG.
	\item Sik-Yum Lee and Xin-Yuan Song - Basic and advanced
	Bayesian structural equation modeling: With applications in the
	medical and behavioral sciences \cite{lee2012basic}.
\end{itemize}

\section{SEM models}\label{sec:SEM_models}

\noindent \textbf{\underline{Goal}}: to examine the relationships among the variables of interest.

\noindent \textbf{\underline{Approach}}: group observed variables to form \red{\textit{latent variables}} (a combination of several observed variables).
\begin{itemize}
	\item Reduce the number of variables compared to direct regression.
	\item As highly correlated observed variables are grouped into latent variables, the problem induced by multicollinearity is alleviated.
	\item It gives better assessments on the interrelationships of latent constructs.
\end{itemize}

\begin{defbox}{Linear SEMs}
	\begin{definition}\label{def:linearSEM}
		Assume that the \brown{observed variables} $\brown{\y}_p\simiid \rmN_p$ with mean $\bam_p$. Let $\red{\bao}_q$ be \red{latent variables}. $\red{\bao}_q = (\pink{\bet}_{q_1}^\TT,\teal{\bax}_{q_2}^\TT)^\TT$, where $\pink{\bet}_{q_1}$ is the \pink{key outcome
		latent variables}, and $\teal{\bax}_{q_2}$ is the \teal{explanatory latent variables}. Define 
		\begin{equation}\label{eq:linearSEM}
			\begin{aligned}
			\text{(\textbf{Measurement equation} (MeaEq))}\qquad\qquad & \brown{\y}_p = \bam_p + \baL_{p\times q} \red{\bao}_q + \bae_p, \\
			\text{(\textbf{Structural equation} (StEq))}\qquad\qquad &
			\pink{\bet}_{q_1} = \baG_{q_1\times q_2} \teal{\bax}_{q_2} + \bad_{q_1},
			\end{aligned}
		\end{equation}
		where $\baL$ is the unknown \textit{factor loading matrix}, $\baG$ is the unknown matrix of regression
		coeffcients, and $\bae$ and $\bad$ are measurement (residual) errors.   
	\end{definition}
\end{defbox}
The basic SEM is formulated by two components:
\begin{enumerate}
	\item (MeaEq) \red{\textit{Confirmatory factor analysis (CFA)}} model groups the highly correlated observed variables into latent variables. 
	We know the information/structure of $\baL$ (e.g., know $y_1,y_2$ are only affected by $\eta$ and $y_3,y_4$ are affected by $\xi$). 
	\begin{itemize}
		\item \red{\textit{Exploratory factor analysis (EFA)}}: we have $\y$ and just know they have lower-dimensional structure, but don't know the structure of $\baL$ and $q$.   
	\end{itemize}
	It regresses $\y$ on a smaller number of latent variables. 
	\item (StEq) A regression type model, in which $\bet$ is regressed on $\bax$. 
\end{enumerate}




\begin{assbox}{Standard linear SEMs}
\begin{assumption}\label{ass:linearSEM}
For $i=1,\ldots,n$, 
\begin{itemize}
	\item[(A1)] $\bae_i\simiid\rmN[\0_p,\baY_\bae]$, where $\baY_\bae\in\bbR^{p\times p}$
	is diagonal.
	\item[(A2)] $\teal{\bax}_i\simiid\rmN[\0_{q_2},\baF]$,
	where $\baF$ is a general (we can evaluate the correlations of the latent variables). 
	\item[(A3)] $\bad_i\simiid\rmN[\0_{q_1},\baY_{\bad}]$, where $\baY_{\bad}$
	is diagonal.
	\item[(A4)] $\bad_i\indep\bax_i$, and $\bae_i\indep\bao_i,\bad_i.$
\end{itemize} 
\end{assumption}
\end{assbox}
These assumptions imply that 
\begin{sequation*}
	\pink{\bet}_i\simiid\rmN_{q_1}(\0_{q_1},\baG\baF\baG^\TT+\baY_{\bad}),\quad \red{\bao}_i\simiid\rmN_{q}\sbk{\0_{q},\baS_{\red{\bao}}=\begin{bmatrix}
		\baG\baF\baG^\TT+\baY_{\bad} & \baG\baF \\
		\baF\baG^\TT & \baF
	\end{bmatrix}}, \quad \brown{\y}_i\simiid\rmN_{p}(\bam,\baS(\baq)=\baL\baS_{\red{\bao}}\baL^\TT+\baY_{\bae}). 
\end{sequation*}

\noindent\underline{\textbf{Identifiability issue}}: 
The measurement equation is identfied if $\forall \baq_1,\baq_2$, $\mathrm{MeaEq}(\baq_1)=\mathrm{MeaEq}(\baq_2)$ implies $\baq_1=\baq_2$. The structural equation is identfied if $\forall \baq_1,\baq_2$, $\mathrm{StEq}(\baq_1)=\mathrm{StEq}(\baq_2)$ implies $\baq_1=\baq_2$. The SEM is identified if both of its MeaEq and StEq are identfied.
For example, consider \eqref{eq:linearSEM} MeaEq:
\begin{sequation*}
	\y=\bam + (\baL \M) (\M^{-1}\bao) + \bae = \bam + \baL^* \bao^* + \bae, \quad \bao^*\sim\rmN(\0_{q},\M^{-1}\Cov(\bao)\M^{-\TT}).
\end{sequation*}
We have to impose restrictions on $\baL$ and/or $\Cov(\bao)$ such that the only nonsingular $\M=I_q$.    
\begin{itemize}
	\item A simple and common method is using a $\baL$ with the \red{\textit{non-overlapping structure}}, e.g.,
	\begin{sequation*}
		\baL^{\TT}=\begin{bmatrix}1&\lambda_{21}&\lambda_{31}&\lambda_{41}&0&0&0&0&0&0\\0&0&0&0&1&\lambda_{62}&\lambda_{72}&0&0&0\\0&0&0&0&0&0&0&1&\lambda_{93}&\lambda_{10,3}\end{bmatrix}
	\end{sequation*}  
	where $1$'s are fixed to introduce a scale to latent variables.
	\item We can also allow $\lambda_{11}$, $\lambda_{52}$, and/or $\lambda_{83}$ to be unknowns, and fix $\diag(\Cov(\bao))=I_q$, such that $\Cov(\bao)$ is a correlation matrix. But it is not convenient for identifying an SEM with StEq, and induces complication in the Bayesian analysis.  
\end{itemize}


\begin{exbox}{}
	\begin{example}\label{ex:kidney}\rm
		Study the kidney disease of type 2 diabetic patients. We observe: plasma creatine (PCr), urinary albumin creatinine ratio (ACR), systolic blood pressure (SBP), diastolic blood pressure (DBP), body mass index (BMI), waist hip ratio (WHR), glycated hemoglobin (HbAlc), fasting plasma glucose (FPG). Group 
		\begin{itemize}
			\item \{PCr, ACR\}: `kidney disease (KD)'
			\item \{SBP, DBP\}: `blood pressure (BP)'
			\item \{BMI, WHR\}: `obesity (OB)'
			\item \{HbA1c, FPG\}: `glycemic control (GC)'
		\end{itemize}
		$\y=(\rm PCr, ACR, SBP, DBP, BMI, WHR)^\TT$, $\bao=(\rm KD,BP,OB)^\TT$, $\bet=\rm KD$, $\bax=(\rm BP,OB)^\TT$, $p=6$, $q=3,q_1=1,q_2=2$. Then 
		\begin{sequation*}
			\text{(MeaEq)}\qquad\qquad \begin{bmatrix}\mathrm{PCr}\\\mathrm{ACR}\\\mathrm{SBP}\\\mathrm{DBP}\\\mathrm{BMI}\\\mathrm{WHR}\end{bmatrix}=\begin{bmatrix}\mu_{1}\\\mu_{2}\\\mu_{3}\\\mu_{4}\\\mu_{5}\\\mu_{6}\end{bmatrix}+\begin{bmatrix}\lambda_{11}&0&0\\\lambda_{21}&0&0\\0&\lambda_{32}&0\\0&\lambda_{42}&0\\0&0&\lambda_{53}\\0&0&\lambda_{63}\end{bmatrix}\begin{bmatrix}\mathrm{KD}\\\mathrm{BP}\\\mathrm{OB}\end{bmatrix}+\begin{bmatrix}\epsilon_{1}\\\epsilon_{2}\\\epsilon_{3}\\\epsilon_{4}\\\epsilon_{5}\\\epsilon_{6}\end{bmatrix}.
		\end{sequation*}
		We know that KD is only linked with PCr and ACR.
		\begin{sequation*}
			\text{(StEq)}\qquad\qquad \mathrm{KD}=\gamma_{1}\mathrm{BP}+\gamma_{2}\mathrm{OB}+\delta.
		\end{sequation*}
		Suppose we wish to study the effects of $\bax$ on $\bet=(\mathrm{KD},\eta_A)^\TT$, $q_1=2$,   
		\begin{sequation*}
			\begin{pmatrix}\mathrm{KD}\\\eta_{A}\end{pmatrix}=\begin{pmatrix}0&0\\\pi&0\end{pmatrix}\begin{pmatrix}\mathrm{KD}\\\eta_{A}\end{pmatrix}+\begin{pmatrix}\gamma_{1}&\gamma_{2}\\\gamma_{3}&\gamma_{4}\end{pmatrix}\begin{pmatrix}\mathrm{BP}\\\mathrm{OB}\end{pmatrix}+\begin{pmatrix}\delta\\\delta_{A}\end{pmatrix}.
		\end{sequation*}
	\end{example}
\end{exbox} 

\begin{defbox}{More SEM models}
	\begin{definition}\label{def:moreSEM}  
	Extensions of StEq:
	\begin{itemize}
		\item \red{Basic} StEq: ${\bet}_{q_1} = \baG_{q_1\times q_2} {\bax}_{q_2} + \bad_{q_1}$.
		
		\item \red{An extension}: 
		\begin{sequation*}
			{\bet}_{q_1}=\baP_{q_1\times q_1}{\bet}_{q_1}+\baG_{q_1\times q_2} {\bax}_{q_2}+\bad_{q_1},
		\end{sequation*}
		where $\baP$ is a matrix of unknown coeffcients such that $I_{q_1}-\baP$ is nonsingular and the diagonal elements of $\baP$ are zero. 

		\item Add \red{fixed known covariates} $\d$: 
		\begin{sequation*}
			\bet_{q_1}=\B_{q_1\times r_2}\d_{r_2} + \baP_{q_1\times q_1}\bet_{q_1}+\baG_{q_1\times q_2}\bax_{q_2}+\bad_{q_1}.
		\end{sequation*}
		\item Add \red{nonlinear structure}: 
		\begin{sequation}\label{eq:nonlinearStEq}
			\bet_{q_1}=\B_{q_1\times r_2}\d_{r_2}+ \baP_{q_1\times q_1}\bet_{q_1}+\baG_{q_1\times t}\F_{t}(\bax_{q_2})+\bad_{q_1},
		\end{sequation}
		where $\F(\bax)=(f_1(\bax),\ldots,f_t(\bax))^\TT$ with nonzero, known, and linearly independent differentiable functions $f_1,\ldots,f_t$, $t\geq q_2$. 
		To identity StEq, structures like $\F_1(\bax)=(\xi_1,\xi_2,\xi_1^2,\xi_1^2)^\TT$ and $\F_2(\bax)=(\xi_1,\xi_2,\xi_1\xi_2,0)^\TT$ are not allowed.
		
		\item \red{Combine $\d$ and $\F$}: 
		\begin{sequation}\label{eq:nonlinearStEq_comb}
			\bet_{q_1}=\baP_{q_1\times q_1}\bet_{q_1}+\baL_{\bao,q_1\times t}\G_t(\d,\bax)+\bad_{q_1},
		\end{sequation}
		where $\G(\d\bax)=(g_1(\d,\bax),\ldots,g_t(\d,\bax))^\TT$ is a vector-valued function with nonzero, known, and linearly independent differentiable functions.  
		\eqref{eq:nonlinearStEq} can be obtained by letting $\baL_\bao=(\B,\baG)$ and $\G(\d,\bax)=(\d^\TT,\F(\bax)^\TT)^\TT$.  
	\end{itemize}

	An Extension of MeaEq:
	\begin{itemize}
		\item Add fixed known covariates $\c$ (If $\bam_p$ is included, then $\c=[1,\c_2]^\TT$.)
		\begin{sequation}\label{eq:covaMeaEq}
			\y_{p}=\A_{p\times r_1}\c_{r_1}+\baL_{p\times q}\bao_{q}+\bae_{p}.
		\end{sequation}
	\end{itemize}
	\end{definition}
\end{defbox}
Let $\baL_k^\TT$ be the kth row of $\baL$, and $\baL_k^\TT=(\baL_{k\bet}^\TT,\baL_{k\bax}^{\TT})$ be a partition correspondings to the partition of $\bao=(\bet^\TT,\bax^\TT)^\TT$. 
For StEq \eqref{eq:nonlinearStEq} with $r_2=0$ and MeaEq \eqref{eq:linearSEM},
\begin{sequation*}
	\begin{aligned}
	\bbE(\bax)&=\0_{q_2},\quad \bbE(\bet)=[(I_{q_1}-\baP)^{-1}\baG]\bbE(\F(\bax)),\\
	\bbE(y_k)&=\mu_k+\baL_{k\bet}^\TT[(I_{q_1}-\baP)^{-1}\baG]\bbE(\F(\bax)).
	\end{aligned}
\end{sequation*}   
For StEq \eqref{eq:nonlinearStEq_comb} and MeaEq \eqref{eq:covaMeaEq}, let $\A_k^\TT$ be the kth row of $\A$,
\begin{sequation*}
	\bbE(y_k)=\A_k^\TT\c+\baL_{k\bet}^\TT\bbE(\bet)=\A_k^\TT\c+\baL_{k\bet}^\TT[(I_{q_1}-\baP)^{-1}\baL_{\bao}]\bbE(\G(\d,\bax)).
\end{sequation*}  

\noindent\underline{\textbf{Issues of developing a comprehensive SEM}}:
\begin{enumerate}
	\item Make sure the sample size of $\y$ is large enough to achieve accurate statistical results.
	\item If the size of the proposed SEM and the number of parameters are large, we may encounter diffculties in achieving convergence of the related computing algorithm for obtaining statistical results.
	\item So far, the most general SEM is MeaEq \eqref{eq:covaMeaEq} and StEq \eqref{eq:nonlinearStEq_comb}:
	\begin{sequation*}
		\y=\A\c+\baL\bao+\bae,\qquad \bet=\baP\bet+\baL_{\bao}\G(\d,\bax)+\bad.
	\end{sequation*} 
	It has limitations. As $\G(\d,\bax)$ does not involve any $\eta_k$ in $\bet$, nonlinear terms related to $\eta_k$ cannot be used to predict the other $\eta_{k'}$'s, i.e., $\eta_k$ cannot be accommodated in $\G(\d,\bax)$ and nonlinear effects of $\eta_k$ on $\eta_{k'}$ cannot be assessed.         
	
\end{enumerate}

\section{Bayesian methods for estimating SEM}\label{sec:Bayes_est_SEM}
\underline{\textbf{Motivation}}:
A traditional method is the \textit{covariance structure approach}, which focuses on fitting the covariance structure under the proposed model to the sample covariance matrix $\S$.
\begin{itemize}
	\item In many complex situations, deriving the explicit covariance structure $\Sigma=\Cov(\y)$ or obtaining an appropriate $\S$ is difficult (e.g., when $\exists$ missing data).
\end{itemize}
\underline{\textbf{Advarntages of Bayesian estimates (BE)}}: $\log\bbP(\baq\mid\Y)=\log\bbP(\Y\mid\baq)+\log\bbP(\baq)+\mathrm{const}$
\begin{enumerate}
	\item Methods are based on the first moment properties of $\y$ which are simpler than the second moment properties of $\S$. 
	Hence, it has potential to be applied to more complex situations.

	\item It estimates $\bao$ directly, which cannot be obtained with classical methods.
	
	\item As $n\to \infty$, $\log\bbP(\Y\mid\baq)$ could dominate $\log\bbP(\baq)$, hence the BE have the same optimal properties
	as the MLE. 

	\item It allows the use of genuine prior information for producing better results. 
	With small or moderate sample sizes, $\bbP(\baq)$ plays a more substantial role in BE than $\bbP(\Y\mid\baq)$, and is useful for achieving better results. 
	
	\item It provides more easily assessable statistics for goodness-of-fit and model comparison, and also other useful statistics such as the posterior mean and percentiles. (Don't need to derive the asymptotic distributions.)
	\item It can give more reliable results for small samples.
\end{enumerate}

\subsection{Priors for SEM}\label{sec:prior_SEM}
\noindent \underline{\textbf{Informative prior}}:
\begin{assbox}{Conjugate prior for SEMs}
	\begin{assumption}\label{ass:conj_prior_SEM}
		In developing the Bayesian methods for analyzing SEMs, we usually assign fixed known values to the hyperparameters in the conjugate prior distributions.
		Consider
		\begin{sequation*}
			\begin{aligned}
			\y_i&=\bam+\baL\bao_{i}+\bae_{i}, \\
			\bet_{i}&=\B\d_{i}+ \baP\bet_{i}+\baG\F(\bax_{i})+\bad_{i}=\baL_{\bao}\G(\bao_i)+\bad_i,
			\end{aligned}
	\end{sequation*}
	where $\baL_{\bao}=(\B,\baP,\baG)\in\bbR^{q_1\times (r_2+q_1+t)}$, and $\G(\bao_i)=(\d_i^\TT,\bet_i^\TT,\F(\bax_i)^\TT)^\TT\in\bbR^{r_2+q_1+t}$.     
	Assumption \ref{ass:linearSEM} is satisfied.
	${\bax}_i\simiid\rmN[\0_{q_2},\baF]$, $\bae_i\simiid\rmN[\0_p,\baY_\bae=\diag(\psi_{\bae k})]$,and $\bad_i\simiid\rmN[\0_{q_1},\baY_{\bad}=\diag(\psi_{\bad k})]$.
	\begin{itemize}
		\item Prior (conjugate) for $\baq_\y=(\bam,\baL,\baY_{\bae})$: let $\baL_k^\TT$ be the kth row of $\baL$,
		\begin{equation*}
			\begin{aligned}
			\psi_{\bae k}&\sim\IG(\alpha_{0 \bae k}, \beta_{0 \bae k}), \quad [\baL_k\mid\psi_{\bae k}]\sim \rmN_q(\baL_{0k},\psi_{\bae k}\H_{0\y k}), \ k=1,\ldots,p,\\
			\bam&\sim\rmN_p(\bam_0,\baS_0).
			\end{aligned}
		\end{equation*}
		\item Prior (conjugate) for $\baq_\bao=(\baL_\bao,\baY_{\bad},\baF)$: let $\baL_{\bao k}^\TT$ be the kth row of $\baL_{\bao}$, $\baY_{\bad}$,
		\begin{equation*}
			\begin{aligned}
				\baF&\sim \IW_{q_2}(\R_0^{-1},\rho_0), \text{ or } \baF^{-1}\sim\mathrm{W}_{q_2}(\R_0,\rho_0)  \\
				\psi_{\bad k}&\sim \IG(\alpha_{0 \bad k},\beta_{0 \bad k}),\quad [\baL_{\bao k}\mid \psi_{\bad k}]\sim \rmN_{r_2+q_1+t}(\baL_{0\bao k},\psi_{\bad k}\H_{0\bao k}), \ k=1,\ldots,q_1.
			\end{aligned}
		\end{equation*} \
		\item Assume the prior $\baq_\y\indep\baq_\bao$. 
	\end{itemize}
	\end{assumption}
\end{assbox}



{{Hyperparameter selection}}: If we have good prior information about a parameter, select the prior
distribution with a small variance. E.g., 
\begin{itemize}
	\item if $\baL_k\approx\baL_{0k}$, then $\H_{0yk}=0.5 I_q$. If not, select the prior with a larger variance;
	\item since $\epsilon_{ik}\sim \rmN(0,\psi_{\bae k})$, if the variation is small, $\psi_{\bae k}$ is small, then choose small $\bbE(\psi_{\bae k})=\beta_{0\bae k}/(\alpha_{0\bae k}-1)$ and $\Var(\psi_{\bae k})=\beta_{0\bae k}^2/\{(\alpha_{0\bae k}-1)^2(\alpha_{0\bae k}-2)\}$;
	\item if $\baF\approx \baF_{0}$, since $\bbE(\baF)=\R_0^{-1}/(\rho_0-q_2-1)$, choose $\R_0^{-1}=(\rho_0-q_2-1)\baF_0$.   
\end{itemize}

\noindent\textbf{\underline{Noninformative prior (Jeffrey)}}: If information is not available and the sample size is small, 
\begin{sequation*}
	\begin{aligned}
		&\bbP(\baL,\baY_{\bae})\propto \bbP(\psi_{\bae1},\cdots,\psi_{\bae p})\propto\prod_{k=1}^{p}\psi_{\bae k}^{-1},\quad 
		\bbP(\baL_{\bao},\baY_{\bad})\propto \bbP(\psi_{\bad1},\cdots,\psi_{\bad q_{1}})\propto\prod_{k=1}^{q_{1}}\psi_{\bad k}^{-1},\\
		&\bbP(\baF)\propto|\baF|^{-(q_2+1)/2}.\end{aligned}
\end{sequation*}
If the sample size is large, can use a portion of the data to estimate $\baL_{0k}$, $\baL_{0\bao k}$ and $\baF_0$ with noninformative priors. If the sample size is moderate, can use the same data twice.  


\subsection{Bayesian estimation using MCMC}
\textbf{\underline{Model}}: Linear SEM with fixed covariates without intercept:
\begin{equation*}
	\begin{aligned}
		\y_i&=\baL\bao_i+\bae_i, \\
		\bet_i&=\B\d_i+\baP\bet_i+\baG\bax_i+\bad_i=\baL_{\bao}\v_i+\bad_i,
	\end{aligned}
\end{equation*}
where $\baL_{\bao}=(\B,\baP,\baG)\in\bbR^{q_1\times (r_2+q_1+q_2)}$, and $\v_i=(\d_i^\TT,\bet_i^\TT,\bax_i^\TT)^\TT\in\bbR^{r_2+q_1+q_2}$. 
That is, assume $\bam=\0_p$ and $\F(\bax_i)-\bax_i$. 

Denote data $\Y=(\y_1,\ldots,\y_n)=(\Y_1,\ldots,\Y_p)^\TT\in\bbR^{p\times n}$, $\V=(\v_1,\ldots,\v_n)=(\V_1,\ldots,\V_{r_2+q_1+q_2})^\TT$, $\baX_{k}=(\eta_{1k},\cdots,\eta_{nk})^\TT$ for $k=1,\ldots,q_1$, matrix of latent variables $\baO=(\bao_1,\ldots,\bao_n)\in\bbR^{q\times n}$, $\baO_1=(\bet_1,\ldots,\bet_n)$, $\baO_2=(\bax_1,\ldots,\bax_n)$, and 
\begin{sequation*}
	\baq=(\baL,\B,\baP,\baG,\baF,\baY_\bae,\baY_\bad)=(\underbrace{\baL,\baY_\bae}_{\baq_\y},\underbrace{\baL_\bao,\baF,\baY_\bad}_{\baq_{\bao}}).
\end{sequation*}   
\begin{thmbox}{}
	\begin{proposition}\label{prop:post_linearSEM}
		The above model has the following posterior distributions:
		\begin{enumerate} 
			\item Conditional distribution $\red{\bbP(\baO\mid\Y,\baq)}=\prod_{i=1}^n\bbP(\bao_i\mid\y_i,\baq)\propto\prod_{i=1}^{n}\bbP(\bao_i\mid\baq)\bbP(\y_i\mid\bao_i,\baq)$, where 
			\begin{equation*}
				\begin{aligned}
					[\bao_i\mid\baq]&\sim\rmN_q(\bam_{\bao_i},\baS_\bao), \quad 
					[\y_i\mid\bao_i,\baq]\sim\rmN_{p}(\baL\bao_i,\baY_\bae),\\
					[\bao_i\mid\y_i,\baq]&\sim\rmN_q({\baS^{*}}^{-1}(\baS_{\bao}^{-1}\bam_{\bao_i}+\baL^\TT\baY_{\bae}^{-1}\y_i),{\baS^{*}}^{-1})
				\end{aligned}
			\end{equation*}
			where 
			\begin{equation*}
				\begin{aligned}
					\baP_0&=I_{q_1}-\baP, \ \bam_{\bao_i}=\begin{pmatrix}\baP_0^{-1}\B \d_i\\\0_{q_2}\end{pmatrix}, \ \baS_\bao=\begin{bmatrix}\baP_0^{-1}(\baG\baF\baG^\TT+\baY_\bad)\baP_0^{-\TT}&&\baP_0^{-1}\baG\baF\\\baF\baG^\TT\baP_0^{-\TT}&&\baF\end{bmatrix},\\
					\baS^{*}&=\baS_{\bao}^{-1}+\baL^\TT \baY_{\bae}^{-1}\baL.
				\end{aligned}
			\end{equation*}
			\item Assume all elements of $\baL_k$ and $\baL_\bao$ are unknown, conditional distribution $\red{\bbP(\baq\mid\Y,\baO)}=\bbP(\baq_\y\mid\Y,\baO)\bbP(\baq_\bao\mid\Y,\baO)$, where we can show $[\baL_k,\psi_{\bae k}\mid \Y,\baO] \indep$ and $[\baL_{\bao k},\psi_{\bad k}\mid \baO]\indep$, and  
			\begin{sequation*}
				\begin{aligned}
					\bbP(\baq_\y\mid\Y,\baO)&\propto \prod_{k=1}^{p} \bbP(\baL_k,\psi_{\bae k}\mid \Y,\baO),\\
					\bbP(\baq_\bao\mid\Y,\baO)&\propto \mbk{\prod_{k=1}^{q_1}\bbP(\baL_{\bao k},\psi_{\bad k}\mid \baO)}\bbP(\baF\mid\baO_2)
				\end{aligned}
			\end{sequation*} 
			where 
			\begin{sequation*}
				\begin{aligned}
					\bbP(\baL_k,\psi_{\bae k}^{-1}\mid\Y,\baO)&\propto \rmN_q(\a_k,\psi_{\bae k}\A_k)\cdot\Ga(n/2+\alpha_{0\bae k},\beta_{\bae k}), \\
					\bbP(\baL_{\bao k},\psi_{\bad k}^{-1}\mid \baO)&\propto\rmN_{r_2+q_1+q_2}(\a_{\bao k},\psi_{\bad k}\A_{\bao k})\cdot\Ga(n/2+\alpha_{0\bad k}, \beta_{\bad k}),\\
					[\baF\mid\baO_2]&\sim \IW_{q_2}[(\baO_2\baO_2^\TT+\R_0^{-1}),n+\rho_0].
				\end{aligned}
			\end{sequation*}
			where 
			\begin{equation*}
				\begin{aligned}
					\A_k&=(\H_{0yk}^{-1}+\baO\baO^\TT)^{-1}, \ \a_k=\A_k(\H_{0yk}^{-1}\baL_{0k}+\baO\Y_k), \\
					\beta_{\bae k}&=\beta_{0\bae k}+\frac{1}{2}(\Y_k^{\TT}\Y_{k}-\a_{k}^{\TT}\A_{k}^{-1}\a_{k}+\baL_{0k}^{\TT}\H_{0yk}^{-1}\baL_{0k}), \\
					\A_{\bao k}&=(\H_{0\bao k}^{-1}+\V_k\V_k^\TT)^{-1},\ \a_{\bao k}=\A_{\bao k}(\H_{0\bao k}^{-1}\baL_{0\bao k}+\V_k\baX_k), \\
					\beta_{\bad k}&=\beta_{0\bad k}+\frac{1}{2}(\baX_{k}^{\TT}\baX_{k}-\a_{\bao k}^{\TT}\A_{\bao k}^{-1}\a_{\bao k}+\baL_{0\bao k}^{\TT}\H_{0\bao k}^{-1}\baL_{0\bao k}).
				\end{aligned}
			\end{equation*}
		\end{enumerate}
	\end{proposition}
\end{thmbox}

\begin{remark}\label{rmk:post_SEM}
	For general nonlinear SEMs with MeaEq \eqref{eq:covaMeaEq} and StEq \eqref{eq:nonlinearStEq_comb}, we can defind $\u=[\c^\TT, \bao^{\TT}]^\TT\in\bbR^{r_1+q}$ and use similar procedure to derive the full conditional distributions. 
	But by the nonlinear structure $\G(\bao)$, $\bbP(\baO\mid\Y,\baq)$ may not have closed form like normal, while $\bbP(\baq\mid\Y,\baO)$ is not affected and keeps normal-Gamma.
	To handle fixed parameters, see Appendix 3.3 in \cite{lee2012basic} and Sec 4.3.1 and Appendix 4.3 in \cite{lee2007structural}. Also see STAT5020 HW1 Q3.
\end{remark}


\section{Bayesian model comparison}\label{sec:SEM_model_comp}



\section{Hierarchical and Multisample
Data}
\begin{defbox}{Two-level nonlinear SEM with mixed type variables}
	\begin{definition}
	Consider a collection of $p$-variate random vectors $\u_{gi}$, $i=1,\ldots,N_g$, nested within groups $g=1,\ldots,G$.   
	\begin{align*}
		\text{(Within-groups) }\u_{gi}&=\v_{g}+\baL_{1g}\bao_{1gi}+\bae_{1gi},\quad g=1,\cdots,G,\quad i=1,\cdots,N_{g},\\
		\text{(Between-groups) }\v_g&=\bam+\baL_2\bao_{2g}+\bae_{2g},\quad g=1,\ldots,G
	\end{align*}
	where $\baL_{1g}\in\bbR^{p\times q_1}$, $\bae_{1gi}\in\bbR^{q_1}$, $\bae_{1gi}\in\bbR^{p}\sim\rmN(\0,\baY_{1g})$ independent of $\bao_{1gi}$, where $\baY_{1g}$ is diagonal. Then 
	\begin{equation*}
		\mathbf{u}_{\mathrm{gi}}=\mathbf{\mu}+\mathbf{\Lambda}_{2}\mathbf{\omega}_{2\mathrm{g}}+\mathbf{\epsilon}_{2\mathrm{g}}+\mathbf{\Lambda}_{1\mathrm{g}}\mathbf{\omega}_{1\mathrm{gi}}+\mathbf{\epsilon}_{1\mathrm{gi}}.
	\end{equation*}  
	The SEM is 
	\begin{equation*}
		\begin{aligned}&\eta_{1gi}=\Pi_{1g}\eta_{1gi}+\Gamma_{1g}\mathbf{F}_1(\xi_{1gi})+\delta_{1gi},\\&\eta_{2g}=\Pi_2\eta_{2g}+\Gamma_2\mathbf{F}_2(\xi_{2g})+\delta_{2g},\end{aligned}
	\end{equation*}
	\end{definition}
\end{defbox}



\bibliographystyle{abbrv}
\bibliography{mybib}
%%% end of doc
\end{document}