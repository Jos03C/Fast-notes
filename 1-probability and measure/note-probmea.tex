\documentclass[10pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{scribe}
\usepackage{listings}
\usepackage{eufrak}

\newenvironment{hints}{\textbf{Hints.}}{}

\Scribe{Zhuohua Shen}
\Lecturer{}
\LectureNumber{}
\LectureDate{2024}
\LectureTitle{Probability Theory}

\lstset{style=mystyle}

\begin{document}
	\MakeScribeTop
	\tableofcontents

%#############################################################
%#############################################################
%#############################################################
%#############################################################

References: MAT3280, STAT5005 and \textit{Probability: Theory and Examples}, 4th edition, by Richard Durrett, published by Cambridge University Press.

\section{Preliminary}\label{sec:premi}
\subsection{Riemann--Stieltjes integrals}\label{sec:RS-int}
\underline{\textbf{Goal}}: Consider a closed interval $[a,b], a<b$. Let $\alpha(x)$ be a non-decreasing function on $[a,b]$. $f\in \mathrm{bdd[a,b]}$.  Want to define RS integral $\int_{a}^{b}f(x)\rmd\alpha(x)$. 

Let $P:a=x_0<x_1<\cdots<x_n=b$ be a partition of $[a,b]$. Let $M_k (m_k):=\sup (\inf)\{f(x):x_{k-1}\leq x\leq x_k\}$, $t_k$ is chosen arbitrarily in $[x_{k-1},x_k]$, $k=1,2,\ldots, n$, we define
\begin{itemize}
	\item upper sum $U(P,f,\alpha):=\sum_{k=1}^{n} M_k \cdot (\alpha(x_k)-\alpha(\alpha_{k-1}))$,
	\item lower sum $L(P,f,\alpha):=\sum_{k=1}^{n} m_k \cdot (\alpha(x_k)-\alpha(\alpha_{k-1}))$,
	\item \red{Riemann--Stieltjes sum} $S(P,f,\alpha):=\sum_{k=1}^{n} f(t_k) \cdot (\alpha(x_k)-\alpha(\alpha_{k-1}))$.
\end{itemize}
Sandwiching inequalities: $L(P,f,\alpha)\leq S(P,f,\alpha)\leq U(P,f,\alpha)$, $\forall P$. 
\begin{defbox}
	\begin{definition}[Riemann--Stieltjes integrable]\label{def:RSint}
		If $\lim_{\norm{P}\to \infty }L(P,f,\alpha)=U(P,f,\alpha)$, then we say $f$ is \red{Riemann--Stieltjes (RS) integrable} on $[a,b]$ w.r.t. $\alpha(x)$, $f\in\scrR(\alpha)$. We denote the common limit as 
		\begin{sequation*}
			\int_{a}^{b}f\rmd\alpha \ \mathrm{\ or\ } \int_{a}^{b} f(x) \rmd(\alpha(x)).
		\end{sequation*}
		An improper RS integral is defined by the double limit (provided exists and finite, i.e., converges)
		\begin{sequation*}
			\int_{-\infty }^{\infty } f\rmd \alpha := \lim_{(a,b)\to(-\infty ,\infty )}\int_{a}^{b} f\rmd \alpha.
		\end{sequation*}
	\end{definition}
\end{defbox}
Note that if $\beta(x)=\alpha(x)+C$, then $\int_{a}^{b}f d\alpha = \int_{a}^{b} f d\beta$. RS integral can be defined more generally when $\alpha(x)$ is BV. RS integrals have the following properties:
\begin{enumerate}
	\item (\textbf{linearity}) $f,g\in\scrR(\alpha)$,  $\forall c,d\in\bbR$, $cf+dg\in\scrR(\alpha)$, $\int_{a}^{b}(cf+dg)d\alpha=c\int_{a}^{b}fd\alpha + d\int_{a}^{b}gd\alpha$; 
	\item (\textbf{monotonicity}) $f,g\in\scrR(\alpha)$, if $f\leq g,\forall x\in[a,b]$, then $\int_{a}^{b}fd\alpha\leq\int_{a}^{b}gd\alpha$;
	\item (\textbf{additivity of integration}) Let $a<c<b$, if $f\in\scrR(\alpha)$ on $[a,c]$ and on $[c,b]$, then $f\in\scrR(\alpha)$ on $[a,b]$, and $\int_{a}^{b}fd\alpha=\int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha$. 
	\item (\textbf{linearity of} $\balpha$) Suppose $\alpha_1(x)$ and $\alpha_2(x)$ are two non-decreasing functions, $f\in\scrR(\alpha_1),f\in\scrR(\alpha_2),$, then $f\in\scrR(\alpha_1+\alpha_2)$, and $\int_{a}^{b}fd(\alpha_1+\alpha_2)=\int_{a}^{b}fd\alpha_1+\int_{a}^{b}fd\alpha_2$.       
\end{enumerate}

\begin{thmbox}
	\begin{theorem}[Finite discontinuities]\label{thm:RSint-finitediscts}
		\rm $f\in\mathrm{bdd}[a,b]$, and there are finitely many points $c_1,c_2,\ldots,c_n\in[a,b]$ such that $f$ is discontinuous at each $c_i$. If $\alpha$ is continuous on $c_i,\forall i$, then $f\in\scrR(\alpha)$.   
	\end{theorem}

	\begin{theorem}[Continuous functions]\label{thm:RSint-cts}
		\rm Suppose $f\in \rmC[a,b]$ and let $\alpha\in\rmC^1[a,b]$ (or $\alpha'(x)$ Riemann integrable on $[a,b]$) be nondecreasing. If $f\in\scrR(\alpha)$, then the function $f\alpha'$ is Riemann integrable on $[a,b]$ and we have   
	{\setlength\abovedisplayskip{0.15cm}
	\setlength\belowdisplayskip{0.15cm}
	\begin{equation*}
		\int_{a}^{b} f\rmd \alpha = \int_{a}^{b}f(\alpha) \alpha'(x)\rmd x.
	\end{equation*}}
	\end{theorem}
\end{thmbox}

There are two techniques to compute RS integral:
\begin{enumerate}
	\item (integration by parts) $\int_{a}^{b}f\rmd \alpha=f\alpha \big|_{a}^b - \int_{a}^{b}\alpha\rmd f$;
	\item (change of variables) if $g$ is strictly increasing on $[c,d]$, $f\in\scrR(\alpha)$ on $[g(c),g(d)]$, let $h=f\circ g$, $\beta=\alpha\circ g$, then $h\in\scrR(\beta)$ on $[c,d]$, with $\int_{g(c)}^{g(d)}f\rmd \alpha=\int_{c}^{d}h\rmd \beta$.          
\end{enumerate}



\section{Measure Theory}\label{sec:measure}




\subsection{Probability spaces}\label{sec:prob-spaces}
\begin{defbox}
	\begin{definition}[Concepts]\label{def:spaces}
		\
		\begin{itemize}
			\item \red{Sample space} $\Omega$: a set.
			\item \red{Field/algebra} $\scrF'$: collection of subsets of $\Omega$ satisfying: (i)$\Omega\in\scrF'$, (ii) $A\in\scrF$ implies $A^c\in\scrF'$, (iii) $A,B\in\scrF'$ implies $A\cup B\in\scrF'$.  
			\item \red{$\sigma$-field/algebra} $\scrF$: collection of subsets of $\Omega$ satisfying: (i) $\Omega\in\scrF$, (ii) $A\in\scrF$ implies $A^c\in\scrF$, (iii) $A_i\in\scrF$ implies $\cup_{i=1}^\infty  A_i\in\scrF$.
			\item $(\Omega,\scrF)$ is a \red{measurable space}.
			\begin{itemize}
				\item A set in $\scrF$ is called \red{$\scrF$-measurable}, or simply measurable.
				\item If $\scrG$ is a subcollection of $\scrF$, we say that $\scrG$ is a \red{sub-$\sigma$-field} of $\scrF$ if $\scrG$ is also a $\sigma$-field.    
			\end{itemize}
			\item \red{Measure} $\mu:\scrF\to\bbR$, if (i) $\mu(A)\geq 0$, (ii) $\mu(\emptyset)=0$, (iii) $A_i\in\scrF$ disjoint, then $\mu(\biguplus_{i=1}^n A_i )=\sum_{i=1}^{n}\mu(A_i)$.             
			\begin{itemize}
				\item A measure $\mu$ is said to be \red{finite} if $\mu(\Omega)$ is finite.   
				\item If $\mu(\Omega)=1$, then $\mu$ is a \red{probability measure}. 
			\end{itemize}
		\end{itemize}
	\end{definition}
\end{defbox}

Let $\mu$ be a measure on $(\Omega,\scrF)$, it has the following properties:
\begin{itemize}
	\item[(a)] (\textbf{monotonicity}) if $A\subset B$, then $\mu(A)\leq\mu(B)$;
	\item[(b)] (\textbf{addition law}) $\forall A.B\in\scrF$, $\mu(A\cup B)=\mu(A)+\mu(B)-\mu(A\cap B)$;
	\item[(c)] (\textbf{sub-additivity}) $\mu(\cup_{i=1}^\infty A_i)\leq \sum_{i=1}^{\infty }\mu(A_i)$;
	\item[(d)] (\textbf{lower semi-continuity/continuity from
	below}) if $A_n\uparrow A=\cup_{n=1}^\infty A_n$, then $\mu(A_n)\uparrow \mu(A)$;
	\item[(e)] (\textbf{upper semi-continuity/continuity from above}) if $A_n\downarrow A=\cap_{n=1}^\infty A_n$ and $\mu(A_1)<\infty $, then $\mu(A_n)\downarrow \mu(A)$.            
\end{itemize}  

\begin{defbox}
	\begin{definition}[liminf and limsup]\label{def:liminfsup}
		Let $E_1,E_2,\ldots$ be an arbitrary sequence of subsets in a set $\Omega$. The \red{limit inferior} and \red{limit superior} of $(E_i)_{i\geq 1}$ are defined respectively as 
		\begin{sequation*}
			\liminf_i E_i:=\bigcup_{j=1}^\infty\bigcap_{k\geq j}E_k=\{E_i\ i.o.\}, \qquad \limsup_i E_i:=\bigcap_{j=1}^\infty\bigcup_{k\geq j}E_k=\{E_i\ e.v.\}
		\end{sequation*}
	\end{definition}
\end{defbox}
We have $\liminf_i E_i \subseteq \limsup_i E_i$. If equality holds, we say the \red{limit of $(E_i)_{i\geq 1}$} exists, and is defined as $\liminf_i E_i$ or $\limsup_i E_i$.    

\begin{defbox}
	\begin{definition}[generated $\sigma$-field]\label{def:generated-sigma-field}
		Let $\scrA$ be a collection of subsets of $\Omega$,  let $\sigma(\scrA)$ be the \red{$\sigma$-field generated by $\scrA$}, defined as 
		\begin{sequation*}
			\sigma(\scrA) = \bigcap_{\scrA\subseteq\scrF, \scrF \sigma-\text{field}} \scrF.
		\end{sequation*}   
	\end{definition}
\end{defbox}
It is well-defined because if $\{\scrF_i:i\in I\}$ are all $\sigma$-fields, $I$ can be uncountable, then $\cap_{i\in I}\scrF_i$ is also a $\sigma$-field. The $\sigma(\scrA)$ is the smallest $\sigma$-field containing $\scrA$. If $\Omega=\bbR$, $\scrA=\{(a,b):-\infty <a\leq b< \infty  \}$, then $\sigma(\scrA)$ is called the \red{Borel field/algebra}, written as $\scrB(\bbR)$. Likewise, define $\scrB(\bbR^d)$ the $\sigma$-algebra generated by the open balls in $\bbR^d$. A set in $\scrB(\bbR)$ is called a \red{Borel set}. 
\begin{itemize}
	\item $\scrB(\bbR)=\sigma(\{(a,b):a<b\})$, which can be $\{[a,b]\}$, $\{(a,b]\}, \ldots$
	\item $\scrB(\bbR^d)$ can also be generated by open $d$-dimensional open boxes in the form $(a_1,b_1)\times(a_2,b_2)\times\cdots\times(a_d,b_d)$.   
\end{itemize}   

\subsubsection{Measure extension theorem}\label{sec:mea-extension}

\begin{defbox}
	\begin{definition}[Concepts]\label{def:premea-extension-sigmaFinite}
		\
		A \red{pre-measure} $\mu_0:\scrF_0\to [0,\infty ]$ defined on a field $\scrF_0$ is a set function satisfying (i) $\mu_0(\emptyset)=0$ and (ii) if $A_i\in\scrF_0$ mutually disjoint sets in $\Omega$ and $\biguplus_i A_i\in\scrF_0$, then $\mu_0(\biguplus_{i=1}^\infty A_i)=\sum_{i=1}^{\infty }\mu_0(A_i)$.    

		Let $\scrF$ be a $\sigma$-field containing $\scrF_0$. We say that $\mu:\scrF\to[0,\infty ]$ is an \red{extension} of $\mu_0$ if $\mu$ is a measure and satisfies $\mu(E)=\mu_0(E)$, $\forall E\in\scrF_0$.     

		$\mu_0$ is said to be \red{finite} if $\mu_0(\Omega)<\infty $. It is said to be \red{$\sigma$-finite} if $\exists$ at most countably many sets $\Omega_i\in\scrF_0$ such that $\Omega=\cup_{i=1}^\infty \Omega_i$ and $\mu_0(\Omega_i)<\infty $ for all $i$.            
	\end{definition}
\end{defbox}
We note that a probability measure is automatically $\sigma$-finite. WLOG, we may assume $\Omega_i$'s form a partition of $\Omega$. If $\Omega_i$'s are not mutually disjoint, set $\Omega_i=\Omega_i\setminus(\cup_{j=1}^{i-1}\Omega_j)$. 

\begin{thmbox}
	\begin{theorem}[Measure extension theorem/Hahn-Kolmogorov-Carathéodory]\label{thm:extension}
		\rm Let $\scrF_0$ be a field on $\Omega$ and $\mu_0$ be a premeasure on $\scrF_0$. There is a measure $\mu$ defined on $\sigma(\scrF_0)$ that extends to $\mu_0$. Moreover, if $\mu_0$ is $\sigma$-finite, then the extension is unique.         
	\end{theorem}
\end{thmbox}

\begin{itemize}
	\item (Counting measure) $\Omega=\bbN^+$, $\scrF_0$ is the collection of all finite and co-finite sets. $\mu_0(E)=\infty $ if $E$ is infinite and $\abs{E}$ if $E$ is finite. $E=\{2,4,6,8,\ldots\}\notin \scrF_0$, $\mu_0(E)$ is undefined. But it is $\sigma$-finite by taking $\Omega_i=\{i\}$. $\mu_0$ can be extended to the power set $2^{\bbN}$, the counting measure on $\bbN^+$.   
	\item If $\Omega=\bbR$, $\scrF_0=\{\text{finite union of }(a,b], a\in\bbR\cup\{-\infty \}, b\in\bbR\cup\{\infty \}\}$, then $\scrF_0$ is a field. Define $\mu_0$ on $\scrF_0$ by $\mu_0(\biguplus_{i=1}^n (a_i,b_i])=\sum_{i=1}^{n}(b_i-a_i)$ the total length of the set, if all $a_i$ and $b_i<\infty $. Otherwise, the length is $\infty $. It is $\sigma$-finite by taking $\Omega_i=(-i,i]$. $\mu_0$ can be extended to $\scrB(\bbR)$, and the extension is unique, called the \red{Borel measure} on $\bbR$, denoted by $\lambda$.      
\end{itemize}

\subsubsection{Lebesgue--Stieltjes measures}\label{sec:LS-measure}
\underline{\textbf{Stieltjes measure function}}
\begin{defbox}
	\begin{definition}[distribution function]\label{def:dist}
		Let $(\bbR,\scrB(\bbR),\bbP)$ be a probability space. The \red{distribution function} induced by $P$ is defined as 
		\begin{equation*}
			F(x):=\bbP((-\infty ,x]),\quad x\in\bbR.
		\end{equation*}  
	\end{definition}
\end{defbox}
df vs cdf: cdf is for rv, while df is for a probability measure.
\begin{thmbox}
	\begin{theorem}[Properties of df]\label{thm:df-prop}\rm
		The df $F(x)$ of a probability measure satisfies the following properties:
		\begin{enumerate}
			\item $F(x)$ is non-decreasing;
			\item $F(x)$ is right-continuous;
			\item $\lim_{x\to \infty }F(x)=1$;
			\item $\lim_{x\to -\infty}F(x)=0$.  
		\end{enumerate} 
	\end{theorem}
\end{thmbox}

\begin{defbox}
	\begin{definition}[Stieltjes measure function]\label{def:Stieltjes-measure}
		A function $F:\bbR\to\bbR$ is called a \red{Stieltjes measure function} if 
		\begin{enumerate}
			\item $F$ is non-decreasing;
			\item $F$ is right-continuous.  
		\end{enumerate} 
	\end{definition}
\end{defbox}

\noindent\underline{\textbf{Lebesgue-Stieltjes measures}}
Given a Stieltjes measure function, we can apply the measure extension theorem \ref{thm:extension} to construct a measure on $\scrB(\bbR)$. Intuitively, the Lebesgue--Stieltjes measure $\mu$ assigns a length to each Borel set on the real line, where the length of $(a,b]$ is given by $F(b)-F(a)$.
\begin{thmbox}
	\begin{theorem}\label{thm:LSmeasure}\rm
		Let $F$ be a Stieltjes measure function defined on $\bbR$. Then $\exists$ a unique measure $\mu$ defined on $\scrB(\bbR)$ such that $\mu((a,b])=F(b)-F(a)$. The measure $
		\mu$ is called the \red{Lebesgue-Stieltjes (LS) measure} induced by $F$.        
	\end{theorem}
\end{thmbox}

For example,
\begin{itemize}
	\item (Lebesgue measure) take $F(x)=x$, then the LS measure on $\scrB(\bbR)$ is Lebesgue measure, denoted by $\lambda$, $\lambda([a,b])=b-a$. It is translation-invariant;
	\item (Uniform distribution) $F(x)=x\1_{\{0\leq x\leq 1\}}+ \1_{\{1<x\}}$, then the LS measure $\mu$ is the uniform distribution on $[0,1]$, $\mu((a,b])=b-a$ if $0<a<b<1$;
	\item (Continuous distribution) Let $f(x)\geq 0$ be a pdf, i.e., Riemann-integrable on $R$, and $\int_{\bbR}f(x)\rmd x=1$. Then $F(x):=\int_{-\infty }^{x}f(t)\rmd t$ is a Stieltjes measure function. Denote the LS measure by $P$, $P((a,b))=\int_{a}^{b}f(t)\rmd t$, $a<b$;   
	\item (Dirac measure, point mass) $F(x)=\1_{\{x\geq x_0\}}$. $P(A)=\1_{\{x_0\in A\}}$;
	\item (Discrete distribution) $i\in\bbN^+$, $p_i\geq 0$, $\sum_{i=1}^{\infty }p_i=1$. Define $F(x)=\sum_{i=1}^{\infty }p_i\1_{\{x\geq x_i\}}$. Then LS measure $P(\{x\})=\sum_{i=1}^{\infty }p_i\1_{\{x=x_i\}}$.
\end{itemize}

\subsubsection{Null sets and complete measures}\label{sec:null-complete}
Consider a probability space $(\Omega,\scrF,\mu)$. If $\mu(E)=0$, we hope $\forall E'\subseteq E$, $\mu(E')=0$. But $E'$ may not be measurable.
\begin{defbox}
	\begin{definition}\label{def:null-complete}
		Given a measure space $(\Omega,\scrF,\mu)$, we define a set $N\subseteq \Omega$ as a \red{null set} if $\exists E\in\scrF$ s.t. $N\subseteq E$ and $\mu(E)=0$. The measure space $(\Omega,\scrF,\mu)$ is said to be \red{complete} if all null sets are indeed $\scrF$-measurable. 
	\end{definition}
\end{defbox}     

\begin{thmbox}
	\begin{theorem}[Completion]\label{thm:null-completion}
		Let $(\Omega,\scrF,\mu)$ be a measure space and $\scrN$ be the set of all null sets. We can define a new collection of sets
		\begin{sequation*}
			\scrF':=\{A\cup N: A\in\scrF,\ N\in\scrN\}.
		\end{sequation*}
		We can extend the measure $\mu$ to a measure on $\scrF'$, denoted by $\mu'$, by 
		\begin{sequation*}
			\mu'(A\cup N):=\mu(A),
		\end{sequation*}   
		and the extended measure space $(\Omega,\scrF',\mu')$ is complete. 
	\end{theorem}
\end{thmbox}

By enlarging the $\sigma$-field in this way, we may assume that the measure is complete
without loss of generality.

\subsubsection{$\pi$-$\lambda$ theorem and uniqueness of measure extension}\label{sec:pi-lambda-thm}
We will use the term \red{set system} to refer to a collection of subsets in $\Omega$.
\begin{defbox}
	\begin{definition}\label{def:pi-lambda-sys}
		We define a \red{$\pi$-system} in $\Omega$ as a set system $\scrP$ that satisfies 
		\begin{sequation*}
			A,B\in\scrP \Rightarrow A\cap B\in\scrP. 
		\end{sequation*}  
		A \red{$\lambda$-system/Dynkin system} in $\Omega$ is a set system $\scrL$ that satisfies:
		\begin{enumerate}
			\item[(i)] $\Omega\in\scrL$;
			\item[(ii)] $A\in\scrL \Rightarrow \Omega\setminus A\in\scrL $;
			\item[(iii)] If $A_i\in\scrL$, $A_i\uparrow A$, then $A\in\scrL$.      
		\end{enumerate}  
	\end{definition}
\end{defbox} 
\begin{itemize}
	\item (iii) is equivalent to: If $A_i\in\scrL$ mutually disjoint, then $\cup_i A_i\in\scrL$.  
	\item If $\scrA$ is both $\pi$ and $\lambda$-system, then $\scrA$ is a $\sigma$-field.     
\end{itemize}

\begin{thmbox}
	\begin{theorem}[Dynkin's $\pi$-$\lambda$ theorem]\label{thm:pi-lambda-thm}\rm
		If $\scrP$ is a $\pi$-system and $\scrL$ is a $\lambda$-system, and $\scrP\subset \scrL$, then $\sigma(\scrP)\subset \scrL$.      
	\end{theorem}
\end{thmbox}
We can use the $\pi$-$\lambda$ theorem to prove the uniqueness of measure extension.

\begin{thmbox}
	\begin{theorem}[Uniqueness of measure extension]\label{thm:unique-measure-extension}\rm
		Suppose $\scrF_0$ is a field on a sample space $\Omega$, and $\mu_1$ and $\mu_2$ are two measures on $\sigma(\scrF_0)$ such that $\mu_1(A)=\mu_2(A)$ for all $A\in\scrF_0$. Furthermore, suppose $\exists$ a sequence of disjoint sets $\Omega_i\in\scrF_0$, for $i=1,2,\ldots$, such that $\biguplus_i \Omega_i=\Omega$ and $\mu_1(\Omega_i)=\mu_2(\Omega_i)<\infty $ for all $i$. Then $\mu_1(B)=\mu_2(B)$ for all $B\in\sigma(\scrF_0)$.            
	\end{theorem}
\end{thmbox}

As an application of the uniqueness result, we prove that a probability measure on $\bbR$ is
uniquely determined by its Stieltjes measure function.
\begin{thmbox}
	\begin{theorem}\label{thm:unique-prob-measure}\rm
		Let $P$ be a probability measure on $(\bbR,\scrB(\bbR))$, and let $F(x)$ be its induced distribution function. If $\exists$ another probability $Q$ on $\scrB(\bbR)$ such that 
		\begin{sequation*}
			P((-\infty ,x])=Q((-\infty ,x]),\quad \forall x\in\bbR,
		\end{sequation*}      
	then $P(B)=Q(B)$, $\forall B\in\scrB(\bbR)$.  
	\end{theorem}
\end{thmbox}
Multivariate version: On $(\bbR^d,\scrB(\bbR^d))$, $P$ is uniquely determined by multivariate distribution function $F(\x)$.   

\subsection{Measurable functions}\label{sec:mea}

Given a function $f:\Omega\to\Omega'$, the \red{inverse image} of a set $A\subseteq\Omega'$ via the function $f$ is defined as 
\begin{sequation*}
	f^{-1}(A):=\{x\in\Omega:f(x)\in A\}.
\end{sequation*}  
\begin{itemize}
	\item $f^{-1}(A^c)=(f^{-1}(A))^c$.
	\item If $(A_i)_{i\in I}$ is any collection of sets in $\Omega'$, then $f^{-1}(\cap_{i\in I}A_i)=\bigcap_{i\in I}f^{-1}(A_i)$ and $f^{-1}(\cup_{i\in I}A_i)=\bigcup_{i\in I}f^{-1}(A_i).$
	\item If $h=g\circ f$, then $h^{-1}(B)=f^{-1}(g^{-1}(B))$ for any subset $B$ of the codomain of $g$.    
\end{itemize}

\begin{defbox}
	\begin{definition}[\textbf{Measurable functions}]\label{def:mea-fun-rv}
		Let $(\Omega,\scrF)$ and $(\Omega',\scrG)$ be two measurable spaces. A function $f:\Omega\to\Omega'$ is called \red{$(\scrF,\scrG)$-measurable} if $\forall B\in\scrG$, $f^{-1}(B)\in\scrF$. When $\scrG$ is understood from the context, we say that $f$ is \red{$\scrF$-measurable}. If both $\scrF$ and $\scrG$ are understood, we say that $f$ is \red{measurable}.     
		
		When $(\Omega,\scrF)=(\bbR,\scrB(\bbR))$, we will refer to an $\scrF$-measurable function as a \red{Borel measurable} function.

		When $(\Omega',\scrG)=(\bbR,\scrB(\bbR))$, a $(\scrF,\scrB(\bbR))$-measurable function $X:\Omega\to\bbR$ is called a \red{(real-valued) measurable function}. If $(\Omega,\scrF,\mu)$ is a probability space, we refer to $X$ as a \red{random variable}. Furthermore, if $(\Omega',\scrG)=(\bbR^d,\scrB(\bbR^d))$, then $\X$ is called a \red{random vector}.
	\end{definition}
\end{defbox}
Examples:
\begin{itemize}
	\item If $\scrF=2^{\Omega}$ or $\scrG=\{\emptyset,\Omega'\}$, then $\forall f:\Omega\to\Omega'$ is $(\scrF,\scrG)$-measurable.
	\item If $\scrF=\{\emptyset,\Omega\}$, and $\scrG$ is the $\sigma$-field in which all singletons are $\scrG$-measurable, then an $(\scrF,\scrG)$-measurable function is a constant function.        
	\item Let $A\subseteq\Omega$. The indicator function $\1_A(\omega)$ is $(\scrF,\scrB(\bbR))$-measurable iff $A$ is measurable. 
	\begin{itemize}
		\item Let $(\Omega,\scrF)=(\bbR,\scrB(\bbR))$, since $\bbQ$ and Cantor set $C\in\scrB(\bbR)$, $\1_{\bbQ}$ and $\1_{C}$ is Borel measurable. But Vitali set $V\notin\scrB(\bbR)$, so $\1_V$ is not Borel measurable.       
	\end{itemize}
\end{itemize}

\noindent \red{$\sigma$-field generated by $f$}: $\sigma(f):=\{f^{-1}(B):B\in\scrG\}$ is the smallest $\sigma$-field in $\Omega$ to make $f$ measurable.   
\begin{itemize}
	\item If $f(\omega)=a\in\Omega'$, $\forall \omega\in\Omega$, then $\sigma(f)=\{\Omega,\emptyset\}$.
	\item If $f(\omega)=a\1_{A}+b\1_{A^c}$, $a\neq b$, then $\sigma(f)=\{\Omega,\emptyset,A,A^c\}=\sigma(A)$.       
\end{itemize}

\noindent \red{Induced $\sigma$-field}: $\{B\subset\Omega':f^{-1}(B)\in\scrF\}$ is the largest $\sigma$-field in $\Omega'$ to make $f$ measurable.    


\begin{thmbox}
	\begin{theorem}[\textbf{Composition of measurable functions}]\label{thm:comp-mea}\rm
		Suppose $f:(\Omega,\scrF)\to(\Omega',\scrG)$ is $(\scrF,\scrG)$-measurable, and $g:(\Omega',\scrG)\to(\Omega'',\scrH)$ is $(\scrG,\scrH)$-measurable. Then the composed function $h=g\circ f$ is $(\scrF,\scrH)$-measurable.   
	\end{theorem}
	\begin{theorem}[\textbf{Check measurability}]\label{thm:check-mea}\rm
		Let $f:(\Omega,\scrF)\to(\Omega',\scrG)$ be a function, $\sigma(\scrC)=\scrG$. Then $f$ is $(\scrF,\scrG)$-measurable iff $f^{-1}(A)\in\scrF$, $\forall A\in\scrC$. 
	\end{theorem}
	\begin{theorem}[\textbf{Measurability on $\bbR^m$}]\label{thm:cts-bbR}\rm
		A continuous function $f:\bbR^m\to\bbR^n$ is $(\scrB(\bbR^m),\scrB(\bbR^n))$-measurable. In particular, a continuous real-valued function $f:\bbR^m\to\bbR$ is measurable. 
	\end{theorem}
	\begin{theorem}[\textbf{Random vector}]\label{thm:rvec}\rm
		$\X=(X_1 , \ldots , X_d)^T$ is a random vector iff $X_i$ is a rv, $\forall i=1,\ldots,d$.   
	\end{theorem}
\end{thmbox}
\noindent \red{$\sigma(X_1,\ldots,X_d)$}: the smallest $\sigma$-field to make any $X_i$ measurable, $\forall i=1,\ldots,d$, which is $\sigma(\sigma(X_1),\ldots,\sigma(X_d))$; it is also the smallest $\sigma$-field to make $\X=(X_1 , \ldots , X_d)^T$ measurable.

\noindent \red{$\sigma(X_1,X_2,\ldots)$}$=\sigma(\sigma(X_1),\sigma(X_2),\ldots)$ or $\sigma(\X:\Omega\to\bbR^\infty )=\sigma((-\infty ,x_1]\times\cdots(-\infty ,x_d],\ \x\in\bbR^d,\ d=1,2,\ldots,)$, that is, any finite collection of $X_i$ is measurable. 

\subsubsection{Operations of measurable functions}\label{sec:oper-mea}
If $X_1,\ldots,X_n$ are rvs, $f:(\bbR^n,\scrB(\bbR^n))\to(\bbR,\scrB(\bbR))$ is measurable, then $f(X_1,\ldots,X_n)$ is a rv by composition. 

\begin{thmbox}
	\begin{theorem}\label{thm:oper-mea}\rm
		If $f,g:\Omega\to\bbR$ are $\scrF$-measurable, then $f+g$, $f-g$, $c\cdot f$, and $f/g$ are measurable, where $c$ is a constant, and in $f/g$, we assume that $g(\omega)\neq 0$, $\forall\omega$.           		
	\end{theorem}
\end{thmbox}

Define the \red{extended real line} $\bbR^*=[-\infty ,\infty ]$, $\scrB(\bbR^*)=\sigma\{A,A\cup\{-\infty \},A\cup\{\infty \}, A\cup\{\infty ,-\infty \}:A\in\scrB(\bbR)\}=\sigma((-\infty ,x]:x\in\bbR\cup\{\infty \})$. By convention, $0\cdot \infty =0$ and $0\cdot(-\infty )=0$. If $f:\Omega\to\bbR^*$ is measurable, then it is called \red{generalized random variable}.   

\begin{thmbox}
	\begin{theorem}\label{thm:max-mea}\rm
		Suppose $f_i:\Omega\to\bbR^*$ is measurable for $i\in\bbN^+$, then the functions
		\begin{sequation*}
			\inf_i f_i, \ \sup_i f_i,\ \liminf_i f_i,\ \limsup_i f_i, \ \lim_i f_i \ (\text{if exists})
		\end{sequation*}
		 are measurable, i.e., generalized rvs.       
	\end{theorem}
\end{thmbox}
Let $\Omega_0=\{\omega\in\Omega:\lim_n f_n(\omega) \text{ exists and finite}\}=\{\omega\in\Omega:\limsup_n f_n(\omega)-\liminf_n f_n(\omega)=0\}\in\scrF$. If $P(\Omega_0)=1$, then we say $\{f_n\}_{n=1}^\infty $ \red{converges almost surely}.   

\subsubsection{Random variables and distributions}\label{sec:rv-dist}
Let $X:(\Omega,\scrF,\bbP)\to(\bbR,\scrB(\bbR))$ be a rv. Then its induced measure on $\bbR$ is called the \red{probability distribution} of $X$, i.e., $\bbP(X\in B):=\bbP(X^{-1}(B))$, $\forall B\in\scrB(\bbR)$. The \red{distribution function} (df) of $X$ is defined to be $F_X(x)=\bbP(X\leq x)$, $\forall x\in\bbR$. The properties in Theorem \ref{thm:df-prop} hold.
\begin{itemize}
	\item (Fact) If $X$ has cdf $F$, $F$ is cts, then $Y=F(X)\sim\mathrm{Unif}(0,1)$.    
\end{itemize}
\begin{thmbox}
	\begin{theorem}\label{thm:dist-coupling}\rm
		Let $\Omega=(0,1)$, $\scrF=\scrB((0,1))$, and $\bbP$ is the Lebesgue measure on $(0,1)$. Let $F$ be an arbitrary distribution function. 
		Define $X(\omega)=F^{-1}(\omega)$, $\omega\in(0,1)$, where 
		\begin{sequation*}
			F^{-1}(\omega):=\inf\{y\in\bbR:F(y)\geq \omega\} (=\sup\{y\in\bbR:F(y)<\omega\}).
		\end{sequation*}
		Then $X$ is regarded as a rv in $\Omega$ having df $F$.            
	\end{theorem}
\end{thmbox}
It is useful to define several rvs on the same probability space, called \red{coupling}.

\subsection{Statistical independence}\label{sec:independence}



\subsection{Expectation}\label{sec:measure-expectation}
\begin{lemma}\label{lamma:measure-expectation-expInt}
	Let $X\geq 0$, $p>0$, we have $\bbE X^p=\int_{0}^{\infty }px^{p-1}\Unif(X>x) \rmd x$.
\end{lemma}



\section{Law of Large Numbers}\label{sec:LLN}




\subsection{Almost Surely Convergence}\label{sec:LLN-AS}

This lemma gives an equivalent relation between expectation and sum of tail probability.
\begin{lemma}\label{lemma:LLN-AS-expectation-sumTail}
	Let $X_i$ iid and $\varepsilon>0$, then $\sum_{n=1}^{\infty } \bbP(\abs{X_n}>n\varepsilon) \leq \varepsilon^{-1}\bbE\abs{X_i} \leq  \sum_{n=0}^\infty \bbP(\abs{X_n}>n\varepsilon)$.  
\end{lemma}


\section{Central Limit Theorem}\label{sec:CLT}

\section{Random Walks}\label{sec:RW}
\red{Random walk (RW)}: Let $\X_i$ be iid rvs in $\bbR^d$. Let $\S_n=\sum_{i=1}^{n}\X_i$. Then $\{\S_n:n\geq 1\}$ is called a RW. Take $\S_0=\0$.  

\noindent \red{Simple random walk (SRW)}: If $\bbP(X_i=1)=\bbP(X_i=-1)=1/2$, then $\{S_n\}$ is called a SRW in $\bbR^1$. If $\bbP(\X_i=(1,1))=\bbP(\X_i=(1,-1))=\bbP(\X_i=(-1,1))=\bbP(\X_i=(-1,-1))=1/4$, then called a SRW in $\bbR^2$.

\subsection{Stopping Times (\ref{sec:proof-stoppingTimes})}\label{sec:stoppingTimes}

\noindent \underline{\textbf{Long-term behavior of RW}}

\noindent \red{Permutable (or exchangeable)}: An event that does not change under finite permutation of $\{\X_1,\X_2,\ldots\}$.
\begin{itemize}
	\item All events in the tail $\sigma$-field $\calT$ are permutable.
	\item $\{\omega:\S_{n}(\omega)\in B \ \mathrm{i.o.}\}$ is permutable but not tail event. 
	\item $\{\omega:\lim\sup_{n\to\infty}\S_{n}(\omega)/c_{n}\geq1\}$.
\end{itemize}

\begin{thmbox}
\begin{theorem}[Hewitt-Savage 0-1 law]\label{thm:HS01Law}
	If $\X_i$ iid and event $A$ is permutable, then $\bbP(A)=0$ or $1$.   
\end{theorem}
\end{thmbox}

\begin{thmbox}
	\begin{theorem}[Long-term behavior of RW]\label{thm:longTermRW-1d}
		For a RW in $\bbR$, one of the following has probability $1$:
		\begin{enumerate}[label=(\roman*)]
			\item $S_n=0$ for all $n$ ;
			\item $S_n\to \infty $ as $n\to \infty $;
			\item $S_n\to - \infty $ as $n\to \infty $;
			\item $- \infty = \liminf_{n} S_n < \limsup_n S_n=\infty $.   
		\end{enumerate}  
	\end{theorem}
\end{thmbox}

\noindent \underline{\textbf{For two levels $a<b$, find the probability that RW reaches $b$ before $a$}}

\noindent \red{Filtration}: Let $\X_i$ be a sequence of rvs, $\{\calF_n:=\sigma(\X_1,\ldots,\X_n)\}_{n=1}^\infty $ as an increasing sequence of $\sigma$-fields, is called a filtration. We usually take $\calF_0=\{\phi,\Omega\}$.  

\noindent \red{Stopping time/optional random variable/optimal time/Markov time}: $\tau\in\bbN^+\cup\{\infty \}$ is a stopping time w.r.t. $\{\calF_n\}$ if $\{\tau=n\}\in\calF_n$, $\forall n\in\bbN^+$. (Equivalent def: $\{\tau\leq n\}\in\calF_n$ or $\{\tau\geq n+1\}\in\calF_n$ for $n\in\bbN^+$)
\begin{itemize}
	\item Constant $\tau=n$ is a stopping time. 
	\item If $\tau_1,\tau_2$ are stopping time, then $\tau_1\wedge\tau_2$, $\tau_1\vee\tau_2$, $\tau_1+\tau_2$ are stopping times.    
	\item \red{Hitting time of $A$}: let $A$ measurable, then $\tau=\inf\{n\geq 1:\S_n\in A\}$ is a stopping time.     
	\item $\sigma$-field $\calF_N$=the information known at time $N$. Def: $\calF_N$ is the collection of sets $A$ that have $A\cup\{N=n\}\in\calF_n$, $\forall n< \infty $. Example: $\{N\leq n\}\in\calF_N$, i.e., $N$ is $\calF_N$-measurable.          
\end{itemize}

\begin{thmbox}
	\begin{theorem}[Wald's equation]\label{thm:WaldEq}
		
	Let $X_i$ iid and $\tau$ be a stopping time. 
	\begin{enumerate}
		\item (Wald's first equation)
		If $\bbE \abs{X_1}< \infty $ and $\bbE \tau < \infty $, then $\bbE S_{\tau}=\bbE X_1 \bbE \tau$.     
		\item (Wald's second equation) If $\bbE X_1=0$, $\bbE X_1^2=\sigma^2<\infty$, $\bbE \tau < \infty $, then $\bbE S_{\tau}^2=\sigma^2 \bbE \tau$.    
	\end{enumerate}
	\end{theorem}
\end{thmbox}

\begin{exbox}
\begin{example}[Results for 1-d SRW]\label{eg:1dSRW}
	For 1-d SRW, let $a,b\in\bbZ$, $a<0<b$. Let $N=\inf \{n: S_n\notin (a,b)\}=\inf\{n:S_n=a \text{ or } b\}$. Then 
	\begin{enumerate}
		\item $\bbE N <\infty $,
		\item $S_N=a$ or $b$,
		\item $\bbP (S_N=a)=b/(b-a)$, $\bbP(S_N=b)=-a/(b-a)$,
		\item $\bbE N=\bbE S_N^2=(-a)b$.      
	\end{enumerate} 
\end{example}
\end{exbox}

\subsection{Recurrence vs. Transience (\ref{sec:proof-RecTrans})}\label{sec:RecTrans}
\noindent \underline{\textbf{When RW return to 0?}}
We consider SRW on $\bbR^d$ and define its first, second, ..., $n$th returning time to the origin to be 
\begin{align*}
	\tau_{1}&=\inf\{m\geq1:\S_{m}=\0\},\\\tau_{n}&=\inf\{m>\tau_{n-1}:\S_{m}=\0\}.
\end{align*}

\begin{thmbox}
	\begin{theorem}\label{thm:returnTimeRW}
		For any RW, the following are equivalent:
		\begin{enumerate}[label=(\roman*)]
			\item $\bbP(\tau_{1}<\infty)=1$ 
			\item $\bbP(\tau_{n}<\infty)=1, \forall n=1,2,3,\ldots$
			\item $\bbP(\S_{m}=\0 \ i.o.)=1$
			\item $\sum_{m=1}^{\infty}\bbP(\S_{m}=\0)=\infty.$ 
		\end{enumerate}
	\end{theorem}
\end{thmbox}
\begin{itemize}
	\item $\bbP(\tau_n<\infty)=(\bbP(\tau_1<\infty))^n$. 
\end{itemize}

\noindent \red{Recurrent}: If $\bbP (\tau_1<\infty )=1$, then the RW is called recurrent.

\noindent \red{Transient}: If $\bbP (\tau_1<\infty )<1$, then the RW is called Transient.


\begin{thmbox}
	\begin{theorem}[Recurrence of SRW]\label{thm:RecSRW}
		SRW is recurrent in $d\leq 2$ and transient in $d\geq 3$.   
	\end{theorem}
\end{thmbox}
\begin{itemize}
	\item We define the first time a random walk starting from $\a$ reaches $\b$: $\tau_{\a\to \b}:=\inf\{m\geq1:\a+\S_m=\b\}.$ It can be proved that $\bbP(\tau_1<\infty )=1$ iff $\bbP(\tau_{\a\to \b}<\infty )=1$, $\forall \a,\b$.   
\end{itemize}


\subsection{Reflection Principle and Arcsine Distribution (\ref{sec:proof-ReflectionArcsine})}\label{sec:ReflectionArcsine}
\noindent \underline{\textbf{What is the distribution of the time spent above 0?}} We consider the SRW, $d=1$, and think of the sequence $S_1,\ldots,S_n$ as being represented by a polygonal line with segments $(k-1,S_{k-1})\rightarrow (k,S_k)$.  
\begin{thmbox}
	\begin{theorem}[Reflection Principle]\label{thm:ReflectionPrin}
		\
		\begin{itemize}
			\item (Reflection principle for numbers) If $x,y>0$, then the number of paths from $(0,x)$ to $(n,y)$ that are 0 at some time is equal to the number of paths from $(0,-x)$ to $(n,y)$.    
			\item (Reflection principle for SRW) 
			Let $X_i$ be SRW with $d=1$. Then $\forall b\in\bbN^+$, 
			\begin{equation*}
				\bbP(\max_{1\leq k\leq n} S_k\geq b)=2\bbP(S_n>b) + \bbP(S_n=b).
			\end{equation*}  
		\end{itemize}
	\end{theorem}
		
	\begin{theorem}[Hit 0 time]\label{thm:ReflectionHit0} $\bbP(S_1> 0,\ldots,S_{2n}> 0)=\frac12\bbP(S_{2n}=0)$, and
		$\bbP(S_1\neq 0,\ldots,S_{2n}\neq 0)=\bbP(S_{2n}=0)$. 
	\end{theorem}
\end{thmbox}

\noindent \red{Arcsine distribution}: a continuous distribution with density $\frac1{\pi\sqrt{x(1-x)}}$, $x\in(0,1)$. Define 
\begin{align*}
	L_{2n}&:=\sup\{m\leq2n:S_m=0\}, &\text{(last time at 0)}\\
	F_n&:=\inf\{0\leq m\leq n:S_m=\max\limits_{0\leq k\leq n}S_k\},&\text{(first time at maximum)}\\
	\pi_{2n}&:=\text{ number of } k:1\leq k\leq2n \text{ such that the line } (k-1,S_{k-1})\to(k,S_k) \text{ is above the $x$-axis}.
\end{align*}

\begin{thmbox}
	\begin{theorem}[Arcsine law]\label{thm:ArcsineLaw}
		$\frac{L_{2n}}{2n},\frac{F_n}n,\frac{\pi_{2n}}{2n}$ all converge in distribution to the arcsine distribution.
	\end{theorem}
\end{thmbox}

\section{Martingales}\label{sec:martingales}
\subsection{Conditional expectation \ref{sec:proof-cond_exp}}\label{sec:cond_exp}
\begin{defbox}
\begin{definition}[Conditional expectation]\label{def:cond_exp}
	$X$ is a random variable on $(\Omega, \calF, \bbP)$ with $\bbE\abs{X}<\infty $. $\calA$ is a $\sigma$-field and $\calA\subset \calF$. We define the \red{conditional expectation} of $X$ given $\calA$, $\bbE(X|\calA)$, to be any random variable $Y$ satisfying
	\begin{enumerate}
		\item[(i)] $Y$ is $\calA$-measurable, and
		\item[(ii)] $\forall A\in\calA$, $\bbE[X\1_A]=\bbE[Y\1_A]$.    
	\end{enumerate}
	\begin{itemize}
		\item For rvs $X$ and $Y$, $\bbE(X|Y):=\bbE(X|\sigma(Y))$. 
		\item For set $A$, $\bbP(A|\calA):=\bbE(\1_A|\calA)$.
		\item For set $A,B$, $\bbP(A|B):=\bbP(A\cap B)/\bbP(B)$ given $\bbP(B)>0$.     
	\end{itemize}
\end{definition}
\end{defbox}
\begin{itemize}
	\item If $Y$ satisfies (i) and (ii), then $\bbE \abs{Y} \leq \bbE\abs{X}<\infty $. 
	\item $\bbE Y = \bbE X$, i.e., $\bbE[\bbE(X|\calA)]=\bbE X$.
	\item \textbf{Uniqueness}: If $Y'$ also satisfies (i) and (ii), then $Y=Y'$ a.s. Any such $Y$ is said to be a \red{version} of $\bbE(X|\calA)$.
	\item \textbf{Existence}: By Radon-Nikodym theorem.
\end{itemize}

\begin{exbox}
	\begin{example}\label{eg:cond_exp}\rm
		\
		\begin{itemize}
			\item If $X$ is $\calA$-measurable, then $\bbE(X|\calA)=X$. So a constant $c=\bbE(c|\calA)$. (Know $X$, the ``best guess'' is $X$)
			\item  If $\sigma(X)\indep \calA$, then $\bbE(X|\calA)=\bbE X$. (Don't know anything about $X$, the best guess is its mean) 
			\item Suppose $\Omega_1,\Omega_2,\ldots$ is a finite or infinite partition of $\Omega$ into disjoint sets with positive probability, and let $\calA=\sigma(\Omega_1,\ldots)$, then $\bbE(X|\calA)=\bbE(X\1_{\Omega_i})/\bbP(\Omega_i)$ on $\Omega_i$. 
			\begin{itemize}
				\item Let $\calA=\{\emptyset, \Omega\}$, then $\bbE(X|\calA)=\bbE X$.  
			\end{itemize}   
			\item (Bayes's formula)	Let $G\in\calG$, then $\bbP (G|A)=\int_G \bbP(A|\calG) \rmd \bbP \left/ \int_\Omega \bbP(A|\calG) \rmd \bbP \right.$. When $\calG$ is a $\sigma$-field generated by a partition, this reduces to the usual Bayes' formula $\bbP(G_i|A)=\bbP(A|G_i)P(G_i)\left/ \sum_j \bbP(A|G_j)\bbP(G_j)\right.$.
		\end{itemize}
	\end{example}
\end{exbox}

\begin{thmbox}
	\begin{theorem}[Properties]\label{thm:cond_exp_prop}\rm
		If $\bbE \abs{X}$, $\bbE\abs{X_n}$, $\bbE \abs{Y}<\infty $, then 
		\begin{itemize}
			\item[(a)] (linearity) $\bbE(aX+Y|\calA)=a\bbE(X|\calA)+\bbE(Y|\calA).$ 
			\item[(b)] (monotonicity) If $X\leq Y$, then $\bbE(X|\calA)\leq \bbE(Y|\calA)$.
			\item[(c)] (MCT) If $X_n\geq 0$, $X_{n}\uparrow X$, and $\bbE X<\infty $, then $E(X_n|\mathcal{A})\uparrow E(X|\mathcal{A})$.
			\item[(d)] (Fatou) If $X_n\geq 0$, $\bbE X_n<\infty $, and $\bbE[ \liminf_n X_n]<\infty $, then $\liminf_n\bbE(X_n|\calA)\leq \bbE[\liminf_n X_n|\calA]$.   
			\item[(e)] (DCT) If $X_n\to X$ a.s., $\abs{X_n}\leq Y$, $\bbE|Y|<\infty $, then $\bbE(X_n|\calA)\to\bbE(X|\calA)$. 
			\item[(f)] If $X$ is $\calA$-measurable, $\bbE|XY|<\infty $, then $\bbE(XY|\calA)=X\bbE(Y|\calA)$.     
			\item[(g)] (Tower property) If $\calA_1\subset\calA_2$, then $\bbE[\bbE(X|\calA_1)|\calA_2]=\bbE(X|\calA_1)$, and $\bbE[\bbE(X|\calA_2)|\calA_1]=\bbE(X|\calA_1)$.
		\end{itemize}  
	\end{theorem}
\end{thmbox}

\begin{thmbox}
	\begin{theorem}[Inequalities]\label{thm:cond_exp_ineq}\rm
		If $\bbE \abs{X}$, $\bbE \abs{Y}<\infty $, then 
		\begin{itemize}
			\item[(i)] (Jensen's inequality) If $\varphi$ is convex, $\bbE|\varphi(X)|<\infty $, then $\varphi(\bbE[X|\calA])\leq \bbE[\varphi(X)|\calA]$.  
			\item[(ii)] (Markov's inequality) If $X\geq 0$, $a>0$, then $\bbP(X\geq a|\calA)\leq a^{-1}\bbE(X|\calA)$.
			\item[(iii)] (Chebyshev's inequality) If $a>0$, then $\bbP(\abs{X}\geq a|\calA)\leq a^{-2}\bbE(X^2|\calA)$.
			\item[(iv)] (Hölder's Inequality) If $p\geq 1$, $p^{-1}+q^{-1}=1$, and $\bbE|X|^p$, $\bbE|Y|^p<\infty $, then 
			{\setlength\abovedisplayskip{0.15cm}
			\setlength\belowdisplayskip{0.15cm}
			\begin{equation*}
				\abs{\bbE(XY|\calA)}\leq \{\bbE(\abs{X}^p|\calA)\}^{1/p} \{\bbE(\abs{Y}^q|\calA)\}^{1/q}.
			\end{equation*}}
			\item[(v)] (Minkowski inequality) If $p\geq 1$, $\bbE|X|^p$, $\bbE|Y|^p<\infty $, then 
			{\setlength\abovedisplayskip{0.15cm}
			\setlength\belowdisplayskip{0.15cm}
			\begin{equation*}
				\cbk{\bbE(\abs{X+Y}^p|\calA)}^{1/p} \leq	\cbk{\bbE(\abs{X}^p|\calA)}^{1/p} + \cbk{\bbE(\abs{Y}^p|\calA)}^{1/p}.
			\end{equation*}}
			\item[(vi)] (Triangular inequality) If $\bbE X^2 <\infty $. Then for any $\calA$-measurable $Y$ with $\bbE Y^2<\infty $, we have  
			{\setlength\abovedisplayskip{0.15cm}
			\setlength\belowdisplayskip{0.15cm}
			\begin{equation*}
				\norm{X-\bbE(X|\calA)}^2 \leq \norm{X-Y}^2.
			\end{equation*}}    
		\end{itemize}  
	\end{theorem}
\end{thmbox}
By (vi), $\bbE(X|\calA)$ is the projection of $X$ onto $\calL^2(\calA)$, that is, $\bbE(X|\calA)=\arg \min_Y \norm{X-Y}^2$, for $\calA$-measurable $Y$. 

\subsection{Martingales}\label{sec:martingale}
\begin{defbox}
	\begin{definition}\label{def:martingale}
		Let $\{\calF_n\}$ be a \red{filtration}, an increasing sequence of $\sigma$-fields. A sequence $\{S_n\}$ is said to be \red{adapted to} $\{\calF_n\}$ if $S_n$ is $\calF_n$-measurable. $\{S_n\}$ is called a \red{martingale} w.r.t. $\{\calF_n\}$ if 
		\begin{enumerate}
			\item[(i)] $\bbE|S_n|<\infty $.
			\item[(ii)] $\{\S_n\}$ is adapted to $\{\calF_n\}$.     
			\item[(iii)] $\bbE(S_n|\calF_{n-1})=S_{n-1}$.  
		\end{enumerate}     
		If in (iii), $\bbE(S_n|\calF_{n-1})\leq S_{n-1}$ (or $\geq $), then $\{S_n\}$ is said to be a \red{supermartingale} (or \red{submartingale}).  
	\end{definition}
\end{defbox}
\noindent Simple facts:
\begin{itemize}
	\item If $\{S_n\}$ is a martingale, then $\bbE S_1=\cdots=\bbE S_n =\cdots$, and $\bbE|S_1|\leq\bbE|S_2|\leq\cdots$. 
	\item If $\{S_n\}$ is a supermartingale, then $\bbE S_1\geq\bbE S_{2}\geq \cdots$; if submartingale, $\bbE S_1\leq\bbE S_2\leq\cdots$.
	\item If $\{S_n\}$ is a supermartingale, then $\{-S_n\}$ is a submartingale, and vice versa. 
	\item Let $n>m$, if $\{S_n\}$ is a 
	\begin{itemize}
		\item martingale $\quad \quad \ \implies \bbE(S_n|\calF_m)=S_m$
		\item supermartingale $\implies \bbE(S_n|\calF_m)\leq S_m$
		\item submartingale $\quad   \implies \bbE(S_n|\calF_m)\geq S_m$
	\end{itemize}  
\end{itemize}

\begin{thmbox}
	\begin{theorem}[\textbf{Martingale transforms}]\label{thm:martingale_trans}\rm
		\
		\begin{enumerate}
			\item[(1)] If $\{S_n\}$ is a martingale and $\varphi$ is a convex (concave) function such that $\bbE\abs{\varphi(S_n)}<\infty $, then $\varphi(S_n)$ is a submartingale (supermartingale).   
			\item[(2)] If $S_n$ is a submartingale and $\varphi$ is an increasing convex (concave) function such that $\bbE\abs{\varphi(S_n)}<\infty $, then $\varphi(S_n)$ is a submartingale (supermartingale). 
		\end{enumerate}
	\end{theorem}
\end{thmbox}
\begin{itemize}
	\item From (1), if $p\geq 1$ and $\bbE|S_n|^p<\infty $, then $\abs{S_n}^p$ is a submartingale.   
	\item From (2), if $S_n$ is a submartingale, then $(X_n-a)^+$ is a submartingale; if $X_n$ is a supermartingale, then $X_n\wedge a$ is a supermartingale.    
\end{itemize}

\subsection{Martingale convergence}\label{sec:martingale_cvg}

\red{Predictable sequence}: $H_n$, $n\geq 2$, which is $\calF_{n-1}$-measurable. $(H\cdot S)_n=\sum_{m=1}^{n}H_m(S_{m}-S_{m-1})$.    
\begin{itemize}
	\item If $\{S_n\}$ is a supermartingale (submartingale), $H_n$ ($n\geq 2$) is predictable, $H_n\geq 0$, and $H_n$ is bounded. Then $(H\cdot S)_n$ is a supermartingale (submartingale). For martingale, it is true without assuming $H_n\geq 0$.    
	\item Let $N$ be a stopping time, and $H_n=\1_{\{n\leq N\}}$, then $S_{n\wedge N}=(H\cdot S)_n+S_0$ is a supermartingale/submartingale/martingale as $S_n$ is.
\end{itemize}

Let $a<b$, $N_0=1$, for $k\geq 1$, let $N_{2k-1}=\inf\{m>N_{2k-2}: S_m\leq a\}$ and $N_{2k}=\inf\{m>N_{2k-1}: S_m\geq b\}$, all are stopping times. Let $H_m=\1\{N_{2k-1}<m<N_{2k}\text{ for some } k\}$ be the indicator of climbing. Let $U_n=\sup\{k:N_{2k}\leq n\}$ be the number of \red{upcrossings} by time $n$. From the picture we have 
\begin{enumerate}
	\item[(1)] $H_m$ is predictable;
	\item[(2)] $(b-a) U_n \leq \sum_{m=2}^{n}H_m(S_m-S_{m-1})$ $\Rightarrow (b-a)\bbE U_n\leq \bbE(S_n-S_1)$.   
\end{enumerate} 
\begin{thmbox}
	\begin{theorem}[Upcrossing inequality]\label{thm:upcrossings_ineq}\rm
		$\{S_n\}$ is a submartingale. $a<b$ are two constants. For $U_n$ defined above,
		{\setlength\abovedisplayskip{0.15cm}
			\setlength\belowdisplayskip{0.15cm}
			\begin{equation*}
				\bbE U_n\leq \frac{1}{b-a}\mbk{
					\bbE(S_n-a)^+ - \bbE(S_1-a)^+
				}
			\end{equation*}}     
	\end{theorem}
\end{thmbox}

\begin{thmbox}
	\begin{theorem}[\textbf{Martingale convergence theorem}]\label{thm:martingale_cvg}\rm
		Suppose $S_n$ is a submartingale and $\liminf \bbE S_n^+<\infty $ (or $\sup \bbE S_n^+<\infty $), then $S_n\to S$ a.s. with $\bbE\abs{S}<\infty $.   
	\end{theorem}
	\begin{theorem}\label{thm:martingale_cvg_super}\rm
		Suppose $\{S_n\}$ is a supermartingale. If $S_n\geq 0$, then $S_n\to S$ a.s. and $\bbE S\leq\bbE S_1 <\infty $.    
	\end{theorem}
	\begin{corollary}[WLLN for martingales]\label{cor:WLLN_mart}\rm
		Suppose $X_i$ are identically distributed and $\bbE\abs{X_1}<\infty $. Let $\calF_n=\sigma(X_1,\ldots,X_n)$. Let $S_1=X_1$, and $S_n=S_{n-1}+X_n-\bbE(X_n|\calF_{n-1})$, $n\geq 2$. Then $\{S_n\}$ is a martingale and ${S_n}/{n} \pto 0.$       
	\end{corollary}
\end{thmbox}

\begin{exbox}
	\begin{example}[Branching processes]\label{eg:branching_process}\rm
		Let $\xi_i^{n}\in\bbN$, $i,n\geq 1$ be iid ($n$: time, i: the $i$th parent). Define $Z_n$ by $Z_0=1$, and 
		{\setlength\abovedisplayskip{0.15cm}
		\setlength\belowdisplayskip{0.15cm}
		\begin{equation*}
			Z_{n+1}=\begin{cases}
				\xi_1^{n+1}+\cdots + \xi_{Z_n}^{n+1} & \text{ if } Z_n > 0 \\
				0	& \text{ if } Z_n=0.
			\end{cases}
		\end{equation*}}  
		$Z_n$ is called a \red{Galton-Watson process}. Let $\calF_n=\sigma(\xi_i^m:i\geq 1, 1\leq m\leq n)$ and $\mu=\bbE \xi_i^m\in(0,\infty )$, then $\{Z_n/\mu^n\}$ is a martingale w.r.t. $\calF_n$.    
		\begin{itemize}
			\item If $\mu<1$, then $Z_n=0$ for all $n$ sufficiently large, so $Z_n/\mu^n\to 0$.
			\item If $\mu=1$ and $\bbP(\xi_i^m=1)<1$, then $Z_n=0$ for all $n$ sufficiently large.        
		\end{itemize}
	\end{example}
\end{exbox}


\subsection{Doob's inequality; $\calL^p$ convergence; CLT}\label{sec:doob_Lp_CLT}
\begin{thmbox}
	\begin{theorem}[\textbf{Doob's inequality}]\label{thm:Doob_ineq}\rm
		$\{S_n\}$ is a submartingale w.r.t. $\{\calF_n\}$. Then $\forall x>0$,
		{\setlength\abovedisplayskip{0.15cm}
		\setlength\belowdisplayskip{0.15cm}
		\begin{equation*}
			\bbP\sbk{\max_{1\leq k\leq n} S_k\geq x}\leq \frac{1}{x}\bbE\mbk{S_n\1\{\max_{1\leq k\leq n}S_k\geq x\}}\leq \frac{\bbE S_n^+}{x}.
		\end{equation*}}   
	\end{theorem}
\end{thmbox}


\section{Techniques}\label{sec:techniques}
\subsection{Convergence}\label{sec:tech-cvg}
\subsubsection{Convergence of random series}\label{sec:tech-cvg-rseries}
Let $X_i$ be a sequence of rvs. $S_n=\sum_{i=1}^{n}X_i$. By 0-1 law, $\bbP(\lim_{n}S_n \text{ exists})=0$ or $1$.   

\underline{To show the convergence of random series}:
\begin{enumerate}
	\item 
\end{enumerate}

\underline{To show the divergence of random series}:
\begin{enumerate}
	\item If SLLN holds, $S_n/n \to \mu$ a.s., if $\mu>0$, then $S_n\to \infty$ a.s.
	\item Use $S_n'$, $S_n'=S_n$ e.v. a.s., that is, $\bbP(S_n\neq S_n' \ i.o.)=0$. And show $S_n'\to \infty $ a.s.   
\end{enumerate}

Next, we consider $S_n/f(n)$. 


\appendix
\section{Proofs}\label{sec:proof}
\subsection{Proofs - \ref{sec:RW}}\label{sec:proof-RW}
\subsubsection{Proofs - \ref{sec:stoppingTimes}}\label{sec:proof-stoppingTimes}
\begin{proof}[\underline{\textbf{Proof of Theorem \ref{thm:longTermRW-1d}}}]
	By the 0-1 law \ref{thm:HS01Law}, $\{\limsup_n S_n\geq c\}$ has probability 0 or 1, meaning that $\limsup_n S_n=c\in [-\infty ,\infty ]$ w.p.1. Since $S_n \eqd S_{n+1}-X_{1}$, we have $c=c-X_1$.
\begin{enumerate}[label=(\roman*)]
	\item If $c\in\bbR$, then $X_1\equiv 0$ a.s., so $S_n=0$ for all $n$ a.s.
\end{enumerate}
If $X_1\neq 0$ a.s., then $c=-\infty $ or $\infty $,   
\begin{enumerate}
	\item[(ii)] If $c=\infty $, and $\liminf_n S_n=\infty $, then case (ii);
	\item[(iii)] If $c=-\infty $, and $\liminf_n S_n=-\infty $, then case (iii);
	\item[(iv)] If $c=\infty $, and $\liminf_n S_n=-\infty $, then case (iv).       
\end{enumerate}
\end{proof}

\begin{proof}[\underline{\textbf{Proof of Theorem \ref{thm:WaldEq}}}]
	Prove 1: First suppose $X_i\geq 0$. We have 
	\begin{equation*}
		\bbE S_{\tau} = \bbE \sum_{i=1}^{\tau} X_i = \bbE \sum_{i=1}^{\infty } X_i \1_{\{\tau\geq i\}} = \sum_{i=1}^{\infty }\bbE X_i\1_{\{\tau\geq i\}} = \sum_{i=1}^{\infty }\bbE X_i \bbE \1_{\{\tau\geq i\}} = \bbE X_1 \bbE \tau,
	\end{equation*} 
	where the 3rd equality uses Fubini by $X_i\geq 0$, and the 4th uses $\{\tau\geq i\}\in \calF_{i-1}$. For general case, since $ \sum_{i=1}^{\infty }\bbE \abs{X_i}\1_{\{\tau\geq i\}}=\sum_{i=1}^{\infty }\bbE\abs{X_i}\bbE\1_{\{\tau\geq i\}}< \infty $, we can still use the Fubini. 

	Prove 2: If $\tau<n$, then $\tau\wedge n=\tau\wedge(n-1)$, so $S_{\tau\wedge n}^2=S_{\tau\wedge (n-1)}^2$; if $\tau\geq n$, we have $\tau\wedge n=n$ and $\tau\wedge(n-1)=n-1$, so $S_{\tau\wedge n}^2=S_n^2=(S_{n-1}+X_{n})^2=(S_{\tau\wedge(n-1)}+X_{n})^2$. Hence write
	\begin{equation*}
		S_{\tau\wedge n}^2=S_{\tau\wedge(n-1)}^2+(2X_nS_{n-1}+X_n^2)\1_{\{\tau\geq n\}}.
	\end{equation*}  
	Note that all the following expectations exist,
	\begin{equation*}
		\begin{aligned}
			\bbE S_{\tau\wedge n}^{2}& =\bbE S_{\tau\wedge(n-1)}^{2}+\bbE \left(2X_{n}S_{n-1}1_{\{\tau\geq n\}}\right)+\bbE [X_{n}^{2}1_{\{\tau\geq n\}}] \\
			&=\bbE S_{\tau\wedge(n-1)}^{2}+\sigma^{2}\bbP (\tau\geq n) & \quad (\text{stopping time, independence, and } \bbE X_i=0) \\
			&=\ldots & (\mathrm{reduce~to~}n-2,n-3,\ldots) \\
			&=\sigma^2\sum_{i=1}^n\bbP(\tau\geq i).
			\end{aligned}
	\end{equation*}
	In the 1st line, the expectation $\bbE X_n S_{n-1}$ exists since both rvs are in $\calL^2$. By the last line, $\norm{S_{\tau\wedge n}-S_{\tau\wedge m}}^2=\sigma^2\sum_{i=m+1}^{n}\bbP (\tau\geq i)\to 0$ as $n,m\to \infty $, $\{S_{\tau\wedge n}\}_{n}$ is a Cauchy sequence in $\calL^2$, so letting $n\to \infty $  gives the result. 
\end{proof}

\begin{proof}[\underline{\textbf{Proof of Example \ref{eg:1dSRW}}}]
	1. For any positive integer $k$, by dividing the interval $(0,k(b-a))$ into $k$ subintervals of
	equal length and considering an extreme case behavior (keep going upwards) of the random
	walk within each subinterval, we obtain
	\begin{align*}
		\bbE N = &\sum_{i=0}^{\infty }\bbP(N>i) \leq (b-a)\sum_{k=0}^{\infty }\bbP(N>k(b-a)) \\
		&\leq (b-a)\sum_{k=0}^{\infty } \bbP((X_{(j-1)(b-a)+1},\ldots,X_{j(b-a)})\neq (1,\ldots,1), j=1,\ldots,k) \\
		&\leq (b-a)\sum_{k=0}^{\infty } \sbk{1-\frac{1}{2^{b-a}}}^k<\infty. 
	\end{align*}

	\noindent 2. It is obvious.

	\noindent 3. By Wald's first equation \ref{thm:WaldEq}, $0=\bbE S_N = a\bbP(S_N=a)+b\bbP(S_N=b)$, we also have $1=\bbP(S_N=a)+\bbP(S_N=b)$, so solve for the result.
	
	\noindent 4. By Wald's second equation \ref{thm:WaldEq} and $\sigma=1$, we have $\bbE N=\bbE S_N^2 = a^2 \bbP(S_N=a) + b^2 \bbP (S_N=b)$, and use 3.  
\end{proof}

\subsubsection{Proofs - \ref{sec:RecTrans}}\label{sec:proof-RecTrans}
\begin{proof}[\underline{\textbf{Proof of Theorem \ref{thm:returnTimeRW}}}] We have
	\begin{align*} 
		\bbP(\tau_{2}<\infty)& =\bbP(\tau_{1}<\infty,\tau_{2}-\tau_{1}<\infty) \\
		&=\sum_{m,n=1}^\infty \bbP(\tau_1=m,\tau_2-\tau_1=n) \\
		&= \sum_{m,n=1}^\infty \bbP(\X_1+\cdots+\X_m=\0,\X_1+\cdots+\X_u\neq\0,\forall 1\leq u<m; \\
		&\qquad \qquad \quad \X_{m+1}+\cdots+\X_{m+n}=\0,\X_{m+1}+\cdots+\X_{m+v}\neq\0,\forall 1\leq v<n) \\
		&=\sum_{m,n=1}^\infty \bbP(\tau_1=m)\bbP(\tau_1=n) & (\text{iid}) \\
		&=(\bbP(\tau_1<\infty ))^2.
	\end{align*} 
	Similarly, we can prove $\bbP(\tau_n<\infty)=(\bbP(\tau_1<\infty))^n.$ So (i) and (ii) are equivalent. They are equivalent to (iii) by examining their meanings. Finally, 
	\begin{align*}
		\sum_{m=0}^{\infty}\bbP(\S_{m}=\0)& =\sum_{m=0}^{\infty}\bbE\1_{\{\S_{m}=\0\}}=\bbE\sum_{m=0}^{\infty}\1_{\{\S_{m}=\0\}} 
		=\bbE\sum_{n=0}^\infty1_{\{\tau_n<\infty\}} \\
		&=\sum_{n=0}^\infty \bbP(\tau_n<\infty)=\sum_{n=0}^\infty\bbP(\tau_1<\infty)^n=\frac{1}{1-\bbP(\tau_1<\infty )}.
	\end{align*} 
	So (i) and (iv) are equivalent.
\end{proof}

\begin{proof}[\underline{\textbf{Proof of Theorem \ref{thm:RecSRW}}}]
	In $d=1$, use (iv) in Theorem \ref{thm:returnTimeRW} to show. 
	\begin{align*}
		\sum_{m=1}^{\infty}P(S_{m}=0)& =\sum_{n=1}^{\infty}P(S_{2n}=0) & \text{(can only return to 0 at even steps)} \\
		&=\sum\limits_{n=1}^\infty\binom{2n}{n}(\frac{1}{2})^{2n}& \text{(combinatorics)} \\
		&\sim\sum_{n=1}^\infty\frac{\sqrt{2\pi2n}(\frac{2n}e)^{2n}}{(\sqrt{2\pi n}(\frac ne)^n)^2}\frac1{2^{2n}}& \text{(Stirling's formula)} \\
		&=\sum_{n=1}^\infty\frac1{\sqrt{\pi n}}=\infty.
	\end{align*}

	In $d=2$, note that in order for $S_{2n}=0$, we must for some $0\leq m\leq n$ have $m$ up steps, $m$ down steps, $n-m$ to the left, and $n-m$ to the right, so 
	\begin{align*}
		\bbP(\S_{2n}=\0)&=\frac{1}{4^{2n}}\sum_{m=0}^{n}\binom{2n}{m}\binom{2n-m}{m}\binom{2n-2m}{n-m}=\frac{1}{4^{2n}}\sum_{m=0}^{n}\frac{(2n)!}{m! m! (n-m)! (n-m)!}\\
		&=\frac{1}{4^{2n}}\binom{2n}n\sum_{m=0}^n\binom nm\binom n{n-m}=4^{-2n}\binom{2n}n^2 \asymp  n^{-1}.
	\end{align*}  
	by Stirling's formula. So its sum is $\infty $, still recurrent.
	
	For $d=3$, more complicated combinatorics give $\bbP(\S_{2n}=0)\asymp\frac{1}{n^{3/2}}$, summing up to a finite
	number; hence transient. In even higher dimensions, the probabilities become even smaller;
	hence all transient.
\end{proof}

\subsubsection{Proofs - \ref{sec:ReflectionArcsine}}\label{sec:proof-ReflectionArcsine}
\begin{proof}[\underline{\textbf{Proof of Theorem \ref{thm:ReflectionPrin}}}]
	To show the first result, suppose $(0,s_0),(1,s_1),\ldots,(n,s_n)$ is a path from $(0,x)$ to $(0,y)$. Let $K=\inf\{k:s_k=0\}$. Let $s_k'=-s_k$ for $k\leq K$ and $s_k'=s_k$ for $K\leq k\leq n$. Then $(k,s_k')$, $0\leq k\leq n$, is a path from $(0,-x)$ to $(n,y)$. Conversely, given a path from $(0,-x)$ to $(n,y)$, we can also construct a reflected path. We have a one-toone
	correspondence between the two classes of paths, so their numbers must be
	equal.

	Then, we have 
	\begin{equation*}
		\begin{aligned}
			\bbP(\max_{1\leq k\leq n}S_k\geq b)&=P(\max_{1\leq k\leq n}S_k\geq b,S_n>b)+P(\max_{1\leq k\leq n}S_k\geq b,S_n<b)+P(\max_{1\leq k\leq n}S_k\geq b,S_n=b) \\
			&=\bbP(S_n>b) + \bbP (\max_{1\leq k\leq n}S_k\geq b,S_n>b) + \bbP(S_n=b)\\
			&=2\bbP(S_n>b) + \bbP(S_n=b),
	\end{aligned}
	\end{equation*}
	which prove the second result.
\end{proof}

\begin{proof}[\underline{\textbf{Proof of Theorem \ref{thm:ReflectionHit0}}}]
	To count the number of paths from $(0,0)$ to $(n,x)$, denote $a,b\in\bbN$ be the number of positive steps and $b$ negative steps, respectively. $n=a+b$, and $x=a-b$, where $x\in[-n,n]$, and $n-x$ is even. The number of paths from $(0,0)$ to $(n,x)$ is $N_{n,x}=\binom{n}{a}$. 
	
	Since 
	\begin{equation*}
		\bbP(S_{1}>0,\ldots,S_{2n}>0)=\sum_{r=1}^{\infty}\bbP(S_{1}>0,\ldots,S_{2n-1}>0,S_{2n}=2r).	
	\end{equation*}  
	Now we count the number of paths of $(1,1)\rightarrow(2n,2r)$, that are never 0. Since the total number of paths of $(1,1)\rightarrow(2n,2r)$ is $N_{2n-1,2r-1}$, the number of these paths touching 0 is the number of paths of $(1,-1)\rightarrow(2n,2r)$, i.e., $N_{2n-1,2r+1}$, by reflection principle, we have the number of paths of $(1,1)\rightarrow(2n,2r)$ never touching 0 is $N_{2n-1,2r-1}-N_{2n-1,2r+1}$. Hence 
	\begin{align*}
		\sum_{r=1}^{\infty}\bbP(S_{1}>0,\ldots,S_{2n-1}>0,S_{2n}=2r)&=\sum_{r=1}^{\infty}\frac{1}{2}\frac{1}{2^{2n-1}}(N_{2n-1,2r-1}-N_{2n-1,2r+1})=\frac{1}{2^{2n}}N_{2n-1,1},
	\end{align*}        
	where the $1/2$ in the $2$nd term guarantees $S_1>0$. Since $\bbP(S_{2n}=0)=\frac{1}{2^{2n}}N_{2n-1,-1}+\frac{1}{2^{2n}}N_{2n-1,1}=2\cdot \frac{1}{2^{2n}}N_{2n-1,1}$,      
	\begin{equation*}
		\bbP(S_{1}>0,\ldots,S_{2n}>0)=\frac{1}{2^{2n}}N_{2n-1,1}=\frac{1}{2}\bbP(S_{2n}=0).
	\end{equation*}
	Symmetry implies $\bbP(S_{1}<0,\ldots,S_{2n}<0)=(1/2)\bbP(S_{2n}=0)$. Then the proof is completed.
\end{proof}

\subsection{Proofs - \ref{sec:martingales}}\label{sec:proof-martingales}

\subsubsection{Proofs - \ref{sec:cond_exp}}\label{sec:proof-cond_exp}


\bibliographystyle{abbrv}
\bibliography{mybib}
%%% end of doc
\end{document}